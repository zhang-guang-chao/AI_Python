{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad1acaf7-09a1-4b92-b05a-3e0f0122e808",
   "metadata": {},
   "source": [
    "# 搜索增强生成进阶（RAG，Retrieval-Augmented Generation）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d979fde",
   "metadata": {},
   "source": [
    "## 💡 这节课会带给你\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe5e70d",
   "metadata": {},
   "source": [
    "1. RAG 中的进阶知识\n",
    "1. 如何优化RAG系统\n",
    "1. Gradio 上传文件功能\n",
    "\n",
    "开始上课！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53910daa",
   "metadata": {},
   "source": [
    "## 一、实战 RAG 系统的进阶知识\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b88b5e",
   "metadata": {},
   "source": [
    "### 1.1、文本分割的粒度\n",
    "\n",
    "![](./rag-document-split.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e19e6cb",
   "metadata": {},
   "source": [
    "**缺陷**\n",
    "\n",
    "1. 粒度太大可能导致检索不精准，粒度太小可能导致信息不全面\n",
    "2. 问题的答案可能跨越两个片段\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f010e6c2-b69b-4747-9668-16bc5d574772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: gradio 4.36.1\n",
      "Uninstalling gradio-4.36.1:\n",
      "  Successfully uninstalled gradio-4.36.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: chromadb in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (0.3.29)\n",
      "Requirement already satisfied: pandas>=1.3 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.28 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (2.32.3)\n",
      "Collecting pydantic<2.0,>=1.9 (from chromadb)\n",
      "  Using cached pydantic-1.10.16-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (151 kB)\n",
      "Requirement already satisfied: hnswlib>=0.7 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (0.8.0)\n",
      "Requirement already satisfied: clickhouse-connect>=0.5.7 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (0.7.12)\n",
      "Requirement already satisfied: duckdb>=0.7.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (1.0.0)\n",
      "Collecting fastapi==0.85.1 (from chromadb)\n",
      "  Using cached fastapi-0.85.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (1.24.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (1.16.3)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (4.66.4)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: graphlib-backport>=1.0.3 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (1.1.0)\n",
      "Collecting starlette==0.20.4 (from fastapi==0.85.1->chromadb)\n",
      "  Using cached starlette-0.20.4-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from starlette==0.20.4->fastapi==0.85.1->chromadb) (4.4.0)\n",
      "Requirement already satisfied: certifi in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2024.6.2)\n",
      "Requirement already satisfied: urllib3>=1.26 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2.2.2)\n",
      "Requirement already satisfied: pytz in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2024.1)\n",
      "Requirement already satisfied: zstandard in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from clickhouse-connect>=0.5.7->chromadb) (0.22.0)\n",
      "Requirement already satisfied: lz4 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from clickhouse-connect>=0.5.7->chromadb) (4.3.3)\n",
      "Requirement already satisfied: coloredlogs in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb) (24.1)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from pandas>=1.3->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from pandas>=1.3->chromadb) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from requests>=2.28->chromadb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from requests>=2.28->chromadb) (3.7)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from tokenizers>=0.13.2->chromadb) (0.23.4)\n",
      "Requirement already satisfied: click>=7.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.15.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi==0.85.1->chromadb) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi==0.85.1->chromadb) (1.2.1)\n",
      "Using cached fastapi-0.85.1-py3-none-any.whl (55 kB)\n",
      "Using cached starlette-0.20.4-py3-none-any.whl (63 kB)\n",
      "Using cached pydantic-1.10.16-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Installing collected packages: pydantic, starlette, fastapi\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.7.4\n",
      "    Uninstalling pydantic-2.7.4:\n",
      "      Successfully uninstalled pydantic-2.7.4\n",
      "  Attempting uninstall: starlette\n",
      "    Found existing installation: starlette 0.37.2\n",
      "    Uninstalling starlette-0.37.2:\n",
      "      Successfully uninstalled starlette-0.37.2\n",
      "  Attempting uninstall: fastapi\n",
      "    Found existing installation: fastapi 0.111.0\n",
      "    Uninstalling fastapi-0.111.0:\n",
      "      Successfully uninstalled fastapi-0.111.0\n",
      "Successfully installed fastapi-0.85.1 pydantic-1.10.16 starlette-0.20.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pydantic==1.9.0\n",
      "  Using cached pydantic-1.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (121 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from pydantic==1.9.0) (4.12.2)\n",
      "Using cached pydantic-1.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.6 MB)\n",
      "Installing collected packages: pydantic\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.16\n",
      "    Uninstalling pydantic-1.10.16:\n",
      "      Successfully uninstalled pydantic-1.10.16\n",
      "Successfully installed pydantic-1.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall gradio -y\n",
    "!pip install chromadb\n",
    "!pip install pydantic==1.9.0\n",
    "# ISSUE HERE: https://github.com/chroma-core/chroma/issues/774"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e08958-b35c-4a54-ab25-07f229113aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "\n",
    "class MyVectorDBConnector:\n",
    "    def __init__(self, collection_name, embedding_fn):\n",
    "        chroma_client = chromadb.Client(Settings(allow_reset=True))\n",
    "\n",
    "        # 为了演示，实际不需要每次 reset()\n",
    "        chroma_client.reset()\n",
    "\n",
    "        # 创建一个 collection\n",
    "        self.collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "        self.embedding_fn = embedding_fn\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "        '''向 collection 中添加文档与向量'''\n",
    "        self.collection.add(\n",
    "            embeddings=self.embedding_fn(documents),  # 每个文档的向量\n",
    "            documents=documents,  # 文档的原文\n",
    "            ids=[f\"id{i}\" for i in range(len(documents))]  # 每个文档的 id\n",
    "        )\n",
    "\n",
    "    def search(self, query, top_n):\n",
    "        '''检索向量数据库'''\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=self.embedding_fn([query]),\n",
    "            n_results=top_n\n",
    "        )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "351cf43c-2bce-4fa3-89aa-61ea47173b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, model=\"text-embedding-ada-002\",dimensions=None):\n",
    "    '''封装 OpenAI 的 Embedding 模型接口，文档地址 https://platform.openai.com/docs/guides/embeddings/what-are-embeddings'''\n",
    "    if model == \"text-embedding-ada-002\":\n",
    "        dimensions = None\n",
    "    if dimensions:\n",
    "        data = client.embeddings.create(input=texts, model=model, dimensions=dimensions).data\n",
    "    else:\n",
    "        data = client.embeddings.create(input=texts, model=model).data\n",
    "    return [x.embedding for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b9ccfac-bd62-460b-9cb5-41a6df46dc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(filename, page_numbers=None, min_line_length=1):\n",
    "    '''从 PDF 文件中（按指定页码）提取文字'''\n",
    "    paragraphs = []\n",
    "    buffer = ''\n",
    "    full_text = ''\n",
    "    # 提取全部文本\n",
    "    for i, page_layout in enumerate(extract_pages(filename)):\n",
    "        # 如果指定了页码范围，跳过范围外的页\n",
    "        if page_numbers is not None and i not in page_numbers:\n",
    "            continue\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                full_text += element.get_text() + '\\n'\n",
    "                \n",
    "    # 按空行分隔，将文本重新组织成段落\n",
    "    lines = full_text.split('\\n')\n",
    "    for text in lines:\n",
    "        if len(text) >= min_line_length:\n",
    "            buffer += (' '+text) if not text.endswith('-') else text.strip('-')\n",
    "        elif buffer:\n",
    "            paragraphs.append(buffer)\n",
    "            buffer = ''\n",
    "    if buffer:\n",
    "        paragraphs.append(buffer)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e4db04e-1a39-4ef9-b7a9-2cb468e7ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了演示方便，我们只取两页（第一章）\n",
    "paragraphs = extract_text_from_pdf(\n",
    "    \"llama2.pdf\", \n",
    "    page_numbers=[2, 3], \n",
    "    min_line_length=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8cb66e9-e140-437d-b8b3-1c1e0b55fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "# 加载环境变量\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())  # 读取本地 .env 文件，里面定义了 OPENAI_API_KEY\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ebed3d1-6558-4ea4-8b59-339729795c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG_Bot:\n",
    "    def __init__(self, vector_db, llm_api, n_results=2):\n",
    "        self.vector_db = vector_db\n",
    "        self.llm_api = llm_api\n",
    "        self.n_results = n_results\n",
    "\n",
    "    def chat(self, user_query):\n",
    "        # 1. 检索\n",
    "        search_results = self.vector_db.search(user_query, self.n_results)\n",
    "\n",
    "        # 2. 构建 Prompt\n",
    "        prompt = build_prompt(\n",
    "            prompt_template, info=search_results['documents'][0], query=user_query)\n",
    "\n",
    "        # 3. 调用 LLM\n",
    "        response = self.llm_api(prompt)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0b161cb-e0a0-4647-8f65-4c6c3863d116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    '''封装 openai 接口'''\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,  # 模型输出的随机性，0 表示随机性最小\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c58e9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个向量数据库对象\n",
    "vector_db = MyVectorDBConnector(\"demo_text_split\", get_embeddings)\n",
    "# 向向量数据库中添加文档\n",
    "vector_db.add_documents(paragraphs)\n",
    "\n",
    "# 创建一个RAG机器人\n",
    "bot = RAG_Bot(\n",
    "    vector_db,\n",
    "    llm_api=get_completion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96c9904f-17bc-4ba4-88a0-b94c3a77a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(prompt_template, **kwargs):\n",
    "    '''将 Prompt 模板赋值'''\n",
    "    prompt = prompt_template\n",
    "    for k, v in kwargs.items():\n",
    "        if isinstance(v, str):\n",
    "            val = v\n",
    "        elif isinstance(v, list) and all(isinstance(elem, str) for elem in v):\n",
    "            val = '\\n'.join(v)\n",
    "        else:\n",
    "            val = str(v)\n",
    "        prompt = prompt.replace(f\"__{k.upper()}__\", val)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c00865d2-979a-4cd5-8005-56c8d5dad826",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "你是一个问答机器人。\n",
    "你的任务是根据下述给定的已知信息回答用户问题。\n",
    "确保你的回复完全依据下述已知信息。不要编造答案。\n",
    "如果下述已知信息不足以回答用户的问题，请直接回复\"我无法回答您的问题\"。\n",
    "\n",
    "已知信息:\n",
    "__INFO__\n",
    "\n",
    "用户问：\n",
    "__QUERY__\n",
    "\n",
    "请用中文回答用户问题。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3e839d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover all scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform safety testing and tuning tailored to their specific applications of the model. We provide a responsible use guide¶ and code examples‖ to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of our responsible release strategy can be found in Section 5.3.\n",
      "\n",
      " 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.§\n",
      "\n",
      "====回复====\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我无法回答您的问题。'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query = \"llama 2可以商用吗？\"\n",
    "# user_query=\"llama 2 chat有多少参数\"\n",
    "search_results = vector_db.search(user_query, 2)\n",
    "\n",
    "for doc in search_results['documents'][0]:\n",
    "    print(doc+\"\\n\")\n",
    "\n",
    "print(\"====回复====\")\n",
    "bot.chat(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25c532f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Figure 1: Helpfulness human evaluation results for Llama 2-Chat compared to other open-source and closed-source models. Human raters compared model generations on ~4k prompts consisting of both single and multi-turn prompts. The 95% confidence intervals for this evaluation are between 1% and 2%. More details in Section 3.4.2. While reviewing these results, it is important to note that human evaluations can be noisy due to limitations of the prompt set, subjectivity of the review guidelines, subjectivity of individual raters, and the inherent difficulty of comparing generations.\n",
      "\n",
      " Figure 2: Win-rate % for helpfulness andsafety between commercial-licensed baselines and Llama 2-Chat, according to GPT 4. To complement the human evaluation, we used a more capable model, not subject to our own guidance. Green area indicates our model is better according to GPT-4. To remove ties, we used win/(win + loss). The orders in which the model responses are presented to GPT-4 are randomly swapped to alleviate bias.\n",
      "\n",
      " 1 Introduction\n",
      "\n",
      " Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public.\n",
      "\n",
      " The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF). Although the training methodology is simple, high computational requirements have limited the development of LLMs to a few players. There have been public releases of pretrained LLMs (such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla (Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such as ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human preferences, which greatly enhances their usability and safety. This step can require significant costs in compute and human annotation, and is often not transparent or easily reproducible, limiting progress within the community to advance AI alignment research.\n",
      "\n",
      " In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge.\n",
      "\n",
      "Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed source models. Human raters judged model generations for safety violations across ~2,000 adversarial prompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is important to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the prompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these safety evaluations are performed using content standards that are likely to be biased towards the Llama 2-Chat models.\n",
      "\n",
      " We are releasing the following models to the general public for research and commercial use‡:\n",
      "\n",
      " 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.§\n",
      "\n",
      " 2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release\n",
      "\n",
      " variants of this model with 7B, 13B, and 70B parameters as well.\n",
      "\n",
      " We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover all scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform safety testing and tuning tailored to their specific applications of the model. We provide a responsible use guide¶ and code examples‖ to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of our responsible release strategy can be found in Section 5.3.\n",
      "\n",
      " The remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology (Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related work (Section 6), and conclusions (Section 7).\n",
      "\n",
      " ‡https://ai.meta.com/resources/models-and-libraries/llama/ §We are delaying the release of the 34B model due to a lack of time to sufficiently red team. ¶https://ai.meta.com/llama ‖https://github.com/facebookresearch/llama\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in paragraphs:\n",
    "    print(p+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a31c01",
   "metadata": {},
   "source": [
    "**改进**: 按一定粒度，部分重叠式的切割文本，使上下文更完整\n",
    "\n",
    "![](./rag-overlap.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2044d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "\n",
    "\n",
    "def split_text(paragraphs, chunk_size=300, overlap_size=100):\n",
    "    '''按指定 chunk_size 和 overlap_size 交叠割文本'''\n",
    "    sentences = [s.strip() for p in paragraphs for s in sent_tokenize(p)]\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        chunk = sentences[i]\n",
    "        overlap = ''\n",
    "        prev_len = 0\n",
    "        prev = i - 1\n",
    "        # 向前计算重叠部分\n",
    "        while prev >= 0 and len(sentences[prev])+len(overlap) <= overlap_size:\n",
    "            overlap = sentences[prev] + ' ' + overlap\n",
    "            prev -= 1\n",
    "        chunk = overlap+chunk\n",
    "        next = i + 1\n",
    "        # 向后计算当前chunk\n",
    "        while next < len(sentences) and len(sentences[next])+len(chunk) <= chunk_size:\n",
    "            chunk = chunk + ' ' + sentences[next]\n",
    "            next += 1\n",
    "        chunks.append(chunk)\n",
    "        i = next\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90431eb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "此处 sent_tokenize 为针对英文的实现，针对中文的实现请参考 chinese_utils.py\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04fb4b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_text(paragraphs, 300, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a615c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个向量数据库对象\n",
    "vector_db = MyVectorDBConnector(\"demo_text_split\", get_embeddings)\n",
    "# 向向量数据库中添加文档\n",
    "vector_db.add_documents(chunks)\n",
    "# 创建一个RAG机器人\n",
    "bot = RAG_Bot(\n",
    "    vector_db,\n",
    "    llm_api=get_completion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b939657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are releasing the following models to the general public for research and commercial use‡: 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data.\n",
      "\n",
      "Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well. We believe that the open release of LLMs, when done safely, will be a net benefit to society.\n",
      "\n",
      "====回复====\n",
      "可以商用。\n"
     ]
    }
   ],
   "source": [
    "user_query = \"llama 2可以商用吗？\"\n",
    "# user_query=\"llama 2 chat有多少参数\"\n",
    "\n",
    "search_results = vector_db.search(user_query, 2)\n",
    "for doc in search_results['documents'][0]:\n",
    "    print(doc+\"\\n\")\n",
    "\n",
    "response = bot.chat(user_query)\n",
    "print(\"====回复====\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc0e93e",
   "metadata": {},
   "source": [
    "### 1.2、检索后排序（选）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d33295d",
   "metadata": {},
   "source": [
    "**问题**: 有时，最合适的答案不一定排在检索的最前面\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5f0624b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023).\n",
      "\n",
      "We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge. Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed source models.\n",
      "\n",
      "In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models.\n",
      "\n",
      "Additionally, these safety evaluations are performed using content standards that are likely to be biased towards the Llama 2-Chat models. We are releasing the following models to the general public for research and commercial use‡: 1.\n",
      "\n",
      "We provide a responsible use guide¶ and code examples‖ to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of our responsible release strategy can be found in Section 5.3.\n",
      "\n",
      "====回复====\n",
      "根据已知信息，Llama 2的安全性在进行安全发布的情况下将对社会产生净益。然而，像所有LLMs一样，Llama 2作为一项新技术在使用过程中存在潜在风险。由于没有具体的安全评估结果提供，无法准确回答Llama 2的安全性问题。\n"
     ]
    }
   ],
   "source": [
    "user_query = \"how safe is llama 2\"\n",
    "search_results = vector_db.search(user_query, 5)\n",
    "\n",
    "for doc in search_results['documents'][0]:\n",
    "    print(doc+\"\\n\")\n",
    "\n",
    "response = bot.chat(user_query)\n",
    "print(\"====回复====\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d0365e",
   "metadata": {},
   "source": [
    "**方案**:\n",
    "\n",
    "1. 检索时过召回一部分文本\n",
    "2. 通过一个排序模型对 query 和 document 重新打分排序\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96da7011",
   "metadata": {},
   "source": [
    "<img src=\"sbert-rerank.png\" style=\"margin-left: 0px\" width=600px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9652ae8d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>备注：</b>\n",
    "<div>由于 huggingface 被墙，我们已经为您准备好了本章相关模型。请点击以下网盘链接进行下载：\n",
    "    \n",
    "链接: https://pan.baidu.com/s/1X0kfNKasvWqCLUEEyAvO-Q?pwd=3v6y 提取码: 3v6y </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ebcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af04250-2516-4bf3-b22c-98919a9373b5",
   "metadata": {},
   "source": [
    "使用 CrossEncoder 来作为重新打分排序的模型，CrossEncoder 的文档地址：https://www.sbert.net/examples/applications/cross-encoder/README.html\n",
    "\n",
    "<img src=\"./cross-encoder.png\" width=\"600px\"></img>\n",
    "\n",
    "**同时将两个句子传递给 Transformer 网络。它产生一个输出值，表示输入句对的相似性，值越大越相近。**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff559832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe66adaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.598450660705566\tLlama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well. We believe that the open release of LLMs, when done safely, will be a net benefit to society.\n",
      "\n",
      "-0.4974502623081207\tWe are releasing the following models to the general public for research and commercial use‡: 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_query = \"how safe is llama 2\"\n",
    "\n",
    "scores = model.predict([(user_query, doc)\n",
    "                       for doc in search_results['documents'][0]])\n",
    "# 按得分排序\n",
    "sorted_list = sorted(\n",
    "    zip(scores, search_results['documents'][0]), key=lambda x: x[0], reverse=True)\n",
    "for score, doc in sorted_list:\n",
    "    print(f\"{score}\\t{doc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822ef085",
   "metadata": {},
   "source": [
    "### 1.3、混合检索（Hybrid Search）（选）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347b74f0",
   "metadata": {},
   "source": [
    "在**实际生产**中，传统的关键字检索（稀疏表示）与向量检索（稠密表示）各有优劣。\n",
    "\n",
    "举个具体例子，比如文档中包含很长的专有名词，关键字检索往往更精准而向量检索容易引入概念混淆。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caa44bb7-e6c5-46bd-bcc4-9e5c74696e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    '''余弦距离 -- 越大越相似'''\n",
    "    return dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ea35bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance:\n",
      "0.910667535993348\n",
      "0.8895478505819983\n",
      "0.9039165614288258\n",
      "0.9131441645902687\n"
     ]
    }
   ],
   "source": [
    "# 背景说明：在医学中“小细胞肺癌”和“非小细胞肺癌”是两种不同的癌症\n",
    "\n",
    "query = \"非小细胞肺癌的患者\"\n",
    "\n",
    "documents = [\n",
    "    \"李某患有肺癌，癌细胞已转移\",\n",
    "    \"刘某肺癌I期\",\n",
    "    \"张某经诊断为非小细胞肺癌III期\",\n",
    "    \"小细胞肺癌是肺癌的一种\"\n",
    "]\n",
    "\n",
    "query_vec = get_embeddings([query])[0]\n",
    "doc_vecs = get_embeddings(documents)\n",
    "\n",
    "print(\"Cosine distance:\")\n",
    "for vec in doc_vecs:\n",
    "    print(cos_sim(query_vec, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35686302",
   "metadata": {},
   "source": [
    "所以，有时候我们需要结合不同的检索算法，来达到比单一检索算法更优的效果。这就是**混合检索**。\n",
    "\n",
    "混合检索的核心是，综合文档 $d$ 在不同检索算法下的排序名次（rank），为其生成最终排序。\n",
    "\n",
    "一个最常用的算法叫 **Reciprocal Rank Fusion（RRF）**\n",
    "\n",
    "$rrf(d)=\\sum_{a\\in A}\\frac{1}{k+rank_a(d)}$\n",
    "\n",
    "其中 $A$ 表示所有使用的检索算法的集合，$rank_a(d)$ 表示使用算法 $a$ 检索时，文档 $d$ 的排序，$k$ 是个常数。\n",
    "\n",
    "很多向量数据库都支持混合检索，比如 [Weaviate](https://weaviate.io/blog/hybrid-search-explained)、[Pinecone](https://www.pinecone.io/learn/hybrid-search-intro/) 等。也可以根据上述原理自己实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e392db",
   "metadata": {},
   "source": [
    "### 1.3.1、我们手写个简单的例子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aac040",
   "metadata": {},
   "source": [
    "1. 基于关键字检索的排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02b3c6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class MyEsConnector:\n",
    "    def __init__(self, es_client, index_name, keyword_fn):\n",
    "        self.es_client = es_client\n",
    "        self.index_name = index_name\n",
    "        self.keyword_fn = keyword_fn\n",
    "    \n",
    "    def add_documents(self, documents):\n",
    "        '''文档灌库'''\n",
    "        if self.es_client.indices.exists(index=self.index_name):\n",
    "            self.es_client.indices.delete(index=self.index_name)\n",
    "        self.es_client.indices.create(index=self.index_name)\n",
    "        actions = [\n",
    "            {\n",
    "                \"_index\": self.index_name,\n",
    "                \"_source\": {\n",
    "                    \"keywords\": self.keyword_fn(doc),\n",
    "                    \"text\": doc,\n",
    "                    \"id\": f\"doc_{i}\"\n",
    "                }\n",
    "            }\n",
    "            for i,doc in enumerate(documents)\n",
    "        ]\n",
    "        helpers.bulk(self.es_client, actions)\n",
    "        time.sleep(1)\n",
    "\n",
    "    def search(self, query_string, top_n=3):\n",
    "        '''检索'''\n",
    "        search_query = {\n",
    "            \"match\": {\n",
    "                \"keywords\": self.keyword_fn(query_string)\n",
    "            }\n",
    "        }\n",
    "        res = self.es_client.search(index=self.index_name, query=search_query, size=top_n)\n",
    "        return { \n",
    "            hit[\"_source\"][\"id\"] : {\n",
    "                \"text\" : hit[\"_source\"][\"text\"],\n",
    "                \"rank\" : i,\n",
    "            }\n",
    "            for i, hit in enumerate(res[\"hits\"][\"hits\"])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4da318e1-54ed-4ee5-a244-e4560fa212aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (0.42.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad4864f0-9cbc-44c3-9f21-28e69b7d3bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from elasticsearch7 import Elasticsearch, helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "667d0d93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.814 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc_2': {'text': '张某经诊断为非小细胞肺癌III期', 'rank': 0}, 'doc_0': {'text': '李某患有肺癌，癌细胞已转移', 'rank': 1}, 'doc_3': {'text': '小细胞肺癌是肺癌的一种', 'rank': 2}}\n"
     ]
    }
   ],
   "source": [
    "from chinese_utils import to_keywords # 使用中文的关键字提取函数\n",
    "\n",
    "es = Elasticsearch(\n",
    "    hosts=['http://localhost:9200'],  # 服务地址与端口\n",
    "    # http_auth=(\"elastic\", \"FKaB1Jpz0Rlw0l6G\"),  # 用户名，密码\n",
    ")\n",
    "\n",
    "# 创建 ES 连接器\n",
    "es_connector = MyEsConnector(es, \"demo_es_rrf\", to_keywords)\n",
    "\n",
    "# 文档灌库\n",
    "es_connector.add_documents(documents)\n",
    "\n",
    "# 关键字检索\n",
    "keyword_search_results = es_connector.search(query, 3)\n",
    "\n",
    "print(keyword_search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e81c9c7",
   "metadata": {},
   "source": [
    "2. 基于向量检索的排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccef3f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc_3': {'text': '小细胞肺癌是肺癌的一种', 'rank': 0}, 'doc_0': {'text': '李某患有肺癌，癌细胞已转移', 'rank': 1}, 'doc_2': {'text': '张某经诊断为非小细胞肺癌III期', 'rank': 2}}\n"
     ]
    }
   ],
   "source": [
    "# 创建向量数据库连接器\n",
    "vecdb_connector = MyVectorDBConnector(\"demo_vec_rrf\", get_embeddings)\n",
    "\n",
    "# 文档灌库\n",
    "vecdb_connector.add_documents(documents)\n",
    "\n",
    "# 向量检索\n",
    "vector_search_results = {\n",
    "    \"doc_\"+str(documents.index(doc)) : {\n",
    "        \"text\" : doc,\n",
    "        \"rank\" : i\n",
    "    }\n",
    "    for i, doc in enumerate(\n",
    "        vecdb_connector.search(query, 3)[\"documents\"][0]\n",
    "    )\n",
    "} # 把结果转成跟上面关键字检索结果一样的格式\n",
    "\n",
    "print(vector_search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99036ec7",
   "metadata": {},
   "source": [
    "3. 基于 RRF 的融合排序\n",
    "\n",
    "   $rrf(d)=\\sum_{a\\in A}\\frac{1}{k+rank_a(d)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "842780c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf(ranks, k=1):\n",
    "    ret = {}\n",
    "    # 遍历每次的排序结果\n",
    "    for rank in ranks: \n",
    "        # 遍历排序中每个元素\n",
    "        for id, val in rank.items():\n",
    "            if id not in ret:\n",
    "                ret[id] = { \"score\": 0, \"text\": val[\"text\"] }\n",
    "            # 计算 RRF 得分\n",
    "            ret[id][\"score\"] += 1.0/(k+val[\"rank\"])\n",
    "    # 按 RRF 得分排序，并返回\n",
    "    return dict(sorted(ret.items(), key=lambda item: item[1][\"score\"], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93576105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"doc_2\": {\n",
      "        \"score\": 1.3333333333333333,\n",
      "        \"text\": \"张某经诊断为非小细胞肺癌III期\"\n",
      "    },\n",
      "    \"doc_3\": {\n",
      "        \"score\": 1.3333333333333333,\n",
      "        \"text\": \"小细胞肺癌是肺癌的一种\"\n",
      "    },\n",
      "    \"doc_0\": {\n",
      "        \"score\": 1.0,\n",
      "        \"text\": \"李某患有肺癌，癌细胞已转移\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 融合两次检索的排序结果\n",
    "reranked = rrf([keyword_search_results,vector_search_results])\n",
    "\n",
    "print(json.dumps(reranked,indent=4,ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8c9acc",
   "metadata": {},
   "source": [
    "### 1.4、RAG-Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e5320c",
   "metadata": {},
   "source": [
    "RAG-Fusion 就是利用了 RRF 的原理来提升检索的准确性。\n",
    "\n",
    "<img src=\"rag-fusion.jpeg\" style=\"margin-left: 0px\" width=600px>\n",
    "\n",
    "原始项目（一段非常简短的演示代码）：https://github.com/Raudaschl/rag-fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96880a42",
   "metadata": {},
   "source": [
    "## 二、向量模型的本地部署\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e861305c",
   "metadata": {},
   "source": [
    "利用 sentence_transformers 的本地部署能力，部署向量模型。\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>备注：</b>\n",
    "<div>由于 huggingface 被墙，我们已经为您准备好了本章相关模型。请点击以下网盘链接进行下载：\n",
    "    \n",
    "链接: https://pan.baidu.com/s/1X0kfNKasvWqCLUEEyAvO-Q?pwd=3v6y 提取码: 3v6y </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd689aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#model_name = 'BAAI/bge-large-zh-v1.5' #中文\n",
    "model_name = 'moka-ai/m3e-base' #中英双语，但效果一般\n",
    "\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c3dbb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance:\n",
      "0.6958814\n",
      "0.6573525\n",
      "0.6653428\n",
      "0.637189\n",
      "0.69429\n"
     ]
    }
   ],
   "source": [
    "#query = \"国际争端\"\n",
    "query = \"global conflicts\"\n",
    "\n",
    "documents = [\n",
    "    \"联合国就苏丹达尔富尔地区大规模暴力事件发出警告\",\n",
    "    \"土耳其、芬兰、瑞典与北约代表将继续就瑞典“入约”问题进行谈判\",\n",
    "    \"日本岐阜市陆上自卫队射击场内发生枪击事件 3人受伤\",\n",
    "    \"国家游泳中心（水立方）：恢复游泳、嬉水乐园等水上项目运营\",\n",
    "    \"我国首次在空间站开展舱外辐射生物学暴露实验\",\n",
    "]\n",
    "\n",
    "query_vec = model.encode(query)\n",
    "\n",
    "doc_vecs = [\n",
    "    model.encode(doc)\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "print(\"Cosine distance:\")  # 越大越相似\n",
    "#print(cos_sim(query_vec, query_vec))\n",
    "for vec in doc_vecs:\n",
    "    print(cos_sim(query_vec, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee805cc1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>扩展阅读：https://github.com/FlagOpen/FlagEmbedding</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ecad9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>划重点：</b>\n",
    "    <ol>\n",
    "        <li>不是每个 Embedding 模型都对余弦距离和欧氏距离同时有效</li>\n",
    "        <li>哪种相似度计算有效要阅读模型的说明（通常都支持余弦距离计算）</li>\n",
    "    </ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42219b55-7917-4ec7-88db-348c81a5cfa2",
   "metadata": {},
   "source": [
    "## 三、如何持续改进RAG应用效果？（前延扩展，综合思考）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5a58cc-417f-4c8f-9745-0025311e67b2",
   "metadata": {},
   "source": [
    "随着深入使用，你可能发现你的 RAG 应用可能只是能用了，但还有很多问题，比如：\n",
    "\n",
    "- 问题比较抽象或者概念比较模糊，导致大模型没有准确理解使用者的问题。例如，使用者问“兰州拉面去哪吃？”，使用者本来想问附近有没有卖兰州拉面的店铺；假如知识库搜索“兰州”和“拉面”之后，结果排序靠前的语料是位于“兰州”的拉面馆地址，而大模型告诉用户要去买飞机票或者火车票，就是答非所问了。通常我们用改造问题，让使用者的问题更好理解的策略来回避这种情况。\n",
    "- 知识库没有检索到问题的答案，这有可能是由于语料数据没有做好整理就存入知识库，或者是检索策略有问题，参数需要调整导致的。比如，在采用“K个最相似文档块”作为回答的知识这个策略中，如果K值比较小，那么最相似的K个文档块中可能并不包含能解答用户问题的有效知识，那么答案很可能就是错误的。例如，作为旅游手册的知识库中有大量文段是 兰州拉面如何制作的菜谱、兰州拉面的产地、兰州有哪些特产等，只有一两条信息描述了附近拉面馆的地址。那么当用户询问“兰州拉面怎么走？”时，知识库检索到的信息可能只是兰州拉面的选材、调味、烹饪方面的信息，而唯独没有检索到前方50米处有一家兰州拉面馆。用户也没有办法获得有效的答案。\n",
    "- 缺少对答案做兜底验证的机制，假设运气很好，志愿者不仅听懂了游客的问题，也正确查找到了附近最近的两家拉面馆的信息，但是志愿者的回答方式是“向北走200米就到了”。这有可能是一个正确的答案，但不是一个好的答案，实际考察过景区地形后我们可能会发现，志愿者北面是后海，你不太可能穿过湖面去一个地方。实际路径可能是：先向东走50十米，再向北走绕过后海，走到湖对面去，才能走到正确位置。那这个“向北走200米...”的回答，从导航的角度就不能算是准确了。\n",
    "\n",
    "我们会发现许多改进标准 RAG 框架的方法，下面我们将一起了解这些改进思路。\n",
    "\n",
    "![](./rag-promote.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1b45e9-dbf5-4b54-a6f6-7c196a7cc59e",
   "metadata": {},
   "source": [
    "### 3.1 建立评测标准\n",
    "为了持续改进我们的 RAG 应用，首要任务应当是构建一套严谨的评测指标体系，并邀请业务领域专家作为评测方共同参与评测工作，我们可以设置与我们业务相关的多种问题场景，系统性地检查一个RAG系统反应快不快，回答准不准，有没有理解用户问题的意图等方面。通过科学全面的评测，我们可以了解到系统在哪些地方做得好，哪些地方需要改进，从而帮助开发者让RAG系统更好的服务于业务需求。\n",
    "RAG系统一般包括检索和生成两个模块，我们做评测时就可以从这两个模块分别入手建立评价标准和实施方法，当然你也可以用最终效果为标准，建立端到端的评测。在评测指标设计上，我们主要考察检索模块的准确性，如准确率、召回率、F1值等等；在生成模块，我们主要考察生成答案的价值，如相关性、真实性等等。\n",
    "我们可以引入业界认可的一些通用评估策略，比如，你可以参考Ragas提及的评测矩阵指南，你也可以建立一些自己的评测指标，这些评测方法将会有助于你量化和改进每一个子模块的表现。\n",
    "\n",
    "**现在大家先对Ragas有个印象，后面讲Langchain还会讲这个怎么计算和使用**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a98dc8a6-f892-445a-a59c-85147d4868b3",
   "metadata": {},
   "source": [
    "![](./rag-promote.jpg)\n",
    "\n",
    "### 3.2 改造一：提升索引准确率\n",
    "\n",
    "- **优化文本解析过程**\n",
    "  \n",
    "在构建知识库的时候，我们首先需要正确的从文档中提取有效语料。因此，优化文本解析的过程往往对提升RAG的性能有很大帮助。例如，从网页中提取有效信息时，我们需要判断哪些部分应该被去掉（比如页眉页脚标签），哪些部分应该被保留（比如属于网页内容的表格标签）。\n",
    "\n",
    "- **优化chunk切分模式**\n",
    "  \n",
    "Chunk就是数据或信息的一个小片段或者区块。当你在处理大量的文本、数据或知识时，如果你一次性全部交给大模型来阅读和处理，效率是非常低的。所以，我们把它们切分成更小、更易管理的部分，这些部分就是chunk。每个chunk都包含了一些有用的信息，这样当系统根据用户的问题寻找某个知识时，它可以迅速定位到与答案相关信息的chunk，而不是在整个数据库中盲目搜索。因此，通过精心设计的chunk切分策略，系统能够更高效地检索信息，就像图书馆里按照类别和标签整理书籍一样，使得查找特定内容变得容易。优化chunk切分模式，就能加速信息检索、提升回答质量和生成效率。具体方法有很多：\n",
    "\n",
    "    1. 利用领域知识：针对特定领域的文档，利用领域专有知识进行更精准的切分。例如，在法律文档中识别段落编号、条款作为切分依据。\n",
    "    2. 基于固定大小切分：比如默认采用128个词或512个词切分为一个chunk，可以快速实现文本分块。缺点是忽略了语义和上下文完整性。\n",
    "    3. 上下文感知：在切分时考虑前后文关系，避免信息断裂。可以通过保持特定句对或短语相邻，或使用更复杂的算法识别并保留语义完整性。最简单的做法是切分时保留前一句和后一句话。你也可以使用自然语言处理技术识别语义单元，如通过句子相似度计算、主题模型（如LDA）或BERT嵌入聚类来切分文本，确保每个chunk内部语义连贯，减少跨chunk信息依赖。通义实验室提供了一种文本切割模型，输入长文本即可得到切割好的文本块，详情可参考：中文文本分割模型。\n",
    "\n",
    "以上介绍了一些常见策略，你也可以考虑使用更复杂的切分策略，如围绕关键词切分或者采用动态调整的切分策略等，主要目的是为了保证每个chunk中信息的完整性，更好的服务系统提升检索质量。\n",
    "\n",
    "- **句子滑动窗口检索**\n",
    "  \n",
    "这个策略是通过设置window_size（窗口大小）来调整提取句子的数量，当用户的问题匹配到一个chunk语料块时，通过窗函数提取目标语料块的上下文，而不仅仅是语料块本身，这样来获得更完整的语料上下文信息，提升RAG生成质量。\n",
    "\n",
    "![](./rag-context.jpg)\n",
    "图6：句子滑窗检索获取检索到的句子的上下文\n",
    "\n",
    "- **自动合并检索**\n",
    "\n",
    "这个策略是将文档分块，建成一棵语料块的树，比如1024切分、512切分、128切分，并构造出一棵文档结构树。当应用作搜索时，如果同一个父节点的多个叶子节点被选中，则返回整个父节点对应的语料块。从而确保与问题相关的语料信息被完整保留下来，从而大幅提升RAG的生成质量。实测发现这个方法比句子滑动窗口检索效果好。\n",
    "\n",
    "![](./rag-merge.jpg)\n",
    "图7：自动合并检索的方法，返回父节点文本作为检索结果\n",
    "\n",
    "\n",
    "- **选择更适合业务的Embedding模型**\n",
    "\n",
    "\n",
    "经过切分的语料块在提供检索服务之前，我们需要把chunk语料块由原来的文本内容转换为机器可以用于比对计算的一组数字，即变为Embedding向量。我们通过Embedding模型来进行这个转换。但是，由于不同的Embedding模型对于生成Embedding向量质量的影响很大，好的Embedding模型可以提升检索的准确率。\n",
    "比如，针对中文检索的场景，我们应当选择在中文语料上表现更好的模型。那么针对你的业务场景，你也可以建议你的技术团队做Embedding模型的技术选型，挑选针对你的业务场景表现较好的模型。\n",
    "\n",
    "- **选择更适合业务的ReRank模型**\n",
    "\n",
    "\n",
    "除了优化生成向量的质量，我们还需要同时优化做向量排序的ReRank模型，好的ReRank模型会让更贴近用户问题的chunks的排名更靠前。因此，我们也可以挑选能让你的业务应用表现更好的ReRank模型。\n",
    "\n",
    "- **Raptor 用聚类为文档块建立索引**\n",
    "\n",
    "\n",
    "还有一类有意思的做法是采用无监督聚类来生成文档索引。这就像通过文档的内容为文档自动建立目录的过程。假如志愿者拿到的文本资料是没有目录的，志愿者一页一页查找资料必然很慢。因此，可以将词条信息聚类，比如按照商店、公园、酒吧、咖啡店、中餐馆、快餐店等方式进行分组，建立目录，再根据汉语拼音字母来排序。这样志愿者来查找信息的时候就可以更快速地进行定位。\n",
    "\n",
    "![](./raptor.jpg)\n",
    "图8：Raptor 用聚类为文档块建立索引"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae692d64-edd5-4d59-ab89-9ea05c375928",
   "metadata": {},
   "source": [
    "### 3.3 改造二：让问题更好理解\n",
    "我们希望做到能让人们通过口语对话来使用大模型应用。然而，人们在口语化表达自己的目的和意图时，往往会出现一些问题。比如，问题过于简单含糊出现了语义混淆，导致大模型理解错误；问题的要素非常多，而用户又讲得太少，只能在反复对话中不断沟通补全；问题涉及的知识点超出了大模型训练语料，或者知识库的覆盖范围，导致大模型编造了一些信息来回答等等。所以，我们期望能在用户提问的环节进行介入，让大模型能更好的理解用户的问题。针对这个问题进行尝试的论文很多，提供了很多有意思的实现思路，如Multi-Query、RAG-Fusion、Decomposition、Step-back、HyDE等等，我们简要讲解一下这些方法的思路。\n",
    "\n",
    "- **Enrich 完善用户问题**\n",
    "  \n",
    "我们首先介绍一种比较容易想到的思路，让大模型来完善用户的原始问题，产生一个更利于系统理解的完善后的用户问题，再让后续的系统去执行用户的需求。通过用大模型对用户的问题进行专业化改写，特别是加入了知识库的支持，我们可以生成更专业的问题。下图展现了一种理想的对用户问题的Enrich流程。我们通过多轮对话逐步确认用户需求。\n",
    "\n",
    "    - 一种理想的通过多轮对话补全需求的方案。该设想是通过大模型多次主动与用户沟通，不断收集信息，完善对用户真实意图的理解，补全执行用户需求所需的各项参数。如下图所示。\n",
    "\n",
    "![](./rag-qa-flow.jpg)\n",
    "图9：通过多轮对话完善用户问题的工作流\n",
    "\n",
    "以下展示一个通过多轮对话来补全用户问题的案例：\n",
    "\n",
    "![](./rag-chat-example.jpg)\n",
    "\n",
    "但是在实际的生产中，一方面用户可能没有那么大的耐性反复提供程序需要的信息，另一方面开发者也需要考虑如何终止信息采集对话，比如让大模型输出停止语“<EOS>（End Of Sentence）”。所以在实践中，我们需要采用一些更容易实现的方案。\n",
    "\n",
    "    - 让大模型转述用户问题，再进行RAG问答。参考“指令提示词”的思路，我们可以让大模型来转述用户的问题，将用户的问题标准化，规范化。这里我们可以提供一套标准提示词模板，提供一些标准化的示例，也可以用知识库来增强。我们的主要目的是规范用户的输入请求，再生成RAG查询指令，从而提升RAG查询质量。\n",
    "\n",
    "![](./rag-qfill.jpg)\n",
    "图10：让大模型根据知识库来补全用户请求\n",
    "\n",
    "    - 让用户补全信息辅助业务调用。有一些应用场景需要大量的参数支撑，（比如订火车票需要起点、终点、时间、座位等级、座位偏好等等），我们还可以进一步完善上面的思路，一次性告诉用户系统需要什么信息，让用户来补全。首先，需要准确理解用户的意图，实现意图识别的手段很多，如使用向量相似度匹配、使用搜索引擎、或者直接大模型来支持。其次，根据用户意图选择合适的业务需求模板。接着，让大模型参考业务需求模版来生成一段对话发给用户，请求用户补充信息。这时，如果用户进行了信息完善，那么大模型就可以基于用户的回复信息结合用户的请求来生成下一步的行动指令，整个系统就可以实现应用系统自动帮助用户订机票、订酒店，完成知识库问答等应用形式。\n",
    "\n",
    "![](./rag-info-fill.jpg)\n",
    "图11：一个完整的用户信息补全流程示意图\n",
    "\n",
    "Enrich的方法介绍了一种大模型向用户多次确认需求来补全用户想法的做法。自此，我们假设已经获得了补全过的用户需求，但是由于用户面对的现实问题千变万化，而系统或RAG的知识可能会滞后，对用户问题的理解多少存在一些偏差，我们还可以继续对整个系统进行强化，接下来我们继续介绍“如何让系统更好地理解用户的问题”。\n",
    "\n",
    "- **Multi-Query 多路召回**\n",
    "\n",
    "多路召回的方法不是让大模型进行一次改写然后反复向用户确认，而是让大模型自己解决如何理解用户的问题。所以我们首先一次性改写出多种用户问题，让大模型根据用户提出的问题，从多种不同角度去生成有一定提问角度或提问内容上存在差异的问题。让这些存在差异的问题作为大模型对用户真实需求的猜测，然后再把每个问题分别生成答案，并总结出最终答案。\n",
    "\n",
    "例如：用户问“烤鸭店在哪里？”，大模型会生成：\n",
    "\n",
    "![](./rag-answer.jpg)\n",
    "\n",
    "以下就是能生成多路召回策略的提示词模板，你可以在你的项目里直接使用这段提示词模板，其中{question}就是用户输入的问题，你也可以尝试先翻译成中文再使用：\n",
    ">1.You are an AI language model assistant. Your task is to generate five\n",
    ">\n",
    ">2.different versions of the given user question to retrieve relevant documents from a vector database. By > generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search.\n",
    ">\n",
    ">3.Provide these alternative questions separated by newlines. Original question: {question}\n",
    "\n",
    "- **RAG-Fusion 过滤融合**\n",
    "  \n",
    "在经过多路召回获取了各种语料块之后，并不是将所有检索到的语料块都交给大模型，而是先进行一轮筛选，给检索到的语料块进行去重操作，然后按照与原问题的相关性进行排序，再将语料块打包喂给大模型来生成答案。\n",
    "\n",
    "![](./rag-fusion.jpg)\n",
    "\n",
    "\n",
    "经过去重复语料筛选，节省了传递给大模型的tokens数量，再经过排序，将更有价值的语料块传递给大模型，从而提升答案的生成质量。\n",
    "用我们的例子讲就是，志愿者先从多种角度来理解用户的问题，然后对每个问题都去检索资料，查找有用信息，最后把重复信息去掉，再将获取的资料排序。这就能锁定比较接近用户问题的几段语料了，比如“全聚德烤鸭店的地址”，“天外天烤鸭店的地址”，“郭林家常菜店铺的地址”等等，以及这些烤鸭店分布在后海的那些区域，如何步行走过去等等。\n",
    "\n",
    "- **Step Back 问题摘要**\n",
    "\n",
    "让大模型先对问题进行一轮抽象，从大体上去把握用户的问题，获得一层高级思考下的语料块。\n",
    "这个策略的提示词写作\n",
    "\n",
    ">You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\n",
    ">\n",
    "假如是医疗咨询的场景，用户描述了一大段病情、现象、感受、担忧；或者在法律服务的场景，用户描述了现场情况、事发双方的背景、纠纷的由来等一大段话的时候，我们就可以用这个策略，让大模型先理解一下用户的意图是什么，这个事情大体上看是什么问题。\n",
    "\n",
    "- **Decomposition 问题分解**\n",
    "  \n",
    "这个策略讲究细节，有点像提示词工程中的COT的概念，是把用户的问题拆成一个一个小问题来理解，或者可以说是RAG中的COT。这个策略的提示词如\n",
    ">1.You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    ">\n",
    ">2.The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    ">\n",
    ">3.Generate multiple search queries related to: {question} \\n\n",
    ">\n",
    ">4.Output (3 queries):\n",
    ">\n",
    "使用了这段提示词，大模型生成的子问题如下\n",
    "\n",
    "![](./decomposition.jpg)\n",
    "\n",
    "接下来可以使用并行与串行两个策略来执行子任务。并行执行是将每个子任务抛出去获得一个答案，然后再让大模型把所有子任务的答案汇总起来。串行是依次执行子任务，然后将前一个任务生成的答案作为后一个任务的提示词的一部分。这两种执行策略如下图所示\n",
    "\n",
    "![](./rag-decomposition.jpg)\n",
    "\n",
    "\n",
    "- **HyDE 假设答案**\n",
    "  \n",
    "这个策略让大模型先来根据用户的问题生成一段假设答案，然后用这段假设的答案作为新的问题去文档库里匹配新的文档块，再进行总结，生成最终答案。\n",
    "好比志愿者听到用户的问题“推荐一家烤鸭店”，第一时间想到了“全聚德烤鸭店不错，我前两天刚吃过！”，接下来，志愿者按照自己的思路找到了全聚德烤鸭店的地址，并给用户讲解如何走过去。\n",
    "\n",
    "![](./rag-HyDE.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aefd51-e363-4148-a1c4-6008b2b1e651",
   "metadata": {},
   "source": [
    "### 3.4 改造三：改造检索渠道\n",
    "Corrective Retrieval Augmented Generation (CRAG)是一种改善提取信息质量的策略：如果通过知识库检索得到的信息与用户问题相关性太低，我们就主动搜索互联网，将网络搜索到的信息与知识库搜索到的信息合并，再让大模型进行整理给出最终答案。在工程上我们可以有两种实现方式：\n",
    "1. 向量相似度，我们用检索信息得到的向量相似度分来判断。判断每个语料块与用户问题的相似度评分，是否高过某个阈值，如果搜索到的语料块与用户问题的相似度都比较低，就代表知识库中的信息与用户问题不太相关；\n",
    "2. 直接问大模型，我们可以先将知识库检索到的信息交给大模型，让大模型自主判断，这些资料是否能回答用户的问题。\n",
    "\n",
    "![](./crag.jpg)\n",
    "图12：CRAG原理图\n",
    "\n",
    "采用这个搜索策略，当志愿者遇到一个问题，而手边的资料都不能解答这个问题时，志愿者可以上网搜索答案。比如游客问：“天安门升旗仪式是几点钟？”志愿者可能会打开电脑，搜索一下明天天安门升旗仪式的具体时间，然后再回答给游客。这样，至少能让RAG的回答信息的范围有所扩大，回答质量有了提升。\n",
    "在CRAG的论文中，当面临知识库不完备的情况时，先从互联网下载相关资料再回答的准确率比直接回答的准确率有了较大提升。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db4af3a-81eb-45d9-9755-84c874441e7c",
   "metadata": {},
   "source": [
    "### 3.5 改造四：回答前反复思考\n",
    "Self-RAG，也称为self-reflection，是一种通过在应用中设计反馈路径实现自我反思的策略。基于这个思想，我们可以让应用问自己三个问题：\n",
    "\n",
    "- 相关性：我获取的这些材料和问题相关吗？\n",
    "- 无幻觉：我的答案是不是按照材料写的来讲，还是我自己编造的？\n",
    "- 已解答：我的答案是不是解答了问题？\n",
    "\n",
    "![](./self-rag.jpg)\n",
    "图13：Self-RAG原理图\n",
    "\n",
    "这些判断本身可以通过另一段提示词工程让大模型给出判断，整个项目复杂度有了提升，但回答质量有了保障。\n",
    "志愿者至少会通过反思这三个问题，在回答游客之前，让答案的质量有所提升。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b385984-36e3-4ec6-a35f-f9b81bdf14f4",
   "metadata": {},
   "source": [
    "### 3.6 改造五：从多种数据源中获取资料\n",
    "这个策略涉及系统性的改造数据的存储和获取环节。传统RAG我们只分析文本文档，我们把文档作为字符串存在向量数据库和文档数据库中。但是现实中的知识还有结构化数据以及图知识库。因此，有很多工作者在研究从数据库中通过NL2SQL的方式直接获取与用户问题相关的数据或统计信息；从GraphDB中用NL2Cypher（显然这是在用Neo4J）获取关联知识。这些方法显然将给RAG带来更多新奇的体验。\n",
    "\n",
    "![](./rag-search.jpg)\n",
    "图14：通过大模型搜索数据库来抽取信息\n",
    "\n",
    "- **从数据库中获取统计指标**\n",
    "  \n",
    "大模型可以将用户问题转化为SQL语句去数据库中检索相关信息，这个能力就是NL2SQL（Natural Language to SQL）。如果搜索的问题比较简单，只有单表查询，并获取简单的统计数据如求和、求平均等等，大模型还能稳定地生成正确SQL。如果问题比较复杂涉及多表联合查询，或者涉及复杂的过滤条件，或复杂的排序计算公式，大模型生成SQL的正确性就会下降。我们一般用Spider榜单来评测大模型生成SQL的性能。合理构造提示词调用大模型生成SQL，都可以获得可满足应用的NL2SQL能力。\n",
    "\n",
    "![](./rag-spider.jpg)\n",
    "图15：Spider数据集和“执行正确率”评测榜单，榜单上排名靠前的DAIL-SQL+GPT4+Self-Consistency技术，就是使用先检索相似问题构造Few-Shot提示词，再用GPT4来生成SQL，并添加了多路召回策略的方法\n",
    "\n",
    "- **从知识图谱中获取数据**\n",
    "  \n",
    "Neo4j是一款图数据库引擎，可以为我们提供知识图谱构建和计算服务。在知识图谱上，我们调用各种图分析算法，如标签传播、关键节点发现等等，可以快速检索多度关联关系，挖掘隐藏关系。\n",
    "\n",
    "![](./rag-neo4j.jpg)\n",
    "图16：将用户的问题转化为Neo4j的Cypher查询语句，从知识图谱中获取关键知识\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086cdc44",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "![](./rag-conclusion.jpg)\n",
    "图17：增强RAG能力的多种方法汇总\n",
    "\n",
    "我们可以通过多种办法来提升RAG的性能，在经典RAG框架之上，可以进行技术改造的方法层出不穷，RAG也是当前最活跃的技术话题之一，我们期待着这个 AI 领域未来会有更大的发展。\n",
    "\n",
    "### RAG 的流程\n",
    "\n",
    "- 离线步骤：\n",
    "  1. 文档加载\n",
    "  2. 文档切分\n",
    "  3. 向量化\n",
    "  4. 灌入向量数据库\n",
    "     \n",
    "- 在线步骤：\n",
    "  1. 获得用户问题\n",
    "  2. 用户问题向量化\n",
    "  3. 检索向量数据库\n",
    "  4. 将检索结果和用户问题填入 Prompt 模版\n",
    "  5. 用最终获得的 Prompt 调用 LLM\n",
    "  6. 由 LLM 生成回复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e3dd451-b06e-446c-8834-3e2e6ddf278f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping gradio as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Using cached gradio-4.36.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (5.3.0)\n",
      "Requirement already satisfied: fastapi in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (0.85.1)\n",
      "Requirement already satisfied: ffmpy in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (0.3.2)\n",
      "Requirement already satisfied: gradio-client==1.0.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (1.0.1)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (0.23.4)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (6.4.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (3.7.5)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (1.24.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (3.10.5)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (24.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (2.0.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (10.3.0)\n",
      "Collecting pydantic>=2.0 (from gradio)\n",
      "  Using cached pydantic-2.7.4-py3-none-any.whl.metadata (109 kB)\n",
      "Requirement already satisfied: pydub in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (0.0.9)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (0.4.9)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (0.12.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: urllib3~=2.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (0.30.1)\n",
      "Requirement already satisfied: fsspec in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio-client==1.0.1->gradio) (2024.6.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio-client==1.0.1->gradio) (11.0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from altair<6.0,>=4.2.0->gradio) (4.22.0)\n",
      "Requirement already satisfied: toolz in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: anyio in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from httpx>=0.24.1->gradio) (4.4.0)\n",
      "Requirement already satisfied: certifi in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from httpx>=0.24.1->gradio) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: idna in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from httpx>=0.24.1->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from huggingface-hub>=0.19.3->gradio) (3.15.1)\n",
      "Requirement already satisfied: requests in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from importlib-resources<7.0,>=1.3->gradio) (3.19.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from pydantic>=2.0->gradio) (2.18.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
      "INFO: pip is looking at multiple versions of fastapi to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fastapi (from gradio)\n",
      "  Using cached fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n",
      "  Using cached starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from fastapi->gradio) (0.0.4)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from fastapi->gradio) (5.10.0)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from fastapi->gradio) (2.1.2)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (1.3.10)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.22.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Using cached gradio-4.36.1-py3-none-any.whl (12.3 MB)\n",
      "Using cached pydantic-2.7.4-py3-none-any.whl (409 kB)\n",
      "Using cached fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
      "Using cached starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "Installing collected packages: starlette, pydantic, fastapi, gradio\n",
      "  Attempting uninstall: starlette\n",
      "    Found existing installation: starlette 0.20.4\n",
      "    Uninstalling starlette-0.20.4:\n",
      "      Successfully uninstalled starlette-0.20.4\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.9.0\n",
      "    Uninstalling pydantic-1.9.0:\n",
      "      Successfully uninstalled pydantic-1.9.0\n",
      "  Attempting uninstall: fastapi\n",
      "    Found existing installation: fastapi 0.85.1\n",
      "    Uninstalling fastapi-0.85.1:\n",
      "      Successfully uninstalled fastapi-0.85.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 0.3.29 requires fastapi==0.85.1, but you have fastapi 0.111.0 which is incompatible.\n",
      "chromadb 0.3.29 requires pydantic<2.0,>=1.9, but you have pydantic 2.7.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fastapi-0.111.0 gradio-4.36.1 pydantic-2.7.4 starlette-0.37.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall gradio -y\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cc194e5-a529-4fe4-a08a-822c78a61735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/gradio/utils.py:1000: UserWarning: Expected 3 arguments for function <function process_message at 0x7fa57c67f310>, received 2.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/gradio/utils.py:1004: UserWarning: Expected at least 3 arguments for function <function process_message at 0x7fa57c67f310>, received 2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://5c482192ee1fe26db7.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5c482192ee1fe26db7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 399, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/gradio/route_utils.py\", line 720, in __call__\n",
      "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/gradio/route_utils.py\", line 736, in simple_response\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/middleware/exceptions.py\", line 65, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/routing.py\", line 756, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/routing.py\", line 776, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/routing.py\", line 297, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/routing.py\", line 77, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/routing.py\", line 72, in app\n",
      "    response = await func(request)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/fastapi/routing.py\", line 278, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/gradio/routes.py\", line 1111, in upload_file\n",
      "    form = await multipart_parser.parse()\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/gradio/route_utils.py\", line 603, in parse\n",
      "    async for chunk in self.stream:\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/requests.py\", line 238, in stream\n",
      "    raise ClientDisconnect()\n",
      "starlette.requests.ClientDisconnect\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def process_message(message, history, file):\n",
    "    print(message, history, file)\n",
    "    # 加载pdf\n",
    "    # 切 chuck\n",
    "    # 向量数据库\n",
    "    # query 转向量\n",
    "    # 做搜索召回\n",
    "    # 大模型返回\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.ChatInterface(process_message, multimodal=True)\n",
    "    \n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8b0ad2",
   "metadata": {},
   "source": [
    "## 作业\n",
    "\n",
    "做个自己的 ChatPDF。需求：\n",
    "\n",
    "1. 从本地加载 PDF 文件，基于 PDF 的内容对话\n",
    "2. 其它随意发挥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3063874-19a9-434d-b0b9-e966fb5297bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
