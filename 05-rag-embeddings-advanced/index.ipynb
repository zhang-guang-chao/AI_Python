{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad1acaf7-09a1-4b92-b05a-3e0f0122e808",
   "metadata": {},
   "source": [
    "# æœç´¢å¢å¼ºç”Ÿæˆè¿›é˜¶ï¼ˆRAGï¼ŒRetrieval-Augmented Generationï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d979fde",
   "metadata": {},
   "source": [
    "## ğŸ’¡ è¿™èŠ‚è¯¾ä¼šå¸¦ç»™ä½ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe5e70d",
   "metadata": {},
   "source": [
    "1. RAG ä¸­çš„è¿›é˜¶çŸ¥è¯†\n",
    "1. å¦‚ä½•ä¼˜åŒ–RAGç³»ç»Ÿ\n",
    "1. Gradio ä¸Šä¼ æ–‡ä»¶åŠŸèƒ½\n",
    "\n",
    "å¼€å§‹ä¸Šè¯¾ï¼\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53910daa",
   "metadata": {},
   "source": [
    "## ä¸€ã€å®æˆ˜ RAG ç³»ç»Ÿçš„è¿›é˜¶çŸ¥è¯†\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b88b5e",
   "metadata": {},
   "source": [
    "### 1.1ã€æ–‡æœ¬åˆ†å‰²çš„ç²’åº¦\n",
    "\n",
    "![](./rag-document-split.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e19e6cb",
   "metadata": {},
   "source": [
    "**ç¼ºé™·**\n",
    "\n",
    "1. ç²’åº¦å¤ªå¤§å¯èƒ½å¯¼è‡´æ£€ç´¢ä¸ç²¾å‡†ï¼Œç²’åº¦å¤ªå°å¯èƒ½å¯¼è‡´ä¿¡æ¯ä¸å…¨é¢\n",
    "2. é—®é¢˜çš„ç­”æ¡ˆå¯èƒ½è·¨è¶Šä¸¤ä¸ªç‰‡æ®µ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f010e6c2-b69b-4747-9668-16bc5d574772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: gradio 4.36.1\n",
      "Uninstalling gradio-4.36.1:\n",
      "  Successfully uninstalled gradio-4.36.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: chromadb in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (0.3.29)\n",
      "Requirement already satisfied: pandas>=1.3 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.28 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (2.32.3)\n",
      "Collecting pydantic<2.0,>=1.9 (from chromadb)\n",
      "  Using cached pydantic-1.10.16-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (151 kB)\n",
      "Requirement already satisfied: hnswlib>=0.7 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (0.8.0)\n",
      "Requirement already satisfied: clickhouse-connect>=0.5.7 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (0.7.12)\n",
      "Requirement already satisfied: duckdb>=0.7.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (1.0.0)\n",
      "Collecting fastapi==0.85.1 (from chromadb)\n",
      "  Using cached fastapi-0.85.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (1.24.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (1.16.3)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (4.66.4)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: graphlib-backport>=1.0.3 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb) (1.1.0)\n",
      "Collecting starlette==0.20.4 (from fastapi==0.85.1->chromadb)\n",
      "  Using cached starlette-0.20.4-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from starlette==0.20.4->fastapi==0.85.1->chromadb) (4.4.0)\n",
      "Requirement already satisfied: certifi in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2024.6.2)\n",
      "Requirement already satisfied: urllib3>=1.26 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2.2.2)\n",
      "Requirement already satisfied: pytz in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2024.1)\n",
      "Requirement already satisfied: zstandard in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from clickhouse-connect>=0.5.7->chromadb) (0.22.0)\n",
      "Requirement already satisfied: lz4 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from clickhouse-connect>=0.5.7->chromadb) (4.3.3)\n",
      "Requirement already satisfied: coloredlogs in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb) (24.1)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from pandas>=1.3->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from pandas>=1.3->chromadb) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from requests>=2.28->chromadb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from requests>=2.28->chromadb) (3.7)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from tokenizers>=0.13.2->chromadb) (0.23.4)\n",
      "Requirement already satisfied: click>=7.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.15.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi==0.85.1->chromadb) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi==0.85.1->chromadb) (1.2.1)\n",
      "Using cached fastapi-0.85.1-py3-none-any.whl (55 kB)\n",
      "Using cached starlette-0.20.4-py3-none-any.whl (63 kB)\n",
      "Using cached pydantic-1.10.16-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Installing collected packages: pydantic, starlette, fastapi\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.7.4\n",
      "    Uninstalling pydantic-2.7.4:\n",
      "      Successfully uninstalled pydantic-2.7.4\n",
      "  Attempting uninstall: starlette\n",
      "    Found existing installation: starlette 0.37.2\n",
      "    Uninstalling starlette-0.37.2:\n",
      "      Successfully uninstalled starlette-0.37.2\n",
      "  Attempting uninstall: fastapi\n",
      "    Found existing installation: fastapi 0.111.0\n",
      "    Uninstalling fastapi-0.111.0:\n",
      "      Successfully uninstalled fastapi-0.111.0\n",
      "Successfully installed fastapi-0.85.1 pydantic-1.10.16 starlette-0.20.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pydantic==1.9.0\n",
      "  Using cached pydantic-1.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (121 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from pydantic==1.9.0) (4.12.2)\n",
      "Using cached pydantic-1.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.6 MB)\n",
      "Installing collected packages: pydantic\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.16\n",
      "    Uninstalling pydantic-1.10.16:\n",
      "      Successfully uninstalled pydantic-1.10.16\n",
      "Successfully installed pydantic-1.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall gradio -y\n",
    "!pip install chromadb\n",
    "!pip install pydantic==1.9.0\n",
    "# ISSUE HERE: https://github.com/chroma-core/chroma/issues/774"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e08958-b35c-4a54-ab25-07f229113aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "\n",
    "class MyVectorDBConnector:\n",
    "    def __init__(self, collection_name, embedding_fn):\n",
    "        chroma_client = chromadb.Client(Settings(allow_reset=True))\n",
    "\n",
    "        # ä¸ºäº†æ¼”ç¤ºï¼Œå®é™…ä¸éœ€è¦æ¯æ¬¡ reset()\n",
    "        chroma_client.reset()\n",
    "\n",
    "        # åˆ›å»ºä¸€ä¸ª collection\n",
    "        self.collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "        self.embedding_fn = embedding_fn\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "        '''å‘ collection ä¸­æ·»åŠ æ–‡æ¡£ä¸å‘é‡'''\n",
    "        self.collection.add(\n",
    "            embeddings=self.embedding_fn(documents),  # æ¯ä¸ªæ–‡æ¡£çš„å‘é‡\n",
    "            documents=documents,  # æ–‡æ¡£çš„åŸæ–‡\n",
    "            ids=[f\"id{i}\" for i in range(len(documents))]  # æ¯ä¸ªæ–‡æ¡£çš„ id\n",
    "        )\n",
    "\n",
    "    def search(self, query, top_n):\n",
    "        '''æ£€ç´¢å‘é‡æ•°æ®åº“'''\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=self.embedding_fn([query]),\n",
    "            n_results=top_n\n",
    "        )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "351cf43c-2bce-4fa3-89aa-61ea47173b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, model=\"text-embedding-ada-002\",dimensions=None):\n",
    "    '''å°è£… OpenAI çš„ Embedding æ¨¡å‹æ¥å£ï¼Œæ–‡æ¡£åœ°å€ https://platform.openai.com/docs/guides/embeddings/what-are-embeddings'''\n",
    "    if model == \"text-embedding-ada-002\":\n",
    "        dimensions = None\n",
    "    if dimensions:\n",
    "        data = client.embeddings.create(input=texts, model=model, dimensions=dimensions).data\n",
    "    else:\n",
    "        data = client.embeddings.create(input=texts, model=model).data\n",
    "    return [x.embedding for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b9ccfac-bd62-460b-9cb5-41a6df46dc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(filename, page_numbers=None, min_line_length=1):\n",
    "    '''ä» PDF æ–‡ä»¶ä¸­ï¼ˆæŒ‰æŒ‡å®šé¡µç ï¼‰æå–æ–‡å­—'''\n",
    "    paragraphs = []\n",
    "    buffer = ''\n",
    "    full_text = ''\n",
    "    # æå–å…¨éƒ¨æ–‡æœ¬\n",
    "    for i, page_layout in enumerate(extract_pages(filename)):\n",
    "        # å¦‚æœæŒ‡å®šäº†é¡µç èŒƒå›´ï¼Œè·³è¿‡èŒƒå›´å¤–çš„é¡µ\n",
    "        if page_numbers is not None and i not in page_numbers:\n",
    "            continue\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                full_text += element.get_text() + '\\n'\n",
    "                \n",
    "    # æŒ‰ç©ºè¡Œåˆ†éš”ï¼Œå°†æ–‡æœ¬é‡æ–°ç»„ç»‡æˆæ®µè½\n",
    "    lines = full_text.split('\\n')\n",
    "    for text in lines:\n",
    "        if len(text) >= min_line_length:\n",
    "            buffer += (' '+text) if not text.endswith('-') else text.strip('-')\n",
    "        elif buffer:\n",
    "            paragraphs.append(buffer)\n",
    "            buffer = ''\n",
    "    if buffer:\n",
    "        paragraphs.append(buffer)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e4db04e-1a39-4ef9-b7a9-2cb468e7ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸ºäº†æ¼”ç¤ºæ–¹ä¾¿ï¼Œæˆ‘ä»¬åªå–ä¸¤é¡µï¼ˆç¬¬ä¸€ç« ï¼‰\n",
    "paragraphs = extract_text_from_pdf(\n",
    "    \"llama2.pdf\", \n",
    "    page_numbers=[2, 3], \n",
    "    min_line_length=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8cb66e9-e140-437d-b8b3-1c1e0b55fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())  # è¯»å–æœ¬åœ° .env æ–‡ä»¶ï¼Œé‡Œé¢å®šä¹‰äº† OPENAI_API_KEY\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ebed3d1-6558-4ea4-8b59-339729795c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG_Bot:\n",
    "    def __init__(self, vector_db, llm_api, n_results=2):\n",
    "        self.vector_db = vector_db\n",
    "        self.llm_api = llm_api\n",
    "        self.n_results = n_results\n",
    "\n",
    "    def chat(self, user_query):\n",
    "        # 1. æ£€ç´¢\n",
    "        search_results = self.vector_db.search(user_query, self.n_results)\n",
    "\n",
    "        # 2. æ„å»º Prompt\n",
    "        prompt = build_prompt(\n",
    "            prompt_template, info=search_results['documents'][0], query=user_query)\n",
    "\n",
    "        # 3. è°ƒç”¨ LLM\n",
    "        response = self.llm_api(prompt)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0b161cb-e0a0-4647-8f65-4c6c3863d116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    '''å°è£… openai æ¥å£'''\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,  # æ¨¡å‹è¾“å‡ºçš„éšæœºæ€§ï¼Œ0 è¡¨ç¤ºéšæœºæ€§æœ€å°\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c58e9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä¸€ä¸ªå‘é‡æ•°æ®åº“å¯¹è±¡\n",
    "vector_db = MyVectorDBConnector(\"demo_text_split\", get_embeddings)\n",
    "# å‘å‘é‡æ•°æ®åº“ä¸­æ·»åŠ æ–‡æ¡£\n",
    "vector_db.add_documents(paragraphs)\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªRAGæœºå™¨äºº\n",
    "bot = RAG_Bot(\n",
    "    vector_db,\n",
    "    llm_api=get_completion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96c9904f-17bc-4ba4-88a0-b94c3a77a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(prompt_template, **kwargs):\n",
    "    '''å°† Prompt æ¨¡æ¿èµ‹å€¼'''\n",
    "    prompt = prompt_template\n",
    "    for k, v in kwargs.items():\n",
    "        if isinstance(v, str):\n",
    "            val = v\n",
    "        elif isinstance(v, list) and all(isinstance(elem, str) for elem in v):\n",
    "            val = '\\n'.join(v)\n",
    "        else:\n",
    "            val = str(v)\n",
    "        prompt = prompt.replace(f\"__{k.upper()}__\", val)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c00865d2-979a-4cd5-8005-56c8d5dad826",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "ä½ æ˜¯ä¸€ä¸ªé—®ç­”æœºå™¨äººã€‚\n",
    "ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä¸‹è¿°ç»™å®šçš„å·²çŸ¥ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚\n",
    "ç¡®ä¿ä½ çš„å›å¤å®Œå…¨ä¾æ®ä¸‹è¿°å·²çŸ¥ä¿¡æ¯ã€‚ä¸è¦ç¼–é€ ç­”æ¡ˆã€‚\n",
    "å¦‚æœä¸‹è¿°å·²çŸ¥ä¿¡æ¯ä¸è¶³ä»¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œè¯·ç›´æ¥å›å¤\"æˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜\"ã€‚\n",
    "\n",
    "å·²çŸ¥ä¿¡æ¯:\n",
    "__INFO__\n",
    "\n",
    "ç”¨æˆ·é—®ï¼š\n",
    "__QUERY__\n",
    "\n",
    "è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3e839d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023). Testing conducted to date has been in English and has not â€” and could not â€” cover all scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform safety testing and tuning tailored to their specific applications of the model. We provide a responsible use guideÂ¶ and code examplesâ€– to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of our responsible release strategy can be found in Section 5.3.\n",
      "\n",
      " 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.Â§\n",
      "\n",
      "====å›å¤====\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'æˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query = \"llama 2å¯ä»¥å•†ç”¨å—ï¼Ÿ\"\n",
    "# user_query=\"llama 2 chatæœ‰å¤šå°‘å‚æ•°\"\n",
    "search_results = vector_db.search(user_query, 2)\n",
    "\n",
    "for doc in search_results['documents'][0]:\n",
    "    print(doc+\"\\n\")\n",
    "\n",
    "print(\"====å›å¤====\")\n",
    "bot.chat(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25c532f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Figure 1: Helpfulness human evaluation results for Llama 2-Chat compared to other open-source and closed-source models. Human raters compared model generations on ~4k prompts consisting of both single and multi-turn prompts. The 95% confidence intervals for this evaluation are between 1% and 2%. More details in Section 3.4.2. While reviewing these results, it is important to note that human evaluations can be noisy due to limitations of the prompt set, subjectivity of the review guidelines, subjectivity of individual raters, and the inherent difficulty of comparing generations.\n",
      "\n",
      " Figure 2: Win-rate % for helpfulness andsafety between commercial-licensed baselines and Llama 2-Chat, according to GPT 4. To complement the human evaluation, we used a more capable model, not subject to our own guidance. Green area indicates our model is better according to GPT-4. To remove ties, we used win/(win + loss). The orders in which the model responses are presented to GPT-4 are randomly swapped to alleviate bias.\n",
      "\n",
      " 1 Introduction\n",
      "\n",
      " Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public.\n",
      "\n",
      " The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF). Although the training methodology is simple, high computational requirements have limited the development of LLMs to a few players. There have been public releases of pretrained LLMs (such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla (Hoffmann et al., 2022), but none of these models are suitable substitutes for closed â€œproductâ€ LLMs, such as ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human preferences, which greatly enhances their usability and safety. This step can require significant costs in compute and human annotation, and is often not transparent or easily reproducible, limiting progress within the community to advance AI alignment research.\n",
      "\n",
      " In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge.\n",
      "\n",
      "Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed source models. Human raters judged model generations for safety violations across ~2,000 adversarial prompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is important to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the prompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these safety evaluations are performed using content standards that are likely to be biased towards the Llama 2-Chat models.\n",
      "\n",
      " We are releasing the following models to the general public for research and commercial useâ€¡:\n",
      "\n",
      " 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.Â§\n",
      "\n",
      " 2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release\n",
      "\n",
      " variants of this model with 7B, 13B, and 70B parameters as well.\n",
      "\n",
      " We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023). Testing conducted to date has been in English and has not â€” and could not â€” cover all scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform safety testing and tuning tailored to their specific applications of the model. We provide a responsible use guideÂ¶ and code examplesâ€– to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of our responsible release strategy can be found in Section 5.3.\n",
      "\n",
      " The remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology (Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related work (Section 6), and conclusions (Section 7).\n",
      "\n",
      " â€¡https://ai.meta.com/resources/models-and-libraries/llama/ Â§We are delaying the release of the 34B model due to a lack of time to sufficiently red team. Â¶https://ai.meta.com/llama â€–https://github.com/facebookresearch/llama\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in paragraphs:\n",
    "    print(p+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a31c01",
   "metadata": {},
   "source": [
    "**æ”¹è¿›**: æŒ‰ä¸€å®šç²’åº¦ï¼Œéƒ¨åˆ†é‡å å¼çš„åˆ‡å‰²æ–‡æœ¬ï¼Œä½¿ä¸Šä¸‹æ–‡æ›´å®Œæ•´\n",
    "\n",
    "![](./rag-overlap.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2044d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "\n",
    "\n",
    "def split_text(paragraphs, chunk_size=300, overlap_size=100):\n",
    "    '''æŒ‰æŒ‡å®š chunk_size å’Œ overlap_size äº¤å å‰²æ–‡æœ¬'''\n",
    "    sentences = [s.strip() for p in paragraphs for s in sent_tokenize(p)]\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        chunk = sentences[i]\n",
    "        overlap = ''\n",
    "        prev_len = 0\n",
    "        prev = i - 1\n",
    "        # å‘å‰è®¡ç®—é‡å éƒ¨åˆ†\n",
    "        while prev >= 0 and len(sentences[prev])+len(overlap) <= overlap_size:\n",
    "            overlap = sentences[prev] + ' ' + overlap\n",
    "            prev -= 1\n",
    "        chunk = overlap+chunk\n",
    "        next = i + 1\n",
    "        # å‘åè®¡ç®—å½“å‰chunk\n",
    "        while next < len(sentences) and len(sentences[next])+len(chunk) <= chunk_size:\n",
    "            chunk = chunk + ' ' + sentences[next]\n",
    "            next += 1\n",
    "        chunks.append(chunk)\n",
    "        i = next\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90431eb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "æ­¤å¤„ sent_tokenize ä¸ºé’ˆå¯¹è‹±æ–‡çš„å®ç°ï¼Œé’ˆå¯¹ä¸­æ–‡çš„å®ç°è¯·å‚è€ƒ chinese_utils.py\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04fb4b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_text(paragraphs, 300, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a615c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä¸€ä¸ªå‘é‡æ•°æ®åº“å¯¹è±¡\n",
    "vector_db = MyVectorDBConnector(\"demo_text_split\", get_embeddings)\n",
    "# å‘å‘é‡æ•°æ®åº“ä¸­æ·»åŠ æ–‡æ¡£\n",
    "vector_db.add_documents(chunks)\n",
    "# åˆ›å»ºä¸€ä¸ªRAGæœºå™¨äºº\n",
    "bot = RAG_Bot(\n",
    "    vector_db,\n",
    "    llm_api=get_completion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b939657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are releasing the following models to the general public for research and commercial useâ€¡: 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data.\n",
      "\n",
      "Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well. We believe that the open release of LLMs, when done safely, will be a net benefit to society.\n",
      "\n",
      "====å›å¤====\n",
      "å¯ä»¥å•†ç”¨ã€‚\n"
     ]
    }
   ],
   "source": [
    "user_query = \"llama 2å¯ä»¥å•†ç”¨å—ï¼Ÿ\"\n",
    "# user_query=\"llama 2 chatæœ‰å¤šå°‘å‚æ•°\"\n",
    "\n",
    "search_results = vector_db.search(user_query, 2)\n",
    "for doc in search_results['documents'][0]:\n",
    "    print(doc+\"\\n\")\n",
    "\n",
    "response = bot.chat(user_query)\n",
    "print(\"====å›å¤====\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc0e93e",
   "metadata": {},
   "source": [
    "### 1.2ã€æ£€ç´¢åæ’åºï¼ˆé€‰ï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d33295d",
   "metadata": {},
   "source": [
    "**é—®é¢˜**: æœ‰æ—¶ï¼Œæœ€åˆé€‚çš„ç­”æ¡ˆä¸ä¸€å®šæ’åœ¨æ£€ç´¢çš„æœ€å‰é¢\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5f0624b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023).\n",
      "\n",
      "We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge. Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed source models.\n",
      "\n",
      "In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models.\n",
      "\n",
      "Additionally, these safety evaluations are performed using content standards that are likely to be biased towards the Llama 2-Chat models. We are releasing the following models to the general public for research and commercial useâ€¡: 1.\n",
      "\n",
      "We provide a responsible use guideÂ¶ and code examplesâ€– to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of our responsible release strategy can be found in Section 5.3.\n",
      "\n",
      "====å›å¤====\n",
      "æ ¹æ®å·²çŸ¥ä¿¡æ¯ï¼ŒLlama 2çš„å®‰å…¨æ€§åœ¨è¿›è¡Œå®‰å…¨å‘å¸ƒçš„æƒ…å†µä¸‹å°†å¯¹ç¤¾ä¼šäº§ç”Ÿå‡€ç›Šã€‚ç„¶è€Œï¼Œåƒæ‰€æœ‰LLMsä¸€æ ·ï¼ŒLlama 2ä½œä¸ºä¸€é¡¹æ–°æŠ€æœ¯åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­å­˜åœ¨æ½œåœ¨é£é™©ã€‚ç”±äºæ²¡æœ‰å…·ä½“çš„å®‰å…¨è¯„ä¼°ç»“æœæä¾›ï¼Œæ— æ³•å‡†ç¡®å›ç­”Llama 2çš„å®‰å…¨æ€§é—®é¢˜ã€‚\n"
     ]
    }
   ],
   "source": [
    "user_query = \"how safe is llama 2\"\n",
    "search_results = vector_db.search(user_query, 5)\n",
    "\n",
    "for doc in search_results['documents'][0]:\n",
    "    print(doc+\"\\n\")\n",
    "\n",
    "response = bot.chat(user_query)\n",
    "print(\"====å›å¤====\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d0365e",
   "metadata": {},
   "source": [
    "**æ–¹æ¡ˆ**:\n",
    "\n",
    "1. æ£€ç´¢æ—¶è¿‡å¬å›ä¸€éƒ¨åˆ†æ–‡æœ¬\n",
    "2. é€šè¿‡ä¸€ä¸ªæ’åºæ¨¡å‹å¯¹ query å’Œ document é‡æ–°æ‰“åˆ†æ’åº\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96da7011",
   "metadata": {},
   "source": [
    "<img src=\"sbert-rerank.png\" style=\"margin-left: 0px\" width=600px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9652ae8d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>å¤‡æ³¨ï¼š</b>\n",
    "<div>ç”±äº huggingface è¢«å¢™ï¼Œæˆ‘ä»¬å·²ç»ä¸ºæ‚¨å‡†å¤‡å¥½äº†æœ¬ç« ç›¸å…³æ¨¡å‹ã€‚è¯·ç‚¹å‡»ä»¥ä¸‹ç½‘ç›˜é“¾æ¥è¿›è¡Œä¸‹è½½ï¼š\n",
    "    \n",
    "é“¾æ¥: https://pan.baidu.com/s/1X0kfNKasvWqCLUEEyAvO-Q?pwd=3v6y æå–ç : 3v6y </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ebcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af04250-2516-4bf3-b22c-98919a9373b5",
   "metadata": {},
   "source": [
    "ä½¿ç”¨ CrossEncoder æ¥ä½œä¸ºé‡æ–°æ‰“åˆ†æ’åºçš„æ¨¡å‹ï¼ŒCrossEncoder çš„æ–‡æ¡£åœ°å€ï¼šhttps://www.sbert.net/examples/applications/cross-encoder/README.html\n",
    "\n",
    "<img src=\"./cross-encoder.png\" width=\"600px\"></img>\n",
    "\n",
    "**åŒæ—¶å°†ä¸¤ä¸ªå¥å­ä¼ é€’ç»™ Transformer ç½‘ç»œã€‚å®ƒäº§ç”Ÿä¸€ä¸ªè¾“å‡ºå€¼ï¼Œè¡¨ç¤ºè¾“å…¥å¥å¯¹çš„ç›¸ä¼¼æ€§ï¼Œå€¼è¶Šå¤§è¶Šç›¸è¿‘ã€‚**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff559832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe66adaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.598450660705566\tLlama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well. We believe that the open release of LLMs, when done safely, will be a net benefit to society.\n",
      "\n",
      "-0.4974502623081207\tWe are releasing the following models to the general public for research and commercial useâ€¡: 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_query = \"how safe is llama 2\"\n",
    "\n",
    "scores = model.predict([(user_query, doc)\n",
    "                       for doc in search_results['documents'][0]])\n",
    "# æŒ‰å¾—åˆ†æ’åº\n",
    "sorted_list = sorted(\n",
    "    zip(scores, search_results['documents'][0]), key=lambda x: x[0], reverse=True)\n",
    "for score, doc in sorted_list:\n",
    "    print(f\"{score}\\t{doc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822ef085",
   "metadata": {},
   "source": [
    "### 1.3ã€æ··åˆæ£€ç´¢ï¼ˆHybrid Searchï¼‰ï¼ˆé€‰ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347b74f0",
   "metadata": {},
   "source": [
    "åœ¨**å®é™…ç”Ÿäº§**ä¸­ï¼Œä¼ ç»Ÿçš„å…³é”®å­—æ£€ç´¢ï¼ˆç¨€ç–è¡¨ç¤ºï¼‰ä¸å‘é‡æ£€ç´¢ï¼ˆç¨ å¯†è¡¨ç¤ºï¼‰å„æœ‰ä¼˜åŠ£ã€‚\n",
    "\n",
    "ä¸¾ä¸ªå…·ä½“ä¾‹å­ï¼Œæ¯”å¦‚æ–‡æ¡£ä¸­åŒ…å«å¾ˆé•¿çš„ä¸“æœ‰åè¯ï¼Œå…³é”®å­—æ£€ç´¢å¾€å¾€æ›´ç²¾å‡†è€Œå‘é‡æ£€ç´¢å®¹æ˜“å¼•å…¥æ¦‚å¿µæ··æ·†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caa44bb7-e6c5-46bd-bcc4-9e5c74696e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    '''ä½™å¼¦è·ç¦» -- è¶Šå¤§è¶Šç›¸ä¼¼'''\n",
    "    return dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ea35bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance:\n",
      "0.910667535993348\n",
      "0.8895478505819983\n",
      "0.9039165614288258\n",
      "0.9131441645902687\n"
     ]
    }
   ],
   "source": [
    "# èƒŒæ™¯è¯´æ˜ï¼šåœ¨åŒ»å­¦ä¸­â€œå°ç»†èƒè‚ºç™Œâ€å’Œâ€œéå°ç»†èƒè‚ºç™Œâ€æ˜¯ä¸¤ç§ä¸åŒçš„ç™Œç—‡\n",
    "\n",
    "query = \"éå°ç»†èƒè‚ºç™Œçš„æ‚£è€…\"\n",
    "\n",
    "documents = [\n",
    "    \"ææŸæ‚£æœ‰è‚ºç™Œï¼Œç™Œç»†èƒå·²è½¬ç§»\",\n",
    "    \"åˆ˜æŸè‚ºç™ŒIæœŸ\",\n",
    "    \"å¼ æŸç»è¯Šæ–­ä¸ºéå°ç»†èƒè‚ºç™ŒIIIæœŸ\",\n",
    "    \"å°ç»†èƒè‚ºç™Œæ˜¯è‚ºç™Œçš„ä¸€ç§\"\n",
    "]\n",
    "\n",
    "query_vec = get_embeddings([query])[0]\n",
    "doc_vecs = get_embeddings(documents)\n",
    "\n",
    "print(\"Cosine distance:\")\n",
    "for vec in doc_vecs:\n",
    "    print(cos_sim(query_vec, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35686302",
   "metadata": {},
   "source": [
    "æ‰€ä»¥ï¼Œæœ‰æ—¶å€™æˆ‘ä»¬éœ€è¦ç»“åˆä¸åŒçš„æ£€ç´¢ç®—æ³•ï¼Œæ¥è¾¾åˆ°æ¯”å•ä¸€æ£€ç´¢ç®—æ³•æ›´ä¼˜çš„æ•ˆæœã€‚è¿™å°±æ˜¯**æ··åˆæ£€ç´¢**ã€‚\n",
    "\n",
    "æ··åˆæ£€ç´¢çš„æ ¸å¿ƒæ˜¯ï¼Œç»¼åˆæ–‡æ¡£ $d$ åœ¨ä¸åŒæ£€ç´¢ç®—æ³•ä¸‹çš„æ’åºåæ¬¡ï¼ˆrankï¼‰ï¼Œä¸ºå…¶ç”Ÿæˆæœ€ç»ˆæ’åºã€‚\n",
    "\n",
    "ä¸€ä¸ªæœ€å¸¸ç”¨çš„ç®—æ³•å« **Reciprocal Rank Fusionï¼ˆRRFï¼‰**\n",
    "\n",
    "$rrf(d)=\\sum_{a\\in A}\\frac{1}{k+rank_a(d)}$\n",
    "\n",
    "å…¶ä¸­ $A$ è¡¨ç¤ºæ‰€æœ‰ä½¿ç”¨çš„æ£€ç´¢ç®—æ³•çš„é›†åˆï¼Œ$rank_a(d)$ è¡¨ç¤ºä½¿ç”¨ç®—æ³• $a$ æ£€ç´¢æ—¶ï¼Œæ–‡æ¡£ $d$ çš„æ’åºï¼Œ$k$ æ˜¯ä¸ªå¸¸æ•°ã€‚\n",
    "\n",
    "å¾ˆå¤šå‘é‡æ•°æ®åº“éƒ½æ”¯æŒæ··åˆæ£€ç´¢ï¼Œæ¯”å¦‚ [Weaviate](https://weaviate.io/blog/hybrid-search-explained)ã€[Pinecone](https://www.pinecone.io/learn/hybrid-search-intro/) ç­‰ã€‚ä¹Ÿå¯ä»¥æ ¹æ®ä¸Šè¿°åŸç†è‡ªå·±å®ç°ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e392db",
   "metadata": {},
   "source": [
    "### 1.3.1ã€æˆ‘ä»¬æ‰‹å†™ä¸ªç®€å•çš„ä¾‹å­"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aac040",
   "metadata": {},
   "source": [
    "1. åŸºäºå…³é”®å­—æ£€ç´¢çš„æ’åº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02b3c6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class MyEsConnector:\n",
    "    def __init__(self, es_client, index_name, keyword_fn):\n",
    "        self.es_client = es_client\n",
    "        self.index_name = index_name\n",
    "        self.keyword_fn = keyword_fn\n",
    "    \n",
    "    def add_documents(self, documents):\n",
    "        '''æ–‡æ¡£çŒåº“'''\n",
    "        if self.es_client.indices.exists(index=self.index_name):\n",
    "            self.es_client.indices.delete(index=self.index_name)\n",
    "        self.es_client.indices.create(index=self.index_name)\n",
    "        actions = [\n",
    "            {\n",
    "                \"_index\": self.index_name,\n",
    "                \"_source\": {\n",
    "                    \"keywords\": self.keyword_fn(doc),\n",
    "                    \"text\": doc,\n",
    "                    \"id\": f\"doc_{i}\"\n",
    "                }\n",
    "            }\n",
    "            for i,doc in enumerate(documents)\n",
    "        ]\n",
    "        helpers.bulk(self.es_client, actions)\n",
    "        time.sleep(1)\n",
    "\n",
    "    def search(self, query_string, top_n=3):\n",
    "        '''æ£€ç´¢'''\n",
    "        search_query = {\n",
    "            \"match\": {\n",
    "                \"keywords\": self.keyword_fn(query_string)\n",
    "            }\n",
    "        }\n",
    "        res = self.es_client.search(index=self.index_name, query=search_query, size=top_n)\n",
    "        return { \n",
    "            hit[\"_source\"][\"id\"] : {\n",
    "                \"text\" : hit[\"_source\"][\"text\"],\n",
    "                \"rank\" : i,\n",
    "            }\n",
    "            for i, hit in enumerate(res[\"hits\"][\"hits\"])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4da318e1-54ed-4ee5-a244-e4560fa212aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (0.42.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad4864f0-9cbc-44c3-9f21-28e69b7d3bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from elasticsearch7 import Elasticsearch, helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "667d0d93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.814 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc_2': {'text': 'å¼ æŸç»è¯Šæ–­ä¸ºéå°ç»†èƒè‚ºç™ŒIIIæœŸ', 'rank': 0}, 'doc_0': {'text': 'ææŸæ‚£æœ‰è‚ºç™Œï¼Œç™Œç»†èƒå·²è½¬ç§»', 'rank': 1}, 'doc_3': {'text': 'å°ç»†èƒè‚ºç™Œæ˜¯è‚ºç™Œçš„ä¸€ç§', 'rank': 2}}\n"
     ]
    }
   ],
   "source": [
    "from chinese_utils import to_keywords # ä½¿ç”¨ä¸­æ–‡çš„å…³é”®å­—æå–å‡½æ•°\n",
    "\n",
    "es = Elasticsearch(\n",
    "    hosts=['http://localhost:9200'],  # æœåŠ¡åœ°å€ä¸ç«¯å£\n",
    "    # http_auth=(\"elastic\", \"FKaB1Jpz0Rlw0l6G\"),  # ç”¨æˆ·åï¼Œå¯†ç \n",
    ")\n",
    "\n",
    "# åˆ›å»º ES è¿æ¥å™¨\n",
    "es_connector = MyEsConnector(es, \"demo_es_rrf\", to_keywords)\n",
    "\n",
    "# æ–‡æ¡£çŒåº“\n",
    "es_connector.add_documents(documents)\n",
    "\n",
    "# å…³é”®å­—æ£€ç´¢\n",
    "keyword_search_results = es_connector.search(query, 3)\n",
    "\n",
    "print(keyword_search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e81c9c7",
   "metadata": {},
   "source": [
    "2. åŸºäºå‘é‡æ£€ç´¢çš„æ’åº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccef3f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc_3': {'text': 'å°ç»†èƒè‚ºç™Œæ˜¯è‚ºç™Œçš„ä¸€ç§', 'rank': 0}, 'doc_0': {'text': 'ææŸæ‚£æœ‰è‚ºç™Œï¼Œç™Œç»†èƒå·²è½¬ç§»', 'rank': 1}, 'doc_2': {'text': 'å¼ æŸç»è¯Šæ–­ä¸ºéå°ç»†èƒè‚ºç™ŒIIIæœŸ', 'rank': 2}}\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºå‘é‡æ•°æ®åº“è¿æ¥å™¨\n",
    "vecdb_connector = MyVectorDBConnector(\"demo_vec_rrf\", get_embeddings)\n",
    "\n",
    "# æ–‡æ¡£çŒåº“\n",
    "vecdb_connector.add_documents(documents)\n",
    "\n",
    "# å‘é‡æ£€ç´¢\n",
    "vector_search_results = {\n",
    "    \"doc_\"+str(documents.index(doc)) : {\n",
    "        \"text\" : doc,\n",
    "        \"rank\" : i\n",
    "    }\n",
    "    for i, doc in enumerate(\n",
    "        vecdb_connector.search(query, 3)[\"documents\"][0]\n",
    "    )\n",
    "} # æŠŠç»“æœè½¬æˆè·Ÿä¸Šé¢å…³é”®å­—æ£€ç´¢ç»“æœä¸€æ ·çš„æ ¼å¼\n",
    "\n",
    "print(vector_search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99036ec7",
   "metadata": {},
   "source": [
    "3. åŸºäº RRF çš„èåˆæ’åº\n",
    "\n",
    "   $rrf(d)=\\sum_{a\\in A}\\frac{1}{k+rank_a(d)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "842780c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf(ranks, k=1):\n",
    "    ret = {}\n",
    "    # éå†æ¯æ¬¡çš„æ’åºç»“æœ\n",
    "    for rank in ranks: \n",
    "        # éå†æ’åºä¸­æ¯ä¸ªå…ƒç´ \n",
    "        for id, val in rank.items():\n",
    "            if id not in ret:\n",
    "                ret[id] = { \"score\": 0, \"text\": val[\"text\"] }\n",
    "            # è®¡ç®— RRF å¾—åˆ†\n",
    "            ret[id][\"score\"] += 1.0/(k+val[\"rank\"])\n",
    "    # æŒ‰ RRF å¾—åˆ†æ’åºï¼Œå¹¶è¿”å›\n",
    "    return dict(sorted(ret.items(), key=lambda item: item[1][\"score\"], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93576105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"doc_2\": {\n",
      "        \"score\": 1.3333333333333333,\n",
      "        \"text\": \"å¼ æŸç»è¯Šæ–­ä¸ºéå°ç»†èƒè‚ºç™ŒIIIæœŸ\"\n",
      "    },\n",
      "    \"doc_3\": {\n",
      "        \"score\": 1.3333333333333333,\n",
      "        \"text\": \"å°ç»†èƒè‚ºç™Œæ˜¯è‚ºç™Œçš„ä¸€ç§\"\n",
      "    },\n",
      "    \"doc_0\": {\n",
      "        \"score\": 1.0,\n",
      "        \"text\": \"ææŸæ‚£æœ‰è‚ºç™Œï¼Œç™Œç»†èƒå·²è½¬ç§»\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# èåˆä¸¤æ¬¡æ£€ç´¢çš„æ’åºç»“æœ\n",
    "reranked = rrf([keyword_search_results,vector_search_results])\n",
    "\n",
    "print(json.dumps(reranked,indent=4,ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8c9acc",
   "metadata": {},
   "source": [
    "### 1.4ã€RAG-Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e5320c",
   "metadata": {},
   "source": [
    "RAG-Fusion å°±æ˜¯åˆ©ç”¨äº† RRF çš„åŸç†æ¥æå‡æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚\n",
    "\n",
    "<img src=\"rag-fusion.jpeg\" style=\"margin-left: 0px\" width=600px>\n",
    "\n",
    "åŸå§‹é¡¹ç›®ï¼ˆä¸€æ®µéå¸¸ç®€çŸ­çš„æ¼”ç¤ºä»£ç ï¼‰ï¼šhttps://github.com/Raudaschl/rag-fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96880a42",
   "metadata": {},
   "source": [
    "## äºŒã€å‘é‡æ¨¡å‹çš„æœ¬åœ°éƒ¨ç½²\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e861305c",
   "metadata": {},
   "source": [
    "åˆ©ç”¨ sentence_transformers çš„æœ¬åœ°éƒ¨ç½²èƒ½åŠ›ï¼Œéƒ¨ç½²å‘é‡æ¨¡å‹ã€‚\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>å¤‡æ³¨ï¼š</b>\n",
    "<div>ç”±äº huggingface è¢«å¢™ï¼Œæˆ‘ä»¬å·²ç»ä¸ºæ‚¨å‡†å¤‡å¥½äº†æœ¬ç« ç›¸å…³æ¨¡å‹ã€‚è¯·ç‚¹å‡»ä»¥ä¸‹ç½‘ç›˜é“¾æ¥è¿›è¡Œä¸‹è½½ï¼š\n",
    "    \n",
    "é“¾æ¥: https://pan.baidu.com/s/1X0kfNKasvWqCLUEEyAvO-Q?pwd=3v6y æå–ç : 3v6y </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd689aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#model_name = 'BAAI/bge-large-zh-v1.5' #ä¸­æ–‡\n",
    "model_name = 'moka-ai/m3e-base' #ä¸­è‹±åŒè¯­ï¼Œä½†æ•ˆæœä¸€èˆ¬\n",
    "\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c3dbb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance:\n",
      "0.6958814\n",
      "0.6573525\n",
      "0.6653428\n",
      "0.637189\n",
      "0.69429\n"
     ]
    }
   ],
   "source": [
    "#query = \"å›½é™…äº‰ç«¯\"\n",
    "query = \"global conflicts\"\n",
    "\n",
    "documents = [\n",
    "    \"è”åˆå›½å°±è‹ä¸¹è¾¾å°”å¯Œå°”åœ°åŒºå¤§è§„æ¨¡æš´åŠ›äº‹ä»¶å‘å‡ºè­¦å‘Š\",\n",
    "    \"åœŸè€³å…¶ã€èŠ¬å…°ã€ç‘å…¸ä¸åŒ—çº¦ä»£è¡¨å°†ç»§ç»­å°±ç‘å…¸â€œå…¥çº¦â€é—®é¢˜è¿›è¡Œè°ˆåˆ¤\",\n",
    "    \"æ—¥æœ¬å²é˜œå¸‚é™†ä¸Šè‡ªå«é˜Ÿå°„å‡»åœºå†…å‘ç”Ÿæªå‡»äº‹ä»¶ 3äººå—ä¼¤\",\n",
    "    \"å›½å®¶æ¸¸æ³³ä¸­å¿ƒï¼ˆæ°´ç«‹æ–¹ï¼‰ï¼šæ¢å¤æ¸¸æ³³ã€å¬‰æ°´ä¹å›­ç­‰æ°´ä¸Šé¡¹ç›®è¿è¥\",\n",
    "    \"æˆ‘å›½é¦–æ¬¡åœ¨ç©ºé—´ç«™å¼€å±•èˆ±å¤–è¾å°„ç”Ÿç‰©å­¦æš´éœ²å®éªŒ\",\n",
    "]\n",
    "\n",
    "query_vec = model.encode(query)\n",
    "\n",
    "doc_vecs = [\n",
    "    model.encode(doc)\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "print(\"Cosine distance:\")  # è¶Šå¤§è¶Šç›¸ä¼¼\n",
    "#print(cos_sim(query_vec, query_vec))\n",
    "for vec in doc_vecs:\n",
    "    print(cos_sim(query_vec, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee805cc1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>æ‰©å±•é˜…è¯»ï¼šhttps://github.com/FlagOpen/FlagEmbedding</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ecad9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>åˆ’é‡ç‚¹ï¼š</b>\n",
    "    <ol>\n",
    "        <li>ä¸æ˜¯æ¯ä¸ª Embedding æ¨¡å‹éƒ½å¯¹ä½™å¼¦è·ç¦»å’Œæ¬§æ°è·ç¦»åŒæ—¶æœ‰æ•ˆ</li>\n",
    "        <li>å“ªç§ç›¸ä¼¼åº¦è®¡ç®—æœ‰æ•ˆè¦é˜…è¯»æ¨¡å‹çš„è¯´æ˜ï¼ˆé€šå¸¸éƒ½æ”¯æŒä½™å¼¦è·ç¦»è®¡ç®—ï¼‰</li>\n",
    "    </ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42219b55-7917-4ec7-88db-348c81a5cfa2",
   "metadata": {},
   "source": [
    "## ä¸‰ã€å¦‚ä½•æŒç»­æ”¹è¿›RAGåº”ç”¨æ•ˆæœï¼Ÿï¼ˆå‰å»¶æ‰©å±•ï¼Œç»¼åˆæ€è€ƒï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5a58cc-417f-4c8f-9745-0025311e67b2",
   "metadata": {},
   "source": [
    "éšç€æ·±å…¥ä½¿ç”¨ï¼Œä½ å¯èƒ½å‘ç°ä½ çš„ RAG åº”ç”¨å¯èƒ½åªæ˜¯èƒ½ç”¨äº†ï¼Œä½†è¿˜æœ‰å¾ˆå¤šé—®é¢˜ï¼Œæ¯”å¦‚ï¼š\n",
    "\n",
    "- é—®é¢˜æ¯”è¾ƒæŠ½è±¡æˆ–è€…æ¦‚å¿µæ¯”è¾ƒæ¨¡ç³Šï¼Œå¯¼è‡´å¤§æ¨¡å‹æ²¡æœ‰å‡†ç¡®ç†è§£ä½¿ç”¨è€…çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨è€…é—®â€œå…°å·æ‹‰é¢å»å“ªåƒï¼Ÿâ€ï¼Œä½¿ç”¨è€…æœ¬æ¥æƒ³é—®é™„è¿‘æœ‰æ²¡æœ‰å–å…°å·æ‹‰é¢çš„åº—é“ºï¼›å‡å¦‚çŸ¥è¯†åº“æœç´¢â€œå…°å·â€å’Œâ€œæ‹‰é¢â€ä¹‹åï¼Œç»“æœæ’åºé å‰çš„è¯­æ–™æ˜¯ä½äºâ€œå…°å·â€çš„æ‹‰é¢é¦†åœ°å€ï¼Œè€Œå¤§æ¨¡å‹å‘Šè¯‰ç”¨æˆ·è¦å»ä¹°é£æœºç¥¨æˆ–è€…ç«è½¦ç¥¨ï¼Œå°±æ˜¯ç­”éæ‰€é—®äº†ã€‚é€šå¸¸æˆ‘ä»¬ç”¨æ”¹é€ é—®é¢˜ï¼Œè®©ä½¿ç”¨è€…çš„é—®é¢˜æ›´å¥½ç†è§£çš„ç­–ç•¥æ¥å›é¿è¿™ç§æƒ…å†µã€‚\n",
    "- çŸ¥è¯†åº“æ²¡æœ‰æ£€ç´¢åˆ°é—®é¢˜çš„ç­”æ¡ˆï¼Œè¿™æœ‰å¯èƒ½æ˜¯ç”±äºè¯­æ–™æ•°æ®æ²¡æœ‰åšå¥½æ•´ç†å°±å­˜å…¥çŸ¥è¯†åº“ï¼Œæˆ–è€…æ˜¯æ£€ç´¢ç­–ç•¥æœ‰é—®é¢˜ï¼Œå‚æ•°éœ€è¦è°ƒæ•´å¯¼è‡´çš„ã€‚æ¯”å¦‚ï¼Œåœ¨é‡‡ç”¨â€œKä¸ªæœ€ç›¸ä¼¼æ–‡æ¡£å—â€ä½œä¸ºå›ç­”çš„çŸ¥è¯†è¿™ä¸ªç­–ç•¥ä¸­ï¼Œå¦‚æœKå€¼æ¯”è¾ƒå°ï¼Œé‚£ä¹ˆæœ€ç›¸ä¼¼çš„Kä¸ªæ–‡æ¡£å—ä¸­å¯èƒ½å¹¶ä¸åŒ…å«èƒ½è§£ç­”ç”¨æˆ·é—®é¢˜çš„æœ‰æ•ˆçŸ¥è¯†ï¼Œé‚£ä¹ˆç­”æ¡ˆå¾ˆå¯èƒ½å°±æ˜¯é”™è¯¯çš„ã€‚ä¾‹å¦‚ï¼Œä½œä¸ºæ—…æ¸¸æ‰‹å†Œçš„çŸ¥è¯†åº“ä¸­æœ‰å¤§é‡æ–‡æ®µæ˜¯ å…°å·æ‹‰é¢å¦‚ä½•åˆ¶ä½œçš„èœè°±ã€å…°å·æ‹‰é¢çš„äº§åœ°ã€å…°å·æœ‰å“ªäº›ç‰¹äº§ç­‰ï¼Œåªæœ‰ä¸€ä¸¤æ¡ä¿¡æ¯æè¿°äº†é™„è¿‘æ‹‰é¢é¦†çš„åœ°å€ã€‚é‚£ä¹ˆå½“ç”¨æˆ·è¯¢é—®â€œå…°å·æ‹‰é¢æ€ä¹ˆèµ°ï¼Ÿâ€æ—¶ï¼ŒçŸ¥è¯†åº“æ£€ç´¢åˆ°çš„ä¿¡æ¯å¯èƒ½åªæ˜¯å…°å·æ‹‰é¢çš„é€‰æã€è°ƒå‘³ã€çƒ¹é¥ªæ–¹é¢çš„ä¿¡æ¯ï¼Œè€Œå”¯ç‹¬æ²¡æœ‰æ£€ç´¢åˆ°å‰æ–¹50ç±³å¤„æœ‰ä¸€å®¶å…°å·æ‹‰é¢é¦†ã€‚ç”¨æˆ·ä¹Ÿæ²¡æœ‰åŠæ³•è·å¾—æœ‰æ•ˆçš„ç­”æ¡ˆã€‚\n",
    "- ç¼ºå°‘å¯¹ç­”æ¡ˆåšå…œåº•éªŒè¯çš„æœºåˆ¶ï¼Œå‡è®¾è¿æ°”å¾ˆå¥½ï¼Œå¿—æ„¿è€…ä¸ä»…å¬æ‡‚äº†æ¸¸å®¢çš„é—®é¢˜ï¼Œä¹Ÿæ­£ç¡®æŸ¥æ‰¾åˆ°äº†é™„è¿‘æœ€è¿‘çš„ä¸¤å®¶æ‹‰é¢é¦†çš„ä¿¡æ¯ï¼Œä½†æ˜¯å¿—æ„¿è€…çš„å›ç­”æ–¹å¼æ˜¯â€œå‘åŒ—èµ°200ç±³å°±åˆ°äº†â€ã€‚è¿™æœ‰å¯èƒ½æ˜¯ä¸€ä¸ªæ­£ç¡®çš„ç­”æ¡ˆï¼Œä½†ä¸æ˜¯ä¸€ä¸ªå¥½çš„ç­”æ¡ˆï¼Œå®é™…è€ƒå¯Ÿè¿‡æ™¯åŒºåœ°å½¢åæˆ‘ä»¬å¯èƒ½ä¼šå‘ç°ï¼Œå¿—æ„¿è€…åŒ—é¢æ˜¯åæµ·ï¼Œä½ ä¸å¤ªå¯èƒ½ç©¿è¿‡æ¹–é¢å»ä¸€ä¸ªåœ°æ–¹ã€‚å®é™…è·¯å¾„å¯èƒ½æ˜¯ï¼šå…ˆå‘ä¸œèµ°50åç±³ï¼Œå†å‘åŒ—èµ°ç»•è¿‡åæµ·ï¼Œèµ°åˆ°æ¹–å¯¹é¢å»ï¼Œæ‰èƒ½èµ°åˆ°æ­£ç¡®ä½ç½®ã€‚é‚£è¿™ä¸ªâ€œå‘åŒ—èµ°200ç±³...â€çš„å›ç­”ï¼Œä»å¯¼èˆªçš„è§’åº¦å°±ä¸èƒ½ç®—æ˜¯å‡†ç¡®äº†ã€‚\n",
    "\n",
    "æˆ‘ä»¬ä¼šå‘ç°è®¸å¤šæ”¹è¿›æ ‡å‡† RAG æ¡†æ¶çš„æ–¹æ³•ï¼Œä¸‹é¢æˆ‘ä»¬å°†ä¸€èµ·äº†è§£è¿™äº›æ”¹è¿›æ€è·¯ã€‚\n",
    "\n",
    "![](./rag-promote.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1b45e9-dbf5-4b54-a6f6-7c196a7cc59e",
   "metadata": {},
   "source": [
    "### 3.1 å»ºç«‹è¯„æµ‹æ ‡å‡†\n",
    "ä¸ºäº†æŒç»­æ”¹è¿›æˆ‘ä»¬çš„ RAG åº”ç”¨ï¼Œé¦–è¦ä»»åŠ¡åº”å½“æ˜¯æ„å»ºä¸€å¥—ä¸¥è°¨çš„è¯„æµ‹æŒ‡æ ‡ä½“ç³»ï¼Œå¹¶é‚€è¯·ä¸šåŠ¡é¢†åŸŸä¸“å®¶ä½œä¸ºè¯„æµ‹æ–¹å…±åŒå‚ä¸è¯„æµ‹å·¥ä½œï¼Œæˆ‘ä»¬å¯ä»¥è®¾ç½®ä¸æˆ‘ä»¬ä¸šåŠ¡ç›¸å…³çš„å¤šç§é—®é¢˜åœºæ™¯ï¼Œç³»ç»Ÿæ€§åœ°æ£€æŸ¥ä¸€ä¸ªRAGç³»ç»Ÿååº”å¿«ä¸å¿«ï¼Œå›ç­”å‡†ä¸å‡†ï¼Œæœ‰æ²¡æœ‰ç†è§£ç”¨æˆ·é—®é¢˜çš„æ„å›¾ç­‰æ–¹é¢ã€‚é€šè¿‡ç§‘å­¦å…¨é¢çš„è¯„æµ‹ï¼Œæˆ‘ä»¬å¯ä»¥äº†è§£åˆ°ç³»ç»Ÿåœ¨å“ªäº›åœ°æ–¹åšå¾—å¥½ï¼Œå“ªäº›åœ°æ–¹éœ€è¦æ”¹è¿›ï¼Œä»è€Œå¸®åŠ©å¼€å‘è€…è®©RAGç³»ç»Ÿæ›´å¥½çš„æœåŠ¡äºä¸šåŠ¡éœ€æ±‚ã€‚\n",
    "RAGç³»ç»Ÿä¸€èˆ¬åŒ…æ‹¬æ£€ç´¢å’Œç”Ÿæˆä¸¤ä¸ªæ¨¡å—ï¼Œæˆ‘ä»¬åšè¯„æµ‹æ—¶å°±å¯ä»¥ä»è¿™ä¸¤ä¸ªæ¨¡å—åˆ†åˆ«å…¥æ‰‹å»ºç«‹è¯„ä»·æ ‡å‡†å’Œå®æ–½æ–¹æ³•ï¼Œå½“ç„¶ä½ ä¹Ÿå¯ä»¥ç”¨æœ€ç»ˆæ•ˆæœä¸ºæ ‡å‡†ï¼Œå»ºç«‹ç«¯åˆ°ç«¯çš„è¯„æµ‹ã€‚åœ¨è¯„æµ‹æŒ‡æ ‡è®¾è®¡ä¸Šï¼Œæˆ‘ä»¬ä¸»è¦è€ƒå¯Ÿæ£€ç´¢æ¨¡å—çš„å‡†ç¡®æ€§ï¼Œå¦‚å‡†ç¡®ç‡ã€å¬å›ç‡ã€F1å€¼ç­‰ç­‰ï¼›åœ¨ç”Ÿæˆæ¨¡å—ï¼Œæˆ‘ä»¬ä¸»è¦è€ƒå¯Ÿç”Ÿæˆç­”æ¡ˆçš„ä»·å€¼ï¼Œå¦‚ç›¸å…³æ€§ã€çœŸå®æ€§ç­‰ç­‰ã€‚\n",
    "æˆ‘ä»¬å¯ä»¥å¼•å…¥ä¸šç•Œè®¤å¯çš„ä¸€äº›é€šç”¨è¯„ä¼°ç­–ç•¥ï¼Œæ¯”å¦‚ï¼Œä½ å¯ä»¥å‚è€ƒRagasæåŠçš„è¯„æµ‹çŸ©é˜µæŒ‡å—ï¼Œä½ ä¹Ÿå¯ä»¥å»ºç«‹ä¸€äº›è‡ªå·±çš„è¯„æµ‹æŒ‡æ ‡ï¼Œè¿™äº›è¯„æµ‹æ–¹æ³•å°†ä¼šæœ‰åŠ©äºä½ é‡åŒ–å’Œæ”¹è¿›æ¯ä¸€ä¸ªå­æ¨¡å—çš„è¡¨ç°ã€‚\n",
    "\n",
    "**ç°åœ¨å¤§å®¶å…ˆå¯¹Ragasæœ‰ä¸ªå°è±¡ï¼Œåé¢è®²Langchainè¿˜ä¼šè®²è¿™ä¸ªæ€ä¹ˆè®¡ç®—å’Œä½¿ç”¨**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a98dc8a6-f892-445a-a59c-85147d4868b3",
   "metadata": {},
   "source": [
    "![](./rag-promote.jpg)\n",
    "\n",
    "### 3.2 æ”¹é€ ä¸€ï¼šæå‡ç´¢å¼•å‡†ç¡®ç‡\n",
    "\n",
    "- **ä¼˜åŒ–æ–‡æœ¬è§£æè¿‡ç¨‹**\n",
    "  \n",
    "åœ¨æ„å»ºçŸ¥è¯†åº“çš„æ—¶å€™ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦æ­£ç¡®çš„ä»æ–‡æ¡£ä¸­æå–æœ‰æ•ˆè¯­æ–™ã€‚å› æ­¤ï¼Œä¼˜åŒ–æ–‡æœ¬è§£æçš„è¿‡ç¨‹å¾€å¾€å¯¹æå‡RAGçš„æ€§èƒ½æœ‰å¾ˆå¤§å¸®åŠ©ã€‚ä¾‹å¦‚ï¼Œä»ç½‘é¡µä¸­æå–æœ‰æ•ˆä¿¡æ¯æ—¶ï¼Œæˆ‘ä»¬éœ€è¦åˆ¤æ–­å“ªäº›éƒ¨åˆ†åº”è¯¥è¢«å»æ‰ï¼ˆæ¯”å¦‚é¡µçœ‰é¡µè„šæ ‡ç­¾ï¼‰ï¼Œå“ªäº›éƒ¨åˆ†åº”è¯¥è¢«ä¿ç•™ï¼ˆæ¯”å¦‚å±äºç½‘é¡µå†…å®¹çš„è¡¨æ ¼æ ‡ç­¾ï¼‰ã€‚\n",
    "\n",
    "- **ä¼˜åŒ–chunkåˆ‡åˆ†æ¨¡å¼**\n",
    "  \n",
    "Chunkå°±æ˜¯æ•°æ®æˆ–ä¿¡æ¯çš„ä¸€ä¸ªå°ç‰‡æ®µæˆ–è€…åŒºå—ã€‚å½“ä½ åœ¨å¤„ç†å¤§é‡çš„æ–‡æœ¬ã€æ•°æ®æˆ–çŸ¥è¯†æ—¶ï¼Œå¦‚æœä½ ä¸€æ¬¡æ€§å…¨éƒ¨äº¤ç»™å¤§æ¨¡å‹æ¥é˜…è¯»å’Œå¤„ç†ï¼Œæ•ˆç‡æ˜¯éå¸¸ä½çš„ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬æŠŠå®ƒä»¬åˆ‡åˆ†æˆæ›´å°ã€æ›´æ˜“ç®¡ç†çš„éƒ¨åˆ†ï¼Œè¿™äº›éƒ¨åˆ†å°±æ˜¯chunkã€‚æ¯ä¸ªchunkéƒ½åŒ…å«äº†ä¸€äº›æœ‰ç”¨çš„ä¿¡æ¯ï¼Œè¿™æ ·å½“ç³»ç»Ÿæ ¹æ®ç”¨æˆ·çš„é—®é¢˜å¯»æ‰¾æŸä¸ªçŸ¥è¯†æ—¶ï¼Œå®ƒå¯ä»¥è¿…é€Ÿå®šä½åˆ°ä¸ç­”æ¡ˆç›¸å…³ä¿¡æ¯çš„chunkï¼Œè€Œä¸æ˜¯åœ¨æ•´ä¸ªæ•°æ®åº“ä¸­ç›²ç›®æœç´¢ã€‚å› æ­¤ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„chunkåˆ‡åˆ†ç­–ç•¥ï¼Œç³»ç»Ÿèƒ½å¤Ÿæ›´é«˜æ•ˆåœ°æ£€ç´¢ä¿¡æ¯ï¼Œå°±åƒå›¾ä¹¦é¦†é‡ŒæŒ‰ç…§ç±»åˆ«å’Œæ ‡ç­¾æ•´ç†ä¹¦ç±ä¸€æ ·ï¼Œä½¿å¾—æŸ¥æ‰¾ç‰¹å®šå†…å®¹å˜å¾—å®¹æ˜“ã€‚ä¼˜åŒ–chunkåˆ‡åˆ†æ¨¡å¼ï¼Œå°±èƒ½åŠ é€Ÿä¿¡æ¯æ£€ç´¢ã€æå‡å›ç­”è´¨é‡å’Œç”Ÿæˆæ•ˆç‡ã€‚å…·ä½“æ–¹æ³•æœ‰å¾ˆå¤šï¼š\n",
    "\n",
    "    1. åˆ©ç”¨é¢†åŸŸçŸ¥è¯†ï¼šé’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„æ–‡æ¡£ï¼Œåˆ©ç”¨é¢†åŸŸä¸“æœ‰çŸ¥è¯†è¿›è¡Œæ›´ç²¾å‡†çš„åˆ‡åˆ†ã€‚ä¾‹å¦‚ï¼Œåœ¨æ³•å¾‹æ–‡æ¡£ä¸­è¯†åˆ«æ®µè½ç¼–å·ã€æ¡æ¬¾ä½œä¸ºåˆ‡åˆ†ä¾æ®ã€‚\n",
    "    2. åŸºäºå›ºå®šå¤§å°åˆ‡åˆ†ï¼šæ¯”å¦‚é»˜è®¤é‡‡ç”¨128ä¸ªè¯æˆ–512ä¸ªè¯åˆ‡åˆ†ä¸ºä¸€ä¸ªchunkï¼Œå¯ä»¥å¿«é€Ÿå®ç°æ–‡æœ¬åˆ†å—ã€‚ç¼ºç‚¹æ˜¯å¿½ç•¥äº†è¯­ä¹‰å’Œä¸Šä¸‹æ–‡å®Œæ•´æ€§ã€‚\n",
    "    3. ä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼šåœ¨åˆ‡åˆ†æ—¶è€ƒè™‘å‰åæ–‡å…³ç³»ï¼Œé¿å…ä¿¡æ¯æ–­è£‚ã€‚å¯ä»¥é€šè¿‡ä¿æŒç‰¹å®šå¥å¯¹æˆ–çŸ­è¯­ç›¸é‚»ï¼Œæˆ–ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•è¯†åˆ«å¹¶ä¿ç•™è¯­ä¹‰å®Œæ•´æ€§ã€‚æœ€ç®€å•çš„åšæ³•æ˜¯åˆ‡åˆ†æ—¶ä¿ç•™å‰ä¸€å¥å’Œåä¸€å¥è¯ã€‚ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯è¯†åˆ«è¯­ä¹‰å•å…ƒï¼Œå¦‚é€šè¿‡å¥å­ç›¸ä¼¼åº¦è®¡ç®—ã€ä¸»é¢˜æ¨¡å‹ï¼ˆå¦‚LDAï¼‰æˆ–BERTåµŒå…¥èšç±»æ¥åˆ‡åˆ†æ–‡æœ¬ï¼Œç¡®ä¿æ¯ä¸ªchunkå†…éƒ¨è¯­ä¹‰è¿è´¯ï¼Œå‡å°‘è·¨chunkä¿¡æ¯ä¾èµ–ã€‚é€šä¹‰å®éªŒå®¤æä¾›äº†ä¸€ç§æ–‡æœ¬åˆ‡å‰²æ¨¡å‹ï¼Œè¾“å…¥é•¿æ–‡æœ¬å³å¯å¾—åˆ°åˆ‡å‰²å¥½çš„æ–‡æœ¬å—ï¼Œè¯¦æƒ…å¯å‚è€ƒï¼šä¸­æ–‡æ–‡æœ¬åˆ†å‰²æ¨¡å‹ã€‚\n",
    "\n",
    "ä»¥ä¸Šä»‹ç»äº†ä¸€äº›å¸¸è§ç­–ç•¥ï¼Œä½ ä¹Ÿå¯ä»¥è€ƒè™‘ä½¿ç”¨æ›´å¤æ‚çš„åˆ‡åˆ†ç­–ç•¥ï¼Œå¦‚å›´ç»•å…³é”®è¯åˆ‡åˆ†æˆ–è€…é‡‡ç”¨åŠ¨æ€è°ƒæ•´çš„åˆ‡åˆ†ç­–ç•¥ç­‰ï¼Œä¸»è¦ç›®çš„æ˜¯ä¸ºäº†ä¿è¯æ¯ä¸ªchunkä¸­ä¿¡æ¯çš„å®Œæ•´æ€§ï¼Œæ›´å¥½çš„æœåŠ¡ç³»ç»Ÿæå‡æ£€ç´¢è´¨é‡ã€‚\n",
    "\n",
    "- **å¥å­æ»‘åŠ¨çª—å£æ£€ç´¢**\n",
    "  \n",
    "è¿™ä¸ªç­–ç•¥æ˜¯é€šè¿‡è®¾ç½®window_sizeï¼ˆçª—å£å¤§å°ï¼‰æ¥è°ƒæ•´æå–å¥å­çš„æ•°é‡ï¼Œå½“ç”¨æˆ·çš„é—®é¢˜åŒ¹é…åˆ°ä¸€ä¸ªchunkè¯­æ–™å—æ—¶ï¼Œé€šè¿‡çª—å‡½æ•°æå–ç›®æ ‡è¯­æ–™å—çš„ä¸Šä¸‹æ–‡ï¼Œè€Œä¸ä»…ä»…æ˜¯è¯­æ–™å—æœ¬èº«ï¼Œè¿™æ ·æ¥è·å¾—æ›´å®Œæ•´çš„è¯­æ–™ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæå‡RAGç”Ÿæˆè´¨é‡ã€‚\n",
    "\n",
    "![](./rag-context.jpg)\n",
    "å›¾6ï¼šå¥å­æ»‘çª—æ£€ç´¢è·å–æ£€ç´¢åˆ°çš„å¥å­çš„ä¸Šä¸‹æ–‡\n",
    "\n",
    "- **è‡ªåŠ¨åˆå¹¶æ£€ç´¢**\n",
    "\n",
    "è¿™ä¸ªç­–ç•¥æ˜¯å°†æ–‡æ¡£åˆ†å—ï¼Œå»ºæˆä¸€æ£µè¯­æ–™å—çš„æ ‘ï¼Œæ¯”å¦‚1024åˆ‡åˆ†ã€512åˆ‡åˆ†ã€128åˆ‡åˆ†ï¼Œå¹¶æ„é€ å‡ºä¸€æ£µæ–‡æ¡£ç»“æ„æ ‘ã€‚å½“åº”ç”¨ä½œæœç´¢æ—¶ï¼Œå¦‚æœåŒä¸€ä¸ªçˆ¶èŠ‚ç‚¹çš„å¤šä¸ªå¶å­èŠ‚ç‚¹è¢«é€‰ä¸­ï¼Œåˆ™è¿”å›æ•´ä¸ªçˆ¶èŠ‚ç‚¹å¯¹åº”çš„è¯­æ–™å—ã€‚ä»è€Œç¡®ä¿ä¸é—®é¢˜ç›¸å…³çš„è¯­æ–™ä¿¡æ¯è¢«å®Œæ•´ä¿ç•™ä¸‹æ¥ï¼Œä»è€Œå¤§å¹…æå‡RAGçš„ç”Ÿæˆè´¨é‡ã€‚å®æµ‹å‘ç°è¿™ä¸ªæ–¹æ³•æ¯”å¥å­æ»‘åŠ¨çª—å£æ£€ç´¢æ•ˆæœå¥½ã€‚\n",
    "\n",
    "![](./rag-merge.jpg)\n",
    "å›¾7ï¼šè‡ªåŠ¨åˆå¹¶æ£€ç´¢çš„æ–¹æ³•ï¼Œè¿”å›çˆ¶èŠ‚ç‚¹æ–‡æœ¬ä½œä¸ºæ£€ç´¢ç»“æœ\n",
    "\n",
    "\n",
    "- **é€‰æ‹©æ›´é€‚åˆä¸šåŠ¡çš„Embeddingæ¨¡å‹**\n",
    "\n",
    "\n",
    "ç»è¿‡åˆ‡åˆ†çš„è¯­æ–™å—åœ¨æä¾›æ£€ç´¢æœåŠ¡ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦æŠŠchunkè¯­æ–™å—ç”±åŸæ¥çš„æ–‡æœ¬å†…å®¹è½¬æ¢ä¸ºæœºå™¨å¯ä»¥ç”¨äºæ¯”å¯¹è®¡ç®—çš„ä¸€ç»„æ•°å­—ï¼Œå³å˜ä¸ºEmbeddingå‘é‡ã€‚æˆ‘ä»¬é€šè¿‡Embeddingæ¨¡å‹æ¥è¿›è¡Œè¿™ä¸ªè½¬æ¢ã€‚ä½†æ˜¯ï¼Œç”±äºä¸åŒçš„Embeddingæ¨¡å‹å¯¹äºç”ŸæˆEmbeddingå‘é‡è´¨é‡çš„å½±å“å¾ˆå¤§ï¼Œå¥½çš„Embeddingæ¨¡å‹å¯ä»¥æå‡æ£€ç´¢çš„å‡†ç¡®ç‡ã€‚\n",
    "æ¯”å¦‚ï¼Œé’ˆå¯¹ä¸­æ–‡æ£€ç´¢çš„åœºæ™¯ï¼Œæˆ‘ä»¬åº”å½“é€‰æ‹©åœ¨ä¸­æ–‡è¯­æ–™ä¸Šè¡¨ç°æ›´å¥½çš„æ¨¡å‹ã€‚é‚£ä¹ˆé’ˆå¯¹ä½ çš„ä¸šåŠ¡åœºæ™¯ï¼Œä½ ä¹Ÿå¯ä»¥å»ºè®®ä½ çš„æŠ€æœ¯å›¢é˜ŸåšEmbeddingæ¨¡å‹çš„æŠ€æœ¯é€‰å‹ï¼ŒæŒ‘é€‰é’ˆå¯¹ä½ çš„ä¸šåŠ¡åœºæ™¯è¡¨ç°è¾ƒå¥½çš„æ¨¡å‹ã€‚\n",
    "\n",
    "- **é€‰æ‹©æ›´é€‚åˆä¸šåŠ¡çš„ReRankæ¨¡å‹**\n",
    "\n",
    "\n",
    "é™¤äº†ä¼˜åŒ–ç”Ÿæˆå‘é‡çš„è´¨é‡ï¼Œæˆ‘ä»¬è¿˜éœ€è¦åŒæ—¶ä¼˜åŒ–åšå‘é‡æ’åºçš„ReRankæ¨¡å‹ï¼Œå¥½çš„ReRankæ¨¡å‹ä¼šè®©æ›´è´´è¿‘ç”¨æˆ·é—®é¢˜çš„chunksçš„æ’åæ›´é å‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æŒ‘é€‰èƒ½è®©ä½ çš„ä¸šåŠ¡åº”ç”¨è¡¨ç°æ›´å¥½çš„ReRankæ¨¡å‹ã€‚\n",
    "\n",
    "- **Raptor ç”¨èšç±»ä¸ºæ–‡æ¡£å—å»ºç«‹ç´¢å¼•**\n",
    "\n",
    "\n",
    "è¿˜æœ‰ä¸€ç±»æœ‰æ„æ€çš„åšæ³•æ˜¯é‡‡ç”¨æ— ç›‘ç£èšç±»æ¥ç”Ÿæˆæ–‡æ¡£ç´¢å¼•ã€‚è¿™å°±åƒé€šè¿‡æ–‡æ¡£çš„å†…å®¹ä¸ºæ–‡æ¡£è‡ªåŠ¨å»ºç«‹ç›®å½•çš„è¿‡ç¨‹ã€‚å‡å¦‚å¿—æ„¿è€…æ‹¿åˆ°çš„æ–‡æœ¬èµ„æ–™æ˜¯æ²¡æœ‰ç›®å½•çš„ï¼Œå¿—æ„¿è€…ä¸€é¡µä¸€é¡µæŸ¥æ‰¾èµ„æ–™å¿…ç„¶å¾ˆæ…¢ã€‚å› æ­¤ï¼Œå¯ä»¥å°†è¯æ¡ä¿¡æ¯èšç±»ï¼Œæ¯”å¦‚æŒ‰ç…§å•†åº—ã€å…¬å›­ã€é…’å§ã€å’–å•¡åº—ã€ä¸­é¤é¦†ã€å¿«é¤åº—ç­‰æ–¹å¼è¿›è¡Œåˆ†ç»„ï¼Œå»ºç«‹ç›®å½•ï¼Œå†æ ¹æ®æ±‰è¯­æ‹¼éŸ³å­—æ¯æ¥æ’åºã€‚è¿™æ ·å¿—æ„¿è€…æ¥æŸ¥æ‰¾ä¿¡æ¯çš„æ—¶å€™å°±å¯ä»¥æ›´å¿«é€Ÿåœ°è¿›è¡Œå®šä½ã€‚\n",
    "\n",
    "![](./raptor.jpg)\n",
    "å›¾8ï¼šRaptor ç”¨èšç±»ä¸ºæ–‡æ¡£å—å»ºç«‹ç´¢å¼•"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae692d64-edd5-4d59-ab89-9ea05c375928",
   "metadata": {},
   "source": [
    "### 3.3 æ”¹é€ äºŒï¼šè®©é—®é¢˜æ›´å¥½ç†è§£\n",
    "æˆ‘ä»¬å¸Œæœ›åšåˆ°èƒ½è®©äººä»¬é€šè¿‡å£è¯­å¯¹è¯æ¥ä½¿ç”¨å¤§æ¨¡å‹åº”ç”¨ã€‚ç„¶è€Œï¼Œäººä»¬åœ¨å£è¯­åŒ–è¡¨è¾¾è‡ªå·±çš„ç›®çš„å’Œæ„å›¾æ—¶ï¼Œå¾€å¾€ä¼šå‡ºç°ä¸€äº›é—®é¢˜ã€‚æ¯”å¦‚ï¼Œé—®é¢˜è¿‡äºç®€å•å«ç³Šå‡ºç°äº†è¯­ä¹‰æ··æ·†ï¼Œå¯¼è‡´å¤§æ¨¡å‹ç†è§£é”™è¯¯ï¼›é—®é¢˜çš„è¦ç´ éå¸¸å¤šï¼Œè€Œç”¨æˆ·åˆè®²å¾—å¤ªå°‘ï¼Œåªèƒ½åœ¨åå¤å¯¹è¯ä¸­ä¸æ–­æ²Ÿé€šè¡¥å…¨ï¼›é—®é¢˜æ¶‰åŠçš„çŸ¥è¯†ç‚¹è¶…å‡ºäº†å¤§æ¨¡å‹è®­ç»ƒè¯­æ–™ï¼Œæˆ–è€…çŸ¥è¯†åº“çš„è¦†ç›–èŒƒå›´ï¼Œå¯¼è‡´å¤§æ¨¡å‹ç¼–é€ äº†ä¸€äº›ä¿¡æ¯æ¥å›ç­”ç­‰ç­‰ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬æœŸæœ›èƒ½åœ¨ç”¨æˆ·æé—®çš„ç¯èŠ‚è¿›è¡Œä»‹å…¥ï¼Œè®©å¤§æ¨¡å‹èƒ½æ›´å¥½çš„ç†è§£ç”¨æˆ·çš„é—®é¢˜ã€‚é’ˆå¯¹è¿™ä¸ªé—®é¢˜è¿›è¡Œå°è¯•çš„è®ºæ–‡å¾ˆå¤šï¼Œæä¾›äº†å¾ˆå¤šæœ‰æ„æ€çš„å®ç°æ€è·¯ï¼Œå¦‚Multi-Queryã€RAG-Fusionã€Decompositionã€Step-backã€HyDEç­‰ç­‰ï¼Œæˆ‘ä»¬ç®€è¦è®²è§£ä¸€ä¸‹è¿™äº›æ–¹æ³•çš„æ€è·¯ã€‚\n",
    "\n",
    "- **Enrich å®Œå–„ç”¨æˆ·é—®é¢˜**\n",
    "  \n",
    "æˆ‘ä»¬é¦–å…ˆä»‹ç»ä¸€ç§æ¯”è¾ƒå®¹æ˜“æƒ³åˆ°çš„æ€è·¯ï¼Œè®©å¤§æ¨¡å‹æ¥å®Œå–„ç”¨æˆ·çš„åŸå§‹é—®é¢˜ï¼Œäº§ç”Ÿä¸€ä¸ªæ›´åˆ©äºç³»ç»Ÿç†è§£çš„å®Œå–„åçš„ç”¨æˆ·é—®é¢˜ï¼Œå†è®©åç»­çš„ç³»ç»Ÿå»æ‰§è¡Œç”¨æˆ·çš„éœ€æ±‚ã€‚é€šè¿‡ç”¨å¤§æ¨¡å‹å¯¹ç”¨æˆ·çš„é—®é¢˜è¿›è¡Œä¸“ä¸šåŒ–æ”¹å†™ï¼Œç‰¹åˆ«æ˜¯åŠ å…¥äº†çŸ¥è¯†åº“çš„æ”¯æŒï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆæ›´ä¸“ä¸šçš„é—®é¢˜ã€‚ä¸‹å›¾å±•ç°äº†ä¸€ç§ç†æƒ³çš„å¯¹ç”¨æˆ·é—®é¢˜çš„Enrichæµç¨‹ã€‚æˆ‘ä»¬é€šè¿‡å¤šè½®å¯¹è¯é€æ­¥ç¡®è®¤ç”¨æˆ·éœ€æ±‚ã€‚\n",
    "\n",
    "    - ä¸€ç§ç†æƒ³çš„é€šè¿‡å¤šè½®å¯¹è¯è¡¥å…¨éœ€æ±‚çš„æ–¹æ¡ˆã€‚è¯¥è®¾æƒ³æ˜¯é€šè¿‡å¤§æ¨¡å‹å¤šæ¬¡ä¸»åŠ¨ä¸ç”¨æˆ·æ²Ÿé€šï¼Œä¸æ–­æ”¶é›†ä¿¡æ¯ï¼Œå®Œå–„å¯¹ç”¨æˆ·çœŸå®æ„å›¾çš„ç†è§£ï¼Œè¡¥å…¨æ‰§è¡Œç”¨æˆ·éœ€æ±‚æ‰€éœ€çš„å„é¡¹å‚æ•°ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚\n",
    "\n",
    "![](./rag-qa-flow.jpg)\n",
    "å›¾9ï¼šé€šè¿‡å¤šè½®å¯¹è¯å®Œå–„ç”¨æˆ·é—®é¢˜çš„å·¥ä½œæµ\n",
    "\n",
    "ä»¥ä¸‹å±•ç¤ºä¸€ä¸ªé€šè¿‡å¤šè½®å¯¹è¯æ¥è¡¥å…¨ç”¨æˆ·é—®é¢˜çš„æ¡ˆä¾‹ï¼š\n",
    "\n",
    "![](./rag-chat-example.jpg)\n",
    "\n",
    "ä½†æ˜¯åœ¨å®é™…çš„ç”Ÿäº§ä¸­ï¼Œä¸€æ–¹é¢ç”¨æˆ·å¯èƒ½æ²¡æœ‰é‚£ä¹ˆå¤§çš„è€æ€§åå¤æä¾›ç¨‹åºéœ€è¦çš„ä¿¡æ¯ï¼Œå¦ä¸€æ–¹é¢å¼€å‘è€…ä¹Ÿéœ€è¦è€ƒè™‘å¦‚ä½•ç»ˆæ­¢ä¿¡æ¯é‡‡é›†å¯¹è¯ï¼Œæ¯”å¦‚è®©å¤§æ¨¡å‹è¾“å‡ºåœæ­¢è¯­â€œ<EOS>ï¼ˆEnd Of Sentenceï¼‰â€ã€‚æ‰€ä»¥åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬éœ€è¦é‡‡ç”¨ä¸€äº›æ›´å®¹æ˜“å®ç°çš„æ–¹æ¡ˆã€‚\n",
    "\n",
    "    - è®©å¤§æ¨¡å‹è½¬è¿°ç”¨æˆ·é—®é¢˜ï¼Œå†è¿›è¡ŒRAGé—®ç­”ã€‚å‚è€ƒâ€œæŒ‡ä»¤æç¤ºè¯â€çš„æ€è·¯ï¼Œæˆ‘ä»¬å¯ä»¥è®©å¤§æ¨¡å‹æ¥è½¬è¿°ç”¨æˆ·çš„é—®é¢˜ï¼Œå°†ç”¨æˆ·çš„é—®é¢˜æ ‡å‡†åŒ–ï¼Œè§„èŒƒåŒ–ã€‚è¿™é‡Œæˆ‘ä»¬å¯ä»¥æä¾›ä¸€å¥—æ ‡å‡†æç¤ºè¯æ¨¡æ¿ï¼Œæä¾›ä¸€äº›æ ‡å‡†åŒ–çš„ç¤ºä¾‹ï¼Œä¹Ÿå¯ä»¥ç”¨çŸ¥è¯†åº“æ¥å¢å¼ºã€‚æˆ‘ä»¬çš„ä¸»è¦ç›®çš„æ˜¯è§„èŒƒç”¨æˆ·çš„è¾“å…¥è¯·æ±‚ï¼Œå†ç”ŸæˆRAGæŸ¥è¯¢æŒ‡ä»¤ï¼Œä»è€Œæå‡RAGæŸ¥è¯¢è´¨é‡ã€‚\n",
    "\n",
    "![](./rag-qfill.jpg)\n",
    "å›¾10ï¼šè®©å¤§æ¨¡å‹æ ¹æ®çŸ¥è¯†åº“æ¥è¡¥å…¨ç”¨æˆ·è¯·æ±‚\n",
    "\n",
    "    - è®©ç”¨æˆ·è¡¥å…¨ä¿¡æ¯è¾…åŠ©ä¸šåŠ¡è°ƒç”¨ã€‚æœ‰ä¸€äº›åº”ç”¨åœºæ™¯éœ€è¦å¤§é‡çš„å‚æ•°æ”¯æ’‘ï¼Œï¼ˆæ¯”å¦‚è®¢ç«è½¦ç¥¨éœ€è¦èµ·ç‚¹ã€ç»ˆç‚¹ã€æ—¶é—´ã€åº§ä½ç­‰çº§ã€åº§ä½åå¥½ç­‰ç­‰ï¼‰ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥è¿›ä¸€æ­¥å®Œå–„ä¸Šé¢çš„æ€è·¯ï¼Œä¸€æ¬¡æ€§å‘Šè¯‰ç”¨æˆ·ç³»ç»Ÿéœ€è¦ä»€ä¹ˆä¿¡æ¯ï¼Œè®©ç”¨æˆ·æ¥è¡¥å…¨ã€‚é¦–å…ˆï¼Œéœ€è¦å‡†ç¡®ç†è§£ç”¨æˆ·çš„æ„å›¾ï¼Œå®ç°æ„å›¾è¯†åˆ«çš„æ‰‹æ®µå¾ˆå¤šï¼Œå¦‚ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦åŒ¹é…ã€ä½¿ç”¨æœç´¢å¼•æ“ã€æˆ–è€…ç›´æ¥å¤§æ¨¡å‹æ¥æ”¯æŒã€‚å…¶æ¬¡ï¼Œæ ¹æ®ç”¨æˆ·æ„å›¾é€‰æ‹©åˆé€‚çš„ä¸šåŠ¡éœ€æ±‚æ¨¡æ¿ã€‚æ¥ç€ï¼Œè®©å¤§æ¨¡å‹å‚è€ƒä¸šåŠ¡éœ€æ±‚æ¨¡ç‰ˆæ¥ç”Ÿæˆä¸€æ®µå¯¹è¯å‘ç»™ç”¨æˆ·ï¼Œè¯·æ±‚ç”¨æˆ·è¡¥å……ä¿¡æ¯ã€‚è¿™æ—¶ï¼Œå¦‚æœç”¨æˆ·è¿›è¡Œäº†ä¿¡æ¯å®Œå–„ï¼Œé‚£ä¹ˆå¤§æ¨¡å‹å°±å¯ä»¥åŸºäºç”¨æˆ·çš„å›å¤ä¿¡æ¯ç»“åˆç”¨æˆ·çš„è¯·æ±‚æ¥ç”Ÿæˆä¸‹ä¸€æ­¥çš„è¡ŒåŠ¨æŒ‡ä»¤ï¼Œæ•´ä¸ªç³»ç»Ÿå°±å¯ä»¥å®ç°åº”ç”¨ç³»ç»Ÿè‡ªåŠ¨å¸®åŠ©ç”¨æˆ·è®¢æœºç¥¨ã€è®¢é…’åº—ï¼Œå®ŒæˆçŸ¥è¯†åº“é—®ç­”ç­‰åº”ç”¨å½¢å¼ã€‚\n",
    "\n",
    "![](./rag-info-fill.jpg)\n",
    "å›¾11ï¼šä¸€ä¸ªå®Œæ•´çš„ç”¨æˆ·ä¿¡æ¯è¡¥å…¨æµç¨‹ç¤ºæ„å›¾\n",
    "\n",
    "Enrichçš„æ–¹æ³•ä»‹ç»äº†ä¸€ç§å¤§æ¨¡å‹å‘ç”¨æˆ·å¤šæ¬¡ç¡®è®¤éœ€æ±‚æ¥è¡¥å…¨ç”¨æˆ·æƒ³æ³•çš„åšæ³•ã€‚è‡ªæ­¤ï¼Œæˆ‘ä»¬å‡è®¾å·²ç»è·å¾—äº†è¡¥å…¨è¿‡çš„ç”¨æˆ·éœ€æ±‚ï¼Œä½†æ˜¯ç”±äºç”¨æˆ·é¢å¯¹çš„ç°å®é—®é¢˜åƒå˜ä¸‡åŒ–ï¼Œè€Œç³»ç»Ÿæˆ–RAGçš„çŸ¥è¯†å¯èƒ½ä¼šæ»åï¼Œå¯¹ç”¨æˆ·é—®é¢˜çš„ç†è§£å¤šå°‘å­˜åœ¨ä¸€äº›åå·®ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ç»§ç»­å¯¹æ•´ä¸ªç³»ç»Ÿè¿›è¡Œå¼ºåŒ–ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬ç»§ç»­ä»‹ç»â€œå¦‚ä½•è®©ç³»ç»Ÿæ›´å¥½åœ°ç†è§£ç”¨æˆ·çš„é—®é¢˜â€ã€‚\n",
    "\n",
    "- **Multi-Query å¤šè·¯å¬å›**\n",
    "\n",
    "å¤šè·¯å¬å›çš„æ–¹æ³•ä¸æ˜¯è®©å¤§æ¨¡å‹è¿›è¡Œä¸€æ¬¡æ”¹å†™ç„¶ååå¤å‘ç”¨æˆ·ç¡®è®¤ï¼Œè€Œæ˜¯è®©å¤§æ¨¡å‹è‡ªå·±è§£å†³å¦‚ä½•ç†è§£ç”¨æˆ·çš„é—®é¢˜ã€‚æ‰€ä»¥æˆ‘ä»¬é¦–å…ˆä¸€æ¬¡æ€§æ”¹å†™å‡ºå¤šç§ç”¨æˆ·é—®é¢˜ï¼Œè®©å¤§æ¨¡å‹æ ¹æ®ç”¨æˆ·æå‡ºçš„é—®é¢˜ï¼Œä»å¤šç§ä¸åŒè§’åº¦å»ç”Ÿæˆæœ‰ä¸€å®šæé—®è§’åº¦æˆ–æé—®å†…å®¹ä¸Šå­˜åœ¨å·®å¼‚çš„é—®é¢˜ã€‚è®©è¿™äº›å­˜åœ¨å·®å¼‚çš„é—®é¢˜ä½œä¸ºå¤§æ¨¡å‹å¯¹ç”¨æˆ·çœŸå®éœ€æ±‚çš„çŒœæµ‹ï¼Œç„¶åå†æŠŠæ¯ä¸ªé—®é¢˜åˆ†åˆ«ç”Ÿæˆç­”æ¡ˆï¼Œå¹¶æ€»ç»“å‡ºæœ€ç»ˆç­”æ¡ˆã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼šç”¨æˆ·é—®â€œçƒ¤é¸­åº—åœ¨å“ªé‡Œï¼Ÿâ€ï¼Œå¤§æ¨¡å‹ä¼šç”Ÿæˆï¼š\n",
    "\n",
    "![](./rag-answer.jpg)\n",
    "\n",
    "ä»¥ä¸‹å°±æ˜¯èƒ½ç”Ÿæˆå¤šè·¯å¬å›ç­–ç•¥çš„æç¤ºè¯æ¨¡æ¿ï¼Œä½ å¯ä»¥åœ¨ä½ çš„é¡¹ç›®é‡Œç›´æ¥ä½¿ç”¨è¿™æ®µæç¤ºè¯æ¨¡æ¿ï¼Œå…¶ä¸­{question}å°±æ˜¯ç”¨æˆ·è¾“å…¥çš„é—®é¢˜ï¼Œä½ ä¹Ÿå¯ä»¥å°è¯•å…ˆç¿»è¯‘æˆä¸­æ–‡å†ä½¿ç”¨ï¼š\n",
    ">1.You are an AI language model assistant. Your task is to generate five\n",
    ">\n",
    ">2.different versions of the given user question to retrieve relevant documents from a vector database. By > generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search.\n",
    ">\n",
    ">3.Provide these alternative questions separated by newlines. Original question: {question}\n",
    "\n",
    "- **RAG-Fusion è¿‡æ»¤èåˆ**\n",
    "  \n",
    "åœ¨ç»è¿‡å¤šè·¯å¬å›è·å–äº†å„ç§è¯­æ–™å—ä¹‹åï¼Œå¹¶ä¸æ˜¯å°†æ‰€æœ‰æ£€ç´¢åˆ°çš„è¯­æ–™å—éƒ½äº¤ç»™å¤§æ¨¡å‹ï¼Œè€Œæ˜¯å…ˆè¿›è¡Œä¸€è½®ç­›é€‰ï¼Œç»™æ£€ç´¢åˆ°çš„è¯­æ–™å—è¿›è¡Œå»é‡æ“ä½œï¼Œç„¶åæŒ‰ç…§ä¸åŸé—®é¢˜çš„ç›¸å…³æ€§è¿›è¡Œæ’åºï¼Œå†å°†è¯­æ–™å—æ‰“åŒ…å–‚ç»™å¤§æ¨¡å‹æ¥ç”Ÿæˆç­”æ¡ˆã€‚\n",
    "\n",
    "![](./rag-fusion.jpg)\n",
    "\n",
    "\n",
    "ç»è¿‡å»é‡å¤è¯­æ–™ç­›é€‰ï¼ŒèŠ‚çœäº†ä¼ é€’ç»™å¤§æ¨¡å‹çš„tokensæ•°é‡ï¼Œå†ç»è¿‡æ’åºï¼Œå°†æ›´æœ‰ä»·å€¼çš„è¯­æ–™å—ä¼ é€’ç»™å¤§æ¨¡å‹ï¼Œä»è€Œæå‡ç­”æ¡ˆçš„ç”Ÿæˆè´¨é‡ã€‚\n",
    "ç”¨æˆ‘ä»¬çš„ä¾‹å­è®²å°±æ˜¯ï¼Œå¿—æ„¿è€…å…ˆä»å¤šç§è§’åº¦æ¥ç†è§£ç”¨æˆ·çš„é—®é¢˜ï¼Œç„¶åå¯¹æ¯ä¸ªé—®é¢˜éƒ½å»æ£€ç´¢èµ„æ–™ï¼ŒæŸ¥æ‰¾æœ‰ç”¨ä¿¡æ¯ï¼Œæœ€åæŠŠé‡å¤ä¿¡æ¯å»æ‰ï¼Œå†å°†è·å–çš„èµ„æ–™æ’åºã€‚è¿™å°±èƒ½é”å®šæ¯”è¾ƒæ¥è¿‘ç”¨æˆ·é—®é¢˜çš„å‡ æ®µè¯­æ–™äº†ï¼Œæ¯”å¦‚â€œå…¨èšå¾·çƒ¤é¸­åº—çš„åœ°å€â€ï¼Œâ€œå¤©å¤–å¤©çƒ¤é¸­åº—çš„åœ°å€â€ï¼Œâ€œéƒ­æ—å®¶å¸¸èœåº—é“ºçš„åœ°å€â€ç­‰ç­‰ï¼Œä»¥åŠè¿™äº›çƒ¤é¸­åº—åˆ†å¸ƒåœ¨åæµ·çš„é‚£äº›åŒºåŸŸï¼Œå¦‚ä½•æ­¥è¡Œèµ°è¿‡å»ç­‰ç­‰ã€‚\n",
    "\n",
    "- **Step Back é—®é¢˜æ‘˜è¦**\n",
    "\n",
    "è®©å¤§æ¨¡å‹å…ˆå¯¹é—®é¢˜è¿›è¡Œä¸€è½®æŠ½è±¡ï¼Œä»å¤§ä½“ä¸Šå»æŠŠæ¡ç”¨æˆ·çš„é—®é¢˜ï¼Œè·å¾—ä¸€å±‚é«˜çº§æ€è€ƒä¸‹çš„è¯­æ–™å—ã€‚\n",
    "è¿™ä¸ªç­–ç•¥çš„æç¤ºè¯å†™ä½œ\n",
    "\n",
    ">You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\n",
    ">\n",
    "å‡å¦‚æ˜¯åŒ»ç–—å’¨è¯¢çš„åœºæ™¯ï¼Œç”¨æˆ·æè¿°äº†ä¸€å¤§æ®µç—…æƒ…ã€ç°è±¡ã€æ„Ÿå—ã€æ‹…å¿§ï¼›æˆ–è€…åœ¨æ³•å¾‹æœåŠ¡çš„åœºæ™¯ï¼Œç”¨æˆ·æè¿°äº†ç°åœºæƒ…å†µã€äº‹å‘åŒæ–¹çš„èƒŒæ™¯ã€çº çº·çš„ç”±æ¥ç­‰ä¸€å¤§æ®µè¯çš„æ—¶å€™ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç”¨è¿™ä¸ªç­–ç•¥ï¼Œè®©å¤§æ¨¡å‹å…ˆç†è§£ä¸€ä¸‹ç”¨æˆ·çš„æ„å›¾æ˜¯ä»€ä¹ˆï¼Œè¿™ä¸ªäº‹æƒ…å¤§ä½“ä¸Šçœ‹æ˜¯ä»€ä¹ˆé—®é¢˜ã€‚\n",
    "\n",
    "- **Decomposition é—®é¢˜åˆ†è§£**\n",
    "  \n",
    "è¿™ä¸ªç­–ç•¥è®²ç©¶ç»†èŠ‚ï¼Œæœ‰ç‚¹åƒæç¤ºè¯å·¥ç¨‹ä¸­çš„COTçš„æ¦‚å¿µï¼Œæ˜¯æŠŠç”¨æˆ·çš„é—®é¢˜æ‹†æˆä¸€ä¸ªä¸€ä¸ªå°é—®é¢˜æ¥ç†è§£ï¼Œæˆ–è€…å¯ä»¥è¯´æ˜¯RAGä¸­çš„COTã€‚è¿™ä¸ªç­–ç•¥çš„æç¤ºè¯å¦‚\n",
    ">1.You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    ">\n",
    ">2.The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    ">\n",
    ">3.Generate multiple search queries related to: {question} \\n\n",
    ">\n",
    ">4.Output (3 queries):\n",
    ">\n",
    "ä½¿ç”¨äº†è¿™æ®µæç¤ºè¯ï¼Œå¤§æ¨¡å‹ç”Ÿæˆçš„å­é—®é¢˜å¦‚ä¸‹\n",
    "\n",
    "![](./decomposition.jpg)\n",
    "\n",
    "æ¥ä¸‹æ¥å¯ä»¥ä½¿ç”¨å¹¶è¡Œä¸ä¸²è¡Œä¸¤ä¸ªç­–ç•¥æ¥æ‰§è¡Œå­ä»»åŠ¡ã€‚å¹¶è¡Œæ‰§è¡Œæ˜¯å°†æ¯ä¸ªå­ä»»åŠ¡æŠ›å‡ºå»è·å¾—ä¸€ä¸ªç­”æ¡ˆï¼Œç„¶åå†è®©å¤§æ¨¡å‹æŠŠæ‰€æœ‰å­ä»»åŠ¡çš„ç­”æ¡ˆæ±‡æ€»èµ·æ¥ã€‚ä¸²è¡Œæ˜¯ä¾æ¬¡æ‰§è¡Œå­ä»»åŠ¡ï¼Œç„¶åå°†å‰ä¸€ä¸ªä»»åŠ¡ç”Ÿæˆçš„ç­”æ¡ˆä½œä¸ºåä¸€ä¸ªä»»åŠ¡çš„æç¤ºè¯çš„ä¸€éƒ¨åˆ†ã€‚è¿™ä¸¤ç§æ‰§è¡Œç­–ç•¥å¦‚ä¸‹å›¾æ‰€ç¤º\n",
    "\n",
    "![](./rag-decomposition.jpg)\n",
    "\n",
    "\n",
    "- **HyDE å‡è®¾ç­”æ¡ˆ**\n",
    "  \n",
    "è¿™ä¸ªç­–ç•¥è®©å¤§æ¨¡å‹å…ˆæ¥æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç”Ÿæˆä¸€æ®µå‡è®¾ç­”æ¡ˆï¼Œç„¶åç”¨è¿™æ®µå‡è®¾çš„ç­”æ¡ˆä½œä¸ºæ–°çš„é—®é¢˜å»æ–‡æ¡£åº“é‡ŒåŒ¹é…æ–°çš„æ–‡æ¡£å—ï¼Œå†è¿›è¡Œæ€»ç»“ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚\n",
    "å¥½æ¯”å¿—æ„¿è€…å¬åˆ°ç”¨æˆ·çš„é—®é¢˜â€œæ¨èä¸€å®¶çƒ¤é¸­åº—â€ï¼Œç¬¬ä¸€æ—¶é—´æƒ³åˆ°äº†â€œå…¨èšå¾·çƒ¤é¸­åº—ä¸é”™ï¼Œæˆ‘å‰ä¸¤å¤©åˆšåƒè¿‡ï¼â€ï¼Œæ¥ä¸‹æ¥ï¼Œå¿—æ„¿è€…æŒ‰ç…§è‡ªå·±çš„æ€è·¯æ‰¾åˆ°äº†å…¨èšå¾·çƒ¤é¸­åº—çš„åœ°å€ï¼Œå¹¶ç»™ç”¨æˆ·è®²è§£å¦‚ä½•èµ°è¿‡å»ã€‚\n",
    "\n",
    "![](./rag-HyDE.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aefd51-e363-4148-a1c4-6008b2b1e651",
   "metadata": {},
   "source": [
    "### 3.4 æ”¹é€ ä¸‰ï¼šæ”¹é€ æ£€ç´¢æ¸ é“\n",
    "Corrective Retrieval Augmented Generation (CRAG)æ˜¯ä¸€ç§æ”¹å–„æå–ä¿¡æ¯è´¨é‡çš„ç­–ç•¥ï¼šå¦‚æœé€šè¿‡çŸ¥è¯†åº“æ£€ç´¢å¾—åˆ°çš„ä¿¡æ¯ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³æ€§å¤ªä½ï¼Œæˆ‘ä»¬å°±ä¸»åŠ¨æœç´¢äº’è”ç½‘ï¼Œå°†ç½‘ç»œæœç´¢åˆ°çš„ä¿¡æ¯ä¸çŸ¥è¯†åº“æœç´¢åˆ°çš„ä¿¡æ¯åˆå¹¶ï¼Œå†è®©å¤§æ¨¡å‹è¿›è¡Œæ•´ç†ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚åœ¨å·¥ç¨‹ä¸Šæˆ‘ä»¬å¯ä»¥æœ‰ä¸¤ç§å®ç°æ–¹å¼ï¼š\n",
    "1. å‘é‡ç›¸ä¼¼åº¦ï¼Œæˆ‘ä»¬ç”¨æ£€ç´¢ä¿¡æ¯å¾—åˆ°çš„å‘é‡ç›¸ä¼¼åº¦åˆ†æ¥åˆ¤æ–­ã€‚åˆ¤æ–­æ¯ä¸ªè¯­æ–™å—ä¸ç”¨æˆ·é—®é¢˜çš„ç›¸ä¼¼åº¦è¯„åˆ†ï¼Œæ˜¯å¦é«˜è¿‡æŸä¸ªé˜ˆå€¼ï¼Œå¦‚æœæœç´¢åˆ°çš„è¯­æ–™å—ä¸ç”¨æˆ·é—®é¢˜çš„ç›¸ä¼¼åº¦éƒ½æ¯”è¾ƒä½ï¼Œå°±ä»£è¡¨çŸ¥è¯†åº“ä¸­çš„ä¿¡æ¯ä¸ç”¨æˆ·é—®é¢˜ä¸å¤ªç›¸å…³ï¼›\n",
    "2. ç›´æ¥é—®å¤§æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆå°†çŸ¥è¯†åº“æ£€ç´¢åˆ°çš„ä¿¡æ¯äº¤ç»™å¤§æ¨¡å‹ï¼Œè®©å¤§æ¨¡å‹è‡ªä¸»åˆ¤æ–­ï¼Œè¿™äº›èµ„æ–™æ˜¯å¦èƒ½å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n",
    "\n",
    "![](./crag.jpg)\n",
    "å›¾12ï¼šCRAGåŸç†å›¾\n",
    "\n",
    "é‡‡ç”¨è¿™ä¸ªæœç´¢ç­–ç•¥ï¼Œå½“å¿—æ„¿è€…é‡åˆ°ä¸€ä¸ªé—®é¢˜ï¼Œè€Œæ‰‹è¾¹çš„èµ„æ–™éƒ½ä¸èƒ½è§£ç­”è¿™ä¸ªé—®é¢˜æ—¶ï¼Œå¿—æ„¿è€…å¯ä»¥ä¸Šç½‘æœç´¢ç­”æ¡ˆã€‚æ¯”å¦‚æ¸¸å®¢é—®ï¼šâ€œå¤©å®‰é—¨å‡æ——ä»ªå¼æ˜¯å‡ ç‚¹é’Ÿï¼Ÿâ€å¿—æ„¿è€…å¯èƒ½ä¼šæ‰“å¼€ç”µè„‘ï¼Œæœç´¢ä¸€ä¸‹æ˜å¤©å¤©å®‰é—¨å‡æ——ä»ªå¼çš„å…·ä½“æ—¶é—´ï¼Œç„¶åå†å›ç­”ç»™æ¸¸å®¢ã€‚è¿™æ ·ï¼Œè‡³å°‘èƒ½è®©RAGçš„å›ç­”ä¿¡æ¯çš„èŒƒå›´æœ‰æ‰€æ‰©å¤§ï¼Œå›ç­”è´¨é‡æœ‰äº†æå‡ã€‚\n",
    "åœ¨CRAGçš„è®ºæ–‡ä¸­ï¼Œå½“é¢ä¸´çŸ¥è¯†åº“ä¸å®Œå¤‡çš„æƒ…å†µæ—¶ï¼Œå…ˆä»äº’è”ç½‘ä¸‹è½½ç›¸å…³èµ„æ–™å†å›ç­”çš„å‡†ç¡®ç‡æ¯”ç›´æ¥å›ç­”çš„å‡†ç¡®ç‡æœ‰äº†è¾ƒå¤§æå‡ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db4af3a-81eb-45d9-9755-84c874441e7c",
   "metadata": {},
   "source": [
    "### 3.5 æ”¹é€ å››ï¼šå›ç­”å‰åå¤æ€è€ƒ\n",
    "Self-RAGï¼Œä¹Ÿç§°ä¸ºself-reflectionï¼Œæ˜¯ä¸€ç§é€šè¿‡åœ¨åº”ç”¨ä¸­è®¾è®¡åé¦ˆè·¯å¾„å®ç°è‡ªæˆ‘åæ€çš„ç­–ç•¥ã€‚åŸºäºè¿™ä¸ªæ€æƒ³ï¼Œæˆ‘ä»¬å¯ä»¥è®©åº”ç”¨é—®è‡ªå·±ä¸‰ä¸ªé—®é¢˜ï¼š\n",
    "\n",
    "- ç›¸å…³æ€§ï¼šæˆ‘è·å–çš„è¿™äº›ææ–™å’Œé—®é¢˜ç›¸å…³å—ï¼Ÿ\n",
    "- æ— å¹»è§‰ï¼šæˆ‘çš„ç­”æ¡ˆæ˜¯ä¸æ˜¯æŒ‰ç…§ææ–™å†™çš„æ¥è®²ï¼Œè¿˜æ˜¯æˆ‘è‡ªå·±ç¼–é€ çš„ï¼Ÿ\n",
    "- å·²è§£ç­”ï¼šæˆ‘çš„ç­”æ¡ˆæ˜¯ä¸æ˜¯è§£ç­”äº†é—®é¢˜ï¼Ÿ\n",
    "\n",
    "![](./self-rag.jpg)\n",
    "å›¾13ï¼šSelf-RAGåŸç†å›¾\n",
    "\n",
    "è¿™äº›åˆ¤æ–­æœ¬èº«å¯ä»¥é€šè¿‡å¦ä¸€æ®µæç¤ºè¯å·¥ç¨‹è®©å¤§æ¨¡å‹ç»™å‡ºåˆ¤æ–­ï¼Œæ•´ä¸ªé¡¹ç›®å¤æ‚åº¦æœ‰äº†æå‡ï¼Œä½†å›ç­”è´¨é‡æœ‰äº†ä¿éšœã€‚\n",
    "å¿—æ„¿è€…è‡³å°‘ä¼šé€šè¿‡åæ€è¿™ä¸‰ä¸ªé—®é¢˜ï¼Œåœ¨å›ç­”æ¸¸å®¢ä¹‹å‰ï¼Œè®©ç­”æ¡ˆçš„è´¨é‡æœ‰æ‰€æå‡ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b385984-36e3-4ec6-a35f-f9b81bdf14f4",
   "metadata": {},
   "source": [
    "### 3.6 æ”¹é€ äº”ï¼šä»å¤šç§æ•°æ®æºä¸­è·å–èµ„æ–™\n",
    "è¿™ä¸ªç­–ç•¥æ¶‰åŠç³»ç»Ÿæ€§çš„æ”¹é€ æ•°æ®çš„å­˜å‚¨å’Œè·å–ç¯èŠ‚ã€‚ä¼ ç»ŸRAGæˆ‘ä»¬åªåˆ†ææ–‡æœ¬æ–‡æ¡£ï¼Œæˆ‘ä»¬æŠŠæ–‡æ¡£ä½œä¸ºå­—ç¬¦ä¸²å­˜åœ¨å‘é‡æ•°æ®åº“å’Œæ–‡æ¡£æ•°æ®åº“ä¸­ã€‚ä½†æ˜¯ç°å®ä¸­çš„çŸ¥è¯†è¿˜æœ‰ç»“æ„åŒ–æ•°æ®ä»¥åŠå›¾çŸ¥è¯†åº“ã€‚å› æ­¤ï¼Œæœ‰å¾ˆå¤šå·¥ä½œè€…åœ¨ç ”ç©¶ä»æ•°æ®åº“ä¸­é€šè¿‡NL2SQLçš„æ–¹å¼ç›´æ¥è·å–ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„æ•°æ®æˆ–ç»Ÿè®¡ä¿¡æ¯ï¼›ä»GraphDBä¸­ç”¨NL2Cypherï¼ˆæ˜¾ç„¶è¿™æ˜¯åœ¨ç”¨Neo4Jï¼‰è·å–å…³è”çŸ¥è¯†ã€‚è¿™äº›æ–¹æ³•æ˜¾ç„¶å°†ç»™RAGå¸¦æ¥æ›´å¤šæ–°å¥‡çš„ä½“éªŒã€‚\n",
    "\n",
    "![](./rag-search.jpg)\n",
    "å›¾14ï¼šé€šè¿‡å¤§æ¨¡å‹æœç´¢æ•°æ®åº“æ¥æŠ½å–ä¿¡æ¯\n",
    "\n",
    "- **ä»æ•°æ®åº“ä¸­è·å–ç»Ÿè®¡æŒ‡æ ‡**\n",
    "  \n",
    "å¤§æ¨¡å‹å¯ä»¥å°†ç”¨æˆ·é—®é¢˜è½¬åŒ–ä¸ºSQLè¯­å¥å»æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œè¿™ä¸ªèƒ½åŠ›å°±æ˜¯NL2SQLï¼ˆNatural Language to SQLï¼‰ã€‚å¦‚æœæœç´¢çš„é—®é¢˜æ¯”è¾ƒç®€å•ï¼Œåªæœ‰å•è¡¨æŸ¥è¯¢ï¼Œå¹¶è·å–ç®€å•çš„ç»Ÿè®¡æ•°æ®å¦‚æ±‚å’Œã€æ±‚å¹³å‡ç­‰ç­‰ï¼Œå¤§æ¨¡å‹è¿˜èƒ½ç¨³å®šåœ°ç”Ÿæˆæ­£ç¡®SQLã€‚å¦‚æœé—®é¢˜æ¯”è¾ƒå¤æ‚æ¶‰åŠå¤šè¡¨è”åˆæŸ¥è¯¢ï¼Œæˆ–è€…æ¶‰åŠå¤æ‚çš„è¿‡æ»¤æ¡ä»¶ï¼Œæˆ–å¤æ‚çš„æ’åºè®¡ç®—å…¬å¼ï¼Œå¤§æ¨¡å‹ç”ŸæˆSQLçš„æ­£ç¡®æ€§å°±ä¼šä¸‹é™ã€‚æˆ‘ä»¬ä¸€èˆ¬ç”¨Spideræ¦œå•æ¥è¯„æµ‹å¤§æ¨¡å‹ç”ŸæˆSQLçš„æ€§èƒ½ã€‚åˆç†æ„é€ æç¤ºè¯è°ƒç”¨å¤§æ¨¡å‹ç”ŸæˆSQLï¼Œéƒ½å¯ä»¥è·å¾—å¯æ»¡è¶³åº”ç”¨çš„NL2SQLèƒ½åŠ›ã€‚\n",
    "\n",
    "![](./rag-spider.jpg)\n",
    "å›¾15ï¼šSpideræ•°æ®é›†å’Œâ€œæ‰§è¡Œæ­£ç¡®ç‡â€è¯„æµ‹æ¦œå•ï¼Œæ¦œå•ä¸Šæ’åé å‰çš„DAIL-SQL+GPT4+Self-ConsistencyæŠ€æœ¯ï¼Œå°±æ˜¯ä½¿ç”¨å…ˆæ£€ç´¢ç›¸ä¼¼é—®é¢˜æ„é€ Few-Shotæç¤ºè¯ï¼Œå†ç”¨GPT4æ¥ç”ŸæˆSQLï¼Œå¹¶æ·»åŠ äº†å¤šè·¯å¬å›ç­–ç•¥çš„æ–¹æ³•\n",
    "\n",
    "- **ä»çŸ¥è¯†å›¾è°±ä¸­è·å–æ•°æ®**\n",
    "  \n",
    "Neo4jæ˜¯ä¸€æ¬¾å›¾æ•°æ®åº“å¼•æ“ï¼Œå¯ä»¥ä¸ºæˆ‘ä»¬æä¾›çŸ¥è¯†å›¾è°±æ„å»ºå’Œè®¡ç®—æœåŠ¡ã€‚åœ¨çŸ¥è¯†å›¾è°±ä¸Šï¼Œæˆ‘ä»¬è°ƒç”¨å„ç§å›¾åˆ†æç®—æ³•ï¼Œå¦‚æ ‡ç­¾ä¼ æ’­ã€å…³é”®èŠ‚ç‚¹å‘ç°ç­‰ç­‰ï¼Œå¯ä»¥å¿«é€Ÿæ£€ç´¢å¤šåº¦å…³è”å…³ç³»ï¼ŒæŒ–æ˜éšè—å…³ç³»ã€‚\n",
    "\n",
    "![](./rag-neo4j.jpg)\n",
    "å›¾16ï¼šå°†ç”¨æˆ·çš„é—®é¢˜è½¬åŒ–ä¸ºNeo4jçš„CypheræŸ¥è¯¢è¯­å¥ï¼Œä»çŸ¥è¯†å›¾è°±ä¸­è·å–å…³é”®çŸ¥è¯†\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086cdc44",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "![](./rag-conclusion.jpg)\n",
    "å›¾17ï¼šå¢å¼ºRAGèƒ½åŠ›çš„å¤šç§æ–¹æ³•æ±‡æ€»\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥é€šè¿‡å¤šç§åŠæ³•æ¥æå‡RAGçš„æ€§èƒ½ï¼Œåœ¨ç»å…¸RAGæ¡†æ¶ä¹‹ä¸Šï¼Œå¯ä»¥è¿›è¡ŒæŠ€æœ¯æ”¹é€ çš„æ–¹æ³•å±‚å‡ºä¸ç©·ï¼ŒRAGä¹Ÿæ˜¯å½“å‰æœ€æ´»è·ƒçš„æŠ€æœ¯è¯é¢˜ä¹‹ä¸€ï¼Œæˆ‘ä»¬æœŸå¾…ç€è¿™ä¸ª AI é¢†åŸŸæœªæ¥ä¼šæœ‰æ›´å¤§çš„å‘å±•ã€‚\n",
    "\n",
    "### RAG çš„æµç¨‹\n",
    "\n",
    "- ç¦»çº¿æ­¥éª¤ï¼š\n",
    "  1. æ–‡æ¡£åŠ è½½\n",
    "  2. æ–‡æ¡£åˆ‡åˆ†\n",
    "  3. å‘é‡åŒ–\n",
    "  4. çŒå…¥å‘é‡æ•°æ®åº“\n",
    "     \n",
    "- åœ¨çº¿æ­¥éª¤ï¼š\n",
    "  1. è·å¾—ç”¨æˆ·é—®é¢˜\n",
    "  2. ç”¨æˆ·é—®é¢˜å‘é‡åŒ–\n",
    "  3. æ£€ç´¢å‘é‡æ•°æ®åº“\n",
    "  4. å°†æ£€ç´¢ç»“æœå’Œç”¨æˆ·é—®é¢˜å¡«å…¥ Prompt æ¨¡ç‰ˆ\n",
    "  5. ç”¨æœ€ç»ˆè·å¾—çš„ Prompt è°ƒç”¨ LLM\n",
    "  6. ç”± LLM ç”Ÿæˆå›å¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e3dd451-b06e-446c-8834-3e2e6ddf278f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping gradio as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Using cached gradio-4.36.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (5.3.0)\n",
      "Requirement already satisfied: fastapi in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (0.85.1)\n",
      "Requirement already satisfied: ffmpy in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (0.3.2)\n",
      "Requirement already satisfied: gradio-client==1.0.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (1.0.1)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (0.23.4)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (6.4.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (3.7.5)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (1.24.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (3.10.5)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (24.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (2.0.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (10.3.0)\n",
      "Collecting pydantic>=2.0 (from gradio)\n",
      "  Using cached pydantic-2.7.4-py3-none-any.whl.metadata (109 kB)\n",
      "Requirement already satisfied: pydub in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (0.0.9)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (0.4.9)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (0.12.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: urllib3~=2.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio) (0.30.1)\n",
      "Requirement already satisfied: fsspec in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio-client==1.0.1->gradio) (2024.6.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from gradio-client==1.0.1->gradio) (11.0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from altair<6.0,>=4.2.0->gradio) (4.22.0)\n",
      "Requirement already satisfied: toolz in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: anyio in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from httpx>=0.24.1->gradio) (4.4.0)\n",
      "Requirement already satisfied: certifi in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from httpx>=0.24.1->gradio) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: idna in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from httpx>=0.24.1->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from huggingface-hub>=0.19.3->gradio) (3.15.1)\n",
      "Requirement already satisfied: requests in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from importlib-resources<7.0,>=1.3->gradio) (3.19.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from pydantic>=2.0->gradio) (2.18.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
      "INFO: pip is looking at multiple versions of fastapi to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fastapi (from gradio)\n",
      "  Using cached fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n",
      "  Using cached starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from fastapi->gradio) (0.0.4)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from fastapi->gradio) (5.10.0)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from fastapi->gradio) (2.1.2)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (1.3.10)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.22.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Using cached gradio-4.36.1-py3-none-any.whl (12.3 MB)\n",
      "Using cached pydantic-2.7.4-py3-none-any.whl (409 kB)\n",
      "Using cached fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
      "Using cached starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "Installing collected packages: starlette, pydantic, fastapi, gradio\n",
      "  Attempting uninstall: starlette\n",
      "    Found existing installation: starlette 0.20.4\n",
      "    Uninstalling starlette-0.20.4:\n",
      "      Successfully uninstalled starlette-0.20.4\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.9.0\n",
      "    Uninstalling pydantic-1.9.0:\n",
      "      Successfully uninstalled pydantic-1.9.0\n",
      "  Attempting uninstall: fastapi\n",
      "    Found existing installation: fastapi 0.85.1\n",
      "    Uninstalling fastapi-0.85.1:\n",
      "      Successfully uninstalled fastapi-0.85.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 0.3.29 requires fastapi==0.85.1, but you have fastapi 0.111.0 which is incompatible.\n",
      "chromadb 0.3.29 requires pydantic<2.0,>=1.9, but you have pydantic 2.7.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fastapi-0.111.0 gradio-4.36.1 pydantic-2.7.4 starlette-0.37.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall gradio -y\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cc194e5-a529-4fe4-a08a-822c78a61735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/gradio/utils.py:1000: UserWarning: Expected 3 arguments for function <function process_message at 0x7fa57c67f310>, received 2.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/gradio/utils.py:1004: UserWarning: Expected at least 3 arguments for function <function process_message at 0x7fa57c67f310>, received 2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://5c482192ee1fe26db7.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5c482192ee1fe26db7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 399, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/gradio/route_utils.py\", line 720, in __call__\n",
      "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/gradio/route_utils.py\", line 736, in simple_response\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/middleware/exceptions.py\", line 65, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/routing.py\", line 756, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/routing.py\", line 776, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/routing.py\", line 297, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/routing.py\", line 77, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/routing.py\", line 72, in app\n",
      "    response = await func(request)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/fastapi/routing.py\", line 278, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/gradio/routes.py\", line 1111, in upload_file\n",
      "    form = await multipart_parser.parse()\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/gradio/route_utils.py\", line 603, in parse\n",
      "    async for chunk in self.stream:\n",
      "  File \"/root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages/starlette/requests.py\", line 238, in stream\n",
      "    raise ClientDisconnect()\n",
      "starlette.requests.ClientDisconnect\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def process_message(message, history, file):\n",
    "    print(message, history, file)\n",
    "    # åŠ è½½pdf\n",
    "    # åˆ‡ chuck\n",
    "    # å‘é‡æ•°æ®åº“\n",
    "    # query è½¬å‘é‡\n",
    "    # åšæœç´¢å¬å›\n",
    "    # å¤§æ¨¡å‹è¿”å›\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.ChatInterface(process_message, multimodal=True)\n",
    "    \n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8b0ad2",
   "metadata": {},
   "source": [
    "## ä½œä¸š\n",
    "\n",
    "åšä¸ªè‡ªå·±çš„ ChatPDFã€‚éœ€æ±‚ï¼š\n",
    "\n",
    "1. ä»æœ¬åœ°åŠ è½½ PDF æ–‡ä»¶ï¼ŒåŸºäº PDF çš„å†…å®¹å¯¹è¯\n",
    "2. å…¶å®ƒéšæ„å‘æŒ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3063874-19a9-434d-b0b9-e966fb5297bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
