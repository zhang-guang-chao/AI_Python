{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f6bb34-c863-4023-bfc6-4a48312e6ff9",
   "metadata": {},
   "source": [
    "## 二、什么是检索增强的生成模型（RAG）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd1c11d-6f61-473b-a35e-a3da141480e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.1、LLM 固有的局限性\n",
    "\n",
    "1. LLM 的知识不是实时的\n",
    "2. LLM 可能不知道你私有的领域/业务知识\n",
    "\n",
    "<img src=\"gpt-llama2.png\" style=\"margin-left: 0px\" width=\"600px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336ab2f8-a3ae-4276-b5cd-9ddaee438f61",
   "metadata": {},
   "source": [
    "### 2.2、检索增强生成\n",
    "\n",
    "天然能想到的，我们自己有产品知识库，有服务手册这些垂直领域的信息，能不能让大模型学会这些垂直领域的信息。\n",
    "我们能想象到的方法有两种：\n",
    "\n",
    "1. 重新训练大模型，把这些垂直领域的数据喂给大模型，让大模型从中学习, 这是微调\n",
    "2. 给大模型添加个外挂的知识库，我们让大模型和这个知识库结合着去给用户回答问题\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>类比：</b>\n",
    "    <li>你可以把这个过程想象成开卷考试。让 LLM 先翻书，再回答问题。这个过程模型本身是不学会知识的。</li>\n",
    "    <li>微调就是闭卷考试，你的先把所有的知识都学会，才能去回答问题。</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287b2ec5-4ab6-4da1-b476-65fba9ea7702",
   "metadata": {},
   "source": [
    "RAG（Retrieval Augmented Generation）顾名思义，通过**检索**的方法来增强**生成模型**的能力。\n",
    "\n",
    "<video src=\"RAG.mp4\" controls=\"controls\" width=800px style=\"margin-left: 0px\"></video>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45014b80-c98f-4697-a0bd-af2eefc74b1c",
   "metadata": {},
   "source": [
    "## 三、RAG 系统的基本搭建流程\n",
    "\n",
    "搭建过程：\n",
    "\n",
    "1. 文档加载，并按一定条件**切割**成片段\n",
    "2. 将切割的文本片段灌入**检索引擎**\n",
    "3. 封装**检索接口**：能从文档里搜索出相关的文档片段\n",
    "4. 构建**调用流程**：Query -> 检索 -> Prompt -> LLM -> 回复\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1146d2-60db-4cb0-8852-db779b163deb",
   "metadata": {},
   "source": [
    "### 3.1、文档的加载与切割\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "318d2aa6-9898-4e81-bc73-26568da0b2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Using cached openai-1.34.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from openai) (4.4.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from openai) (1.10.16)\n",
      "Requirement already satisfied: sniffio in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
      "Requirement already satisfied: certifi in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Using cached openai-1.34.0-py3-none-any.whl (325 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: distro, openai\n",
      "Successfully installed distro-1.9.0 openai-1.34.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fab623c-1bb3-4d0f-8e6b-ab2b321b1b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "  Using cached pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from pdfminer.six) (3.3.2)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six)\n",
      "  Downloading cryptography-42.0.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
      "Using cached pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "Downloading cryptography-42.0.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: cryptography, pdfminer.six\n",
      "Successfully installed cryptography-42.0.8 pdfminer.six-20231228\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 安装 pdf 解析库\n",
    "!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b243f63-21dd-4990-a85c-d859b5950386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ad7e473-1aa7-4dae-8b04-b1e19f62fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(filename, page_numbers=None, min_line_length=1):\n",
    "    '''从 PDF 文件中（按指定页码）提取文字'''\n",
    "    paragraphs = []\n",
    "    buffer = ''\n",
    "    full_text = ''\n",
    "    # 提取全部文本\n",
    "    for i, page_layout in enumerate(extract_pages(filename)):\n",
    "        # 如果指定了页码范围，跳过范围外的页\n",
    "        if page_numbers is not None and i not in page_numbers:\n",
    "            continue\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                full_text += element.get_text() + '\\n'\n",
    "                \n",
    "    # 按空行分隔，将文本重新组织成段落\n",
    "    lines = full_text.split('\\n')\n",
    "    for text in lines:\n",
    "        if len(text) >= min_line_length:\n",
    "            buffer += (' '+text) if not text.endswith('-') else text.strip('-')\n",
    "        elif buffer:\n",
    "            paragraphs.append(buffer)\n",
    "            buffer = ''\n",
    "    if buffer:\n",
    "        paragraphs.append(buffer)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ef36cfd-8694-4a05-a2aa-6a411357c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = extract_text_from_pdf(\"llama2.pdf\", min_line_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2dc5797-4242-4240-8fb0-8accc5109aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "\n",
      " Hugo Touvron∗ Louis Martin† Kevin Stone† Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic Sergey Edunov Thomas Scialom∗\n",
      "\n",
      " GenAI, Meta\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for para in paragraphs[:3]:\n",
    "    print(para+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0144adef-382d-478c-8c39-e9059aaaf5f0",
   "metadata": {},
   "source": [
    "## 3.2、检索引擎\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba25b86-dd3d-4e3d-a0ec-2e6b266d4785",
   "metadata": {},
   "source": [
    "这里我们使用先进的开源搜索引擎 ElasticSearch，它可以实现各种场景下的搜索功能。\n",
    "\n",
    "官方地址：https://www.elastic.co/cn/elasticsearch(有兴趣的同学可以了解)\n",
    "\n",
    "### 安装 ES 服务器\n",
    "\n",
    "安装教程地址 https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html 。\n",
    "（可以使用 cursor 参考学习）\n",
    "\n",
    "安装后，可以通过不同系统的服务状态监测指令查看 ES 运行状态，这里我的 centos 指令为 `service elasticsearch status`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51cd460-5040-4790-9666-483e4bd08552",
   "metadata": {},
   "source": [
    "### 安装 ES 客户端 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dff087d-5a22-4b74-b4b5-699c29fcde10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch7\n",
      "  Using cached elasticsearch7-7.17.9-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting urllib3<2,>=1.21.1 (from elasticsearch7)\n",
      "  Using cached urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Requirement already satisfied: certifi in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from elasticsearch7) (2024.6.2)\n",
      "Using cached elasticsearch7-7.17.9-py2.py3-none-any.whl (386 kB)\n",
      "Using cached urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Installing collected packages: urllib3, elasticsearch7\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.1\n",
      "    Uninstalling urllib3-2.2.1:\n",
      "      Successfully uninstalled urllib3-2.2.1\n",
      "Successfully installed elasticsearch7-7.17.9 urllib3-1.26.18\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install elasticsearch7  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0185dc95-7868-41e3-8569-abb203f3784d",
   "metadata": {},
   "source": [
    "### 安装NLTK（文本处理方法库）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "330bc524-5fe3-49f6-851b-e3ff6a80303d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from nltk) (8.1.7)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.5.15-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from nltk) (4.66.4)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2024.5.15-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (776 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.2/776.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: regex, joblib, nltk\n",
      "Successfully installed joblib-1.4.2 nltk-3.8.1 regex-2024.5.15\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c2185e5-f89b-4852-8f2d-7313d03dcadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch7 import Elasticsearch, helpers\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")  # 屏蔽 ES 的一些Warnings\n",
    "\n",
    "# 下载分词器和停用词库\n",
    "nltk.download('punkt')  # 英文切词、词根、切句等方法\n",
    "nltk.download('stopwords')  # 英文停用词库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69d67927-d3d6-4ee7-ae77-054777a8f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_keywords(input_string):\n",
    "    '''（英文）文本只保留关键字'''\n",
    "    # 使用正则表达式替换所有非字母数字的字符为空格\n",
    "    no_symbols = re.sub(r'[^a-zA-Z0-9\\s]', ' ', input_string)\n",
    "    word_tokens = word_tokenize(no_symbols)\n",
    "    # 加载停用词表\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "    # 去停用词，取词根\n",
    "    filtered_sentence = [ps.stem(w)\n",
    "                         for w in word_tokens if not w.lower() in stop_words]\n",
    "    return ' '.join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ba1c545-f96e-451c-b9a8-ef442f4e6c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mani paramet llama 2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_keywords('how many parameters does llama 2 have?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c90dc1-bf8f-4144-b30a-7b7c2ba5e3ef",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "此处 to_keywords 为针对英文的实现，针对中文的实现请参考 chinese_utils.py\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446db112-51d4-4ee6-9577-bbfb9617c212",
   "metadata": {},
   "source": [
    "将文本灌入检索引擎\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "084c6315-ed89-43dd-84c5-67b59f14b6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(983, [])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 创建Elasticsearch连接\n",
    "es = Elasticsearch(\n",
    "    hosts=['http://localhost:9200'],  # 服务地址与端口\n",
    "    # http_auth=(\"elastic\", \"FKaB1Jpz0Rlw0l6G\"),  # 用户名，密码\n",
    ")\n",
    "\n",
    "# 2. 定义索引名称\n",
    "index_name = \"teacher_demo_index_tmp\"\n",
    "\n",
    "# 3. 如果索引已存在，删除它（仅供演示，实际应用时不需要这步）\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "\n",
    "# 4. 创建索引\n",
    "es.indices.create(index=index_name)\n",
    "\n",
    "# 5. 灌库指令，构建索引\n",
    "actions = [\n",
    "    {\n",
    "        \"_index\": index_name,\n",
    "        \"_source\": {\n",
    "            \"keywords\": to_keywords(para),\n",
    "            \"text\": para\n",
    "        }\n",
    "    }\n",
    "    for para in paragraphs\n",
    "]\n",
    "\n",
    "# 6. 文本灌库\n",
    "helpers.bulk(es, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4edc674-f1d4-4a11-b647-8232d16a9add",
   "metadata": {},
   "source": [
    "实现关键字检索\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "556ea340-01f5-468a-aed8-1bd31207fa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query_string, top_n=3):\n",
    "    # ES 的查询语言\n",
    "    search_query = {\n",
    "        \"match\": {\n",
    "            \"keywords\": to_keywords(query_string)\n",
    "        }\n",
    "    }\n",
    "    res = es.search(index=index_name, query=search_query, size=top_n)\n",
    "    return [hit[\"_source\"][\"text\"] for hit in res[\"hits\"][\"hits\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c086961-3a86-4eab-b027-a16a0d38d6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Llama 2 comes in a range of parameter sizes—7B, 13B, and 70B—as well as pretrained and fine-tuned variations.\n",
      "\n",
      " 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.§\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = search(\"how many parameters does llama 2 have?\", 2)\n",
    "for r in results:\n",
    "    print(r+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f3ff5a-6102-4e4c-90c5-203f57375afa",
   "metadata": {},
   "source": [
    "### 3.3、LLM 接口封装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b31d9f0-f819-40e8-8bf0-55a6c0a8e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "# 加载环境变量\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())  # 读取本地 .env 文件，里面定义了 OPENAI_API_KEY\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ad16e7b-17e1-4d67-9326-061830eacfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    '''封装 openai 接口'''\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,  # 模型输出的随机性，0 表示随机性最小\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1fd487-c3d6-4159-bccf-df735653ad97",
   "metadata": {},
   "source": [
    "### 3.4、Prompt 模板\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "844ca747-3a9b-4bf7-810f-7df06ad1afe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(prompt_template, **kwargs):\n",
    "    '''将 Prompt 模板赋值'''\n",
    "    prompt = prompt_template\n",
    "    for k, v in kwargs.items():\n",
    "        if isinstance(v, str):\n",
    "            val = v\n",
    "        elif isinstance(v, list) and all(isinstance(elem, str) for elem in v):\n",
    "            val = '\\n'.join(v)\n",
    "        else:\n",
    "            val = str(v)\n",
    "        prompt = prompt.replace(f\"__{k.upper()}__\", val)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cec61c52-7284-41ae-9cbf-30a3462ef03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "你是一个问答机器人。\n",
    "你的任务是根据下述给定的已知信息回答用户问题。\n",
    "确保你的回复完全依据下述已知信息。不要编造答案。\n",
    "如果下述已知信息不足以回答用户的问题，请直接回复\"我无法回答您的问题\"。\n",
    "\n",
    "已知信息:\n",
    "__INFO__\n",
    "\n",
    "用户问：\n",
    "__QUERY__\n",
    "\n",
    "请用中文回答用户问题。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82c5c1f5-0d11-4d4a-978d-0bd8159aa1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "你是一个问答机器人。\n",
      "你的任务是根据下述给定的已知信息回答用户问题。\n",
      "确保你的回复完全依据下述已知信息。不要编造答案。\n",
      "如果下述已知信息不足以回答用户的问题，请直接回复\"我无法回答您的问题\"。\n",
      "\n",
      "已知信息:\n",
      "a\n",
      "\n",
      "用户问：\n",
      "b\n",
      "\n",
      "c\n",
      "\n",
      "请用中文回答用户问题。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = build_prompt(prompt_template, info=\"a\", query=\"b\", key=\"c\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5463ab73-2101-4c71-b566-b7c363a6a363",
   "metadata": {},
   "source": [
    "### 3.5、RAG Pipeline 初探\n",
    "\n",
    "\n",
    "<video src=\"RAG.mp4\" controls=\"controls\" width=800px style=\"margin-left: 0px\"></video>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b03eb055-d9c1-46e5-a701-66108fb4f808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Prompt===\n",
      "\n",
      "你是一个问答机器人。\n",
      "你的任务是根据下述给定的已知信息回答用户问题。\n",
      "确保你的回复完全依据下述已知信息。不要编造答案。\n",
      "如果下述已知信息不足以回答用户的问题，请直接回复\"我无法回答您的问题\"。\n",
      "\n",
      "已知信息:\n",
      " Llama 2 comes in a range of parameter sizes—7B, 13B, and 70B—as well as pretrained and fine-tuned variations.\n",
      " 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.§\n",
      "\n",
      "用户问：\n",
      "how many parameters does llama 2 have?\n",
      "\n",
      "请用中文回答用户问题。\n",
      "\n",
      "===回复===\n",
      "Llama 2有7B、13B和70B三种参数大小。\n"
     ]
    }
   ],
   "source": [
    "user_query = \"how many parameters does llama 2 have?\"\n",
    "\n",
    "# 1. 检索\n",
    "search_results = search(user_query, 2)\n",
    "\n",
    "# 2. 构建 Prompt\n",
    "prompt = build_prompt(prompt_template, info=search_results, query=user_query)\n",
    "print(\"===Prompt===\")\n",
    "print(prompt)\n",
    "\n",
    "# 3. 调用 LLM\n",
    "response = get_completion(prompt)\n",
    "\n",
    "print(\"===回复===\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
