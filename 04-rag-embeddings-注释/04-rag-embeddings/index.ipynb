{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad1acaf7-09a1-4b92-b05a-3e0f0122e808",
   "metadata": {},
   "source": [
    "# 搜索增强生成（RAG，Retrieval-Augmented Generation）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d979fde",
   "metadata": {},
   "source": [
    "## 💡 这节课会带给你\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe5e70d",
   "metadata": {},
   "source": [
    "1. 如何用你的垂域数据补充 LLM 的能力\n",
    "1. 如何构建你的垂域（向量）知识库\n",
    "1. 搭建一套完整 RAG 系统 Pipeline\n",
    "\n",
    "开始上课！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb65677",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 一、澄清一个概念\n",
    "\n",
    "RAG **不要** 参考下面这张图！！！\n",
    "\n",
    "<img src=\"rag-paper.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "这张图源自一个[研究工作](https://arxiv.org/pdf/2005.11401.pdf)\n",
    "- 此论文第一次提出 RAG 这个叫法\n",
    "- 在研究中，作者尝试将检索和生成做在一个模型体系中\n",
    "\n",
    "**但是，实际生产中，RAG 不是这么做的！！！**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc3852",
   "metadata": {},
   "source": [
    "## 二、什么是检索增强的生成模型（RAG）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a059032a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.1、LLM 固有的局限性\n",
    "\n",
    "1. LLM 的知识不是实时的\n",
    "2. LLM 可能不知道你私有的领域/业务知识\n",
    "\n",
    "<img src=\"gpt-llama2.png\" style=\"margin-left: 0px\" width=\"600px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab886b1",
   "metadata": {},
   "source": [
    "### 2.2、检索增强生成\n",
    "\n",
    "天然能想到的，我们自己有产品知识库，有服务手册这些垂直领域的信息，能不能让大模型学会这些垂直领域的信息。\n",
    "我们能想象到的方法有两种：\n",
    "\n",
    "1. 重新训练大模型，把这些垂直领域的数据喂给大模型，让大模型从中学习, 这是微调\n",
    "2. 给大模型添加个外挂的知识库，我们让大模型和这个知识库结合着去给用户回答问题\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>类比：</b>\n",
    "    <li>你可以把这个过程想象成开卷考试。让 LLM 先翻书，再回答问题。这个过程模型本身是不学会知识的。</li>\n",
    "    <li>微调就是闭卷考试，你的先把所有的知识都学会，才能去回答问题。</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e524a28",
   "metadata": {},
   "source": [
    "RAG（Retrieval Augmented Generation）顾名思义，通过**检索**的方法来增强**生成模型**的能力。\n",
    "\n",
    "<video src=\"RAG.mp4\" controls=\"controls\" width=800px style=\"margin-left: 0px\"></video>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc536ba",
   "metadata": {},
   "source": [
    "## 三、RAG 系统的基本搭建流程\n",
    "\n",
    "搭建过程：\n",
    "\n",
    "1. 文档加载，并按一定条件**切割**成片段\n",
    "2. 将切割的文本片段灌入**检索引擎**\n",
    "3. 封装**检索接口**：能从文档里搜索出相关的文档片段\n",
    "4. 构建**调用流程**：Query -> 检索 -> Prompt -> LLM -> 回复\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63ebe77",
   "metadata": {},
   "source": [
    "### 3.1、文档的加载与切割\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90e32dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Using cached openai-1.34.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from openai) (4.4.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from openai) (1.10.16)\n",
      "Requirement already satisfied: sniffio in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
      "Requirement already satisfied: certifi in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Using cached openai-1.34.0-py3-none-any.whl (325 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: distro, openai\n",
      "Successfully installed distro-1.9.0 openai-1.34.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b27a296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "  Using cached pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from pdfminer.six) (3.3.2)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six)\n",
      "  Downloading cryptography-42.0.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
      "Using cached pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "Downloading cryptography-42.0.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: cryptography, pdfminer.six\n",
      "Successfully installed cryptography-42.0.8 pdfminer.six-20231228\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 安装 pdf 解析库\n",
    "!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68079010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac932a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(filename, page_numbers=None, min_line_length=1):\n",
    "    '''从 PDF 文件中（按指定页码）提取文字'''\n",
    "    paragraphs = []\n",
    "    buffer = ''\n",
    "    full_text = ''\n",
    "    # 提取全部文本\n",
    "    for i, page_layout in enumerate(extract_pages(filename)):\n",
    "        # 如果指定了页码范围，跳过范围外的页\n",
    "        if page_numbers is not None and i not in page_numbers:\n",
    "            continue\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                full_text += element.get_text() + '\\n'\n",
    "                \n",
    "    # 按空行分隔，将文本重新组织成段落\n",
    "    lines = full_text.split('\\n')\n",
    "    for text in lines:\n",
    "        if len(text) >= min_line_length:\n",
    "            buffer += (' '+text) if not text.endswith('-') else text.strip('-')\n",
    "        elif buffer:\n",
    "            paragraphs.append(buffer)\n",
    "            buffer = ''\n",
    "    if buffer:\n",
    "        paragraphs.append(buffer)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac5724b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = extract_text_from_pdf(\"llama2.pdf\", min_line_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d156c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "\n",
      " Hugo Touvron∗ Louis Martin† Kevin Stone† Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic Sergey Edunov Thomas Scialom∗\n",
      "\n",
      " GenAI, Meta\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for para in paragraphs[:3]:\n",
    "    print(para+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36b0fc0",
   "metadata": {},
   "source": [
    "## 3.2、检索引擎\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968c043c",
   "metadata": {},
   "source": [
    "这里我们使用先进的开源搜索引擎 ElasticSearch，它可以实现各种场景下的搜索功能。\n",
    "\n",
    "官方地址：https://www.elastic.co/cn/elasticsearch(有兴趣的同学可以了解)\n",
    "\n",
    "### 安装 ES 服务器\n",
    "\n",
    "安装教程地址 https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html 。\n",
    "（可以使用 cursor 参考学习）\n",
    "\n",
    "安装后，可以通过不同系统的服务状态监测指令查看 ES 运行状态，这里我的 centos 指令为 `service elasticsearch status`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a6356",
   "metadata": {},
   "source": [
    "### 安装 ES 客户端 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c52191c-fd64-4816-9da3-0423ff2aaf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch7\n",
      "  Using cached elasticsearch7-7.17.9-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting urllib3<2,>=1.21.1 (from elasticsearch7)\n",
      "  Using cached urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Requirement already satisfied: certifi in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from elasticsearch7) (2024.6.2)\n",
      "Using cached elasticsearch7-7.17.9-py2.py3-none-any.whl (386 kB)\n",
      "Using cached urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Installing collected packages: urllib3, elasticsearch7\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.1\n",
      "    Uninstalling urllib3-2.2.1:\n",
      "      Successfully uninstalled urllib3-2.2.1\n",
      "Successfully installed elasticsearch7-7.17.9 urllib3-1.26.18\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install elasticsearch7  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb2ad0-7863-4506-b9d4-c58b44e08411",
   "metadata": {},
   "source": [
    "### 安装NLTK（文本处理方法库）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01f80d66-54db-4fa6-b547-ac8211a63252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from nltk) (8.1.7)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.5.15-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from nltk) (4.66.4)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2024.5.15-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (776 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.2/776.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: regex, joblib, nltk\n",
      "Successfully installed joblib-1.4.2 nltk-3.8.1 regex-2024.5.15\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d8609ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch7 import Elasticsearch, helpers\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")  # 屏蔽 ES 的一些Warnings\n",
    "\n",
    "# 下载分词器和停用词库\n",
    "nltk.download('punkt')  # 英文切词、词根、切句等方法\n",
    "nltk.download('stopwords')  # 英文停用词库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54796cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_keywords(input_string):\n",
    "    '''（英文）文本只保留关键字'''\n",
    "    # 使用正则表达式替换所有非字母数字的字符为空格\n",
    "    no_symbols = re.sub(r'[^a-zA-Z0-9\\s]', ' ', input_string)\n",
    "    word_tokens = word_tokenize(no_symbols)\n",
    "    # 加载停用词表\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "    # 去停用词，取词根\n",
    "    filtered_sentence = [ps.stem(w)\n",
    "                         for w in word_tokens if not w.lower() in stop_words]\n",
    "    return ' '.join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca912d12-4f8f-4f3d-83fa-77a34301814f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mani paramet llama 2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_keywords('how many parameters does llama 2 have?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4818da82",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "此处 to_keywords 为针对英文的实现，针对中文的实现请参考 chinese_utils.py\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc000a0",
   "metadata": {},
   "source": [
    "将文本灌入检索引擎\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a83e3664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(983, [])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 创建Elasticsearch连接\n",
    "es = Elasticsearch(\n",
    "    hosts=['http://localhost:9200'],  # 服务地址与端口\n",
    "    # http_auth=(\"elastic\", \"FKaB1Jpz0Rlw0l6G\"),  # 用户名，密码\n",
    ")\n",
    "\n",
    "# 2. 定义索引名称\n",
    "index_name = \"teacher_demo_index_tmp\"\n",
    "\n",
    "# 3. 如果索引已存在，删除它（仅供演示，实际应用时不需要这步）\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "\n",
    "# 4. 创建索引\n",
    "es.indices.create(index=index_name)\n",
    "\n",
    "# 5. 灌库指令，构建索引\n",
    "actions = [\n",
    "    {\n",
    "        \"_index\": index_name,\n",
    "        \"_source\": {\n",
    "            \"keywords\": to_keywords(para),\n",
    "            \"text\": para\n",
    "        }\n",
    "    }\n",
    "    for para in paragraphs\n",
    "]\n",
    "\n",
    "# 6. 文本灌库\n",
    "helpers.bulk(es, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f7376a",
   "metadata": {},
   "source": [
    "实现关键字检索\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59c7ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query_string, top_n=3):\n",
    "    # ES 的查询语言\n",
    "    search_query = {\n",
    "        \"match\": {\n",
    "            \"keywords\": to_keywords(query_string)\n",
    "        }\n",
    "    }\n",
    "    res = es.search(index=index_name, query=search_query, size=top_n)\n",
    "    return [hit[\"_source\"][\"text\"] for hit in res[\"hits\"][\"hits\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57a29286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Llama 2 comes in a range of parameter sizes—7B, 13B, and 70B—as well as pretrained and fine-tuned variations.\n",
      "\n",
      " 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.§\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = search(\"how many parameters does llama 2 have?\", 2)\n",
    "for r in results:\n",
    "    print(r+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61b15f8",
   "metadata": {},
   "source": [
    "### 3.3、LLM 接口封装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b132e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "# 加载环境变量\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())  # 读取本地 .env 文件，里面定义了 OPENAI_API_KEY\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4148f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    '''封装 openai 接口'''\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,  # 模型输出的随机性，0 表示随机性最小\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc24292b",
   "metadata": {},
   "source": [
    "### 3.4、Prompt 模板\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3116b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(prompt_template, **kwargs):\n",
    "    '''将 Prompt 模板赋值'''\n",
    "    prompt = prompt_template\n",
    "    for k, v in kwargs.items():\n",
    "        if isinstance(v, str):\n",
    "            val = v\n",
    "        elif isinstance(v, list) and all(isinstance(elem, str) for elem in v):\n",
    "            val = '\\n'.join(v)\n",
    "        else:\n",
    "            val = str(v)\n",
    "        prompt = prompt.replace(f\"__{k.upper()}__\", val)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "975cac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "你是一个问答机器人。\n",
    "你的任务是根据下述给定的已知信息回答用户问题。\n",
    "确保你的回复完全依据下述已知信息。不要编造答案。\n",
    "如果下述已知信息不足以回答用户的问题，请直接回复\"我无法回答您的问题\"。\n",
    "\n",
    "已知信息:\n",
    "__INFO__\n",
    "\n",
    "用户问：\n",
    "__QUERY__\n",
    "\n",
    "请用中文回答用户问题。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef701dfa-8800-4c37-9d66-318fa121bcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "你是一个问答机器人。\n",
      "你的任务是根据下述给定的已知信息回答用户问题。\n",
      "确保你的回复完全依据下述已知信息。不要编造答案。\n",
      "如果下述已知信息不足以回答用户的问题，请直接回复\"我无法回答您的问题\"。\n",
      "\n",
      "已知信息:\n",
      "a\n",
      "\n",
      "用户问：\n",
      "b\n",
      "\n",
      "c\n",
      "\n",
      "请用中文回答用户问题。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = build_prompt(prompt_template, info=\"a\", query=\"b\", key=\"c\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2febf6d2",
   "metadata": {},
   "source": [
    "### 3.5、RAG Pipeline 初探\n",
    "\n",
    "\n",
    "<video src=\"RAG.mp4\" controls=\"controls\" width=800px style=\"margin-left: 0px\"></video>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0723e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Prompt===\n",
      "\n",
      "你是一个问答机器人。\n",
      "你的任务是根据下述给定的已知信息回答用户问题。\n",
      "确保你的回复完全依据下述已知信息。不要编造答案。\n",
      "如果下述已知信息不足以回答用户的问题，请直接回复\"我无法回答您的问题\"。\n",
      "\n",
      "已知信息:\n",
      " Llama 2 comes in a range of parameter sizes—7B, 13B, and 70B—as well as pretrained and fine-tuned variations.\n",
      " 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.§\n",
      "\n",
      "用户问：\n",
      "how many parameters does llama 2 have?\n",
      "\n",
      "请用中文回答用户问题。\n",
      "\n",
      "===回复===\n",
      "Llama 2有7B、13B和70B三种参数大小。\n"
     ]
    }
   ],
   "source": [
    "user_query = \"how many parameters does llama 2 have?\"\n",
    "\n",
    "# 1. 检索\n",
    "search_results = search(user_query, 2)\n",
    "\n",
    "# 2. 构建 Prompt\n",
    "prompt = build_prompt(prompt_template, info=search_results, query=user_query)\n",
    "print(\"===Prompt===\")\n",
    "print(prompt)\n",
    "\n",
    "# 3. 调用 LLM\n",
    "response = get_completion(prompt)\n",
    "\n",
    "print(\"===回复===\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fe8d12",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>扩展阅读：</b>\n",
    "<ol>\n",
    "<ul>Elasticsearch（简称ES）是一个广泛应用的开源搜索引擎: https://www.elastic.co/</ul>\n",
    "<ul>关于ES的安装、部署等知识，网上可以找到大量资料，例如: https://juejin.cn/post/7104875268166123528</ul>\n",
    "<ul>关于经典信息检索技术的更多细节，可以参考: https://nlp.stanford.edu/IR-book/information-retrieval-book.html</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156d3dd7",
   "metadata": {},
   "source": [
    "### 3.6、关键字检索的局限性\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71203461",
   "metadata": {},
   "source": [
    "同一个语义，用词不同，可能导致检索不到有效的结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc07e425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release\n",
      "\n",
      " Figure 20: Distribution shift for progressive versions of Llama 2-Chat, from SFT models towards RLHF.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_query=\"Does llama 2 have a chat version?\"\n",
    "# user_query = \"Does llama 2 have a conversational variant?\"\n",
    "\n",
    "search_results = search(user_query, 2)\n",
    "\n",
    "for res in search_results:\n",
    "    print(res+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcdd909",
   "metadata": {},
   "source": [
    "## 四、向量检索\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cca1b0e",
   "metadata": {},
   "source": [
    "### 4.1、文本向量（Text Embeddings）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e571b807",
   "metadata": {},
   "source": [
    "1. 将文本转成一组浮点数：每个下标 $i$，对应一个维度\n",
    "2. 整个数组对应一个 $n$ 维空间的一个点，即**文本向量**又叫 Embeddings\n",
    "3. 向量之间可以计算距离，距离远近对应**语义相似度**大小\n",
    "\n",
    "<br />\n",
    "<img src=\"embeddings.png\" style=\"margin-left: 0px\" width=800px>\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11835a3",
   "metadata": {},
   "source": [
    "### 4.1.1、文本向量是怎么得到的（选）\n",
    "\n",
    "1. 构建相关（正立）与不相关（负例）的句子对儿样本\n",
    "2. 训练双塔式模型，让正例间的距离小，负例间的距离大\n",
    "\n",
    "例如：\n",
    "\n",
    "<img src=\"sbert.png\" style=\"margin-left: 0px\" width=500px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e79d9e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>扩展阅读：https://www.sbert.net</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a937becf",
   "metadata": {},
   "source": [
    "### 4.2、向量间的相似度计算\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2622e44",
   "metadata": {},
   "source": [
    "<img src=\"sim.png\" style=\"margin-left: 0px\" width=500px>\n",
    "\n",
    "余弦相似度取值为-1到1，-1最不相似，1最相似！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6965d9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82461a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    '''余弦距离 -- 越大越相似'''\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "\n",
    "def l2(a, b):\n",
    "    '''欧式距离 -- 越小越相似'''\n",
    "    x = np.asarray(a)-np.asarray(b)\n",
    "    return norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cebc818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, model=\"text-embedding-ada-002\",dimensions=None):\n",
    "    '''封装 OpenAI 的 Embedding 模型接口，文档地址 https://platform.openai.com/docs/guides/embeddings/what-are-embeddings'''\n",
    "    if model == \"text-embedding-ada-002\":\n",
    "        dimensions = None\n",
    "    if dimensions:\n",
    "        data = client.embeddings.create(input=texts, model=model, dimensions=dimensions).data\n",
    "    else:\n",
    "        data = client.embeddings.create(input=texts, model=model).data\n",
    "    return [x.embedding for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bccfe472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.007280634716153145, -0.006147929932922125, -0.010664181783795357, 0.001484171487390995, -0.010678750462830067, 0.029253656044602394, -0.01976952701807022, 0.005444996990263462, -0.01687038503587246, -0.01207733154296875]\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "test_query = [\"测试文本\"]\n",
    "vec = get_embeddings(test_query, dimensions=128)[0]\n",
    "print(vec[:10])\n",
    "print(len(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76e2f784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance:\n",
      "1.0\n",
      "0.7622749944010911\n",
      "0.7563038106493583\n",
      "0.7426665802579038\n",
      "0.7079273699608006\n",
      "0.7254355321045071\n",
      "\n",
      "Euclidean distance:\n",
      "0.0\n",
      "0.6895288502682276\n",
      "0.6981349637998768\n",
      "0.7174028746492277\n",
      "0.764293983363683\n",
      "0.7410323668625171\n"
     ]
    }
   ],
   "source": [
    "# query = \"国际争端\"\n",
    "\n",
    "# 且能支持跨语言\n",
    "query = \"global conflicts\"\n",
    "\n",
    "documents = [\n",
    "    \"联合国就苏丹达尔富尔地区大规模暴力事件发出警告\",\n",
    "    \"土耳其、芬兰、瑞典与北约代表将继续就瑞典“入约”问题进行谈判\",\n",
    "    \"日本岐阜市陆上自卫队射击场内发生枪击事件 3人受伤\",\n",
    "    \"国家游泳中心（水立方）：恢复游泳、嬉水乐园等水上项目运营\",\n",
    "    \"我国首次在空间站开展舱外辐射生物学暴露实验\",\n",
    "]\n",
    "\n",
    "query_vec = get_embeddings([query])[0]\n",
    "doc_vecs = get_embeddings(documents)\n",
    "\n",
    "print(\"Cosine distance:\")\n",
    "print(cos_sim(query_vec, query_vec))\n",
    "\n",
    "for vec in doc_vecs:\n",
    "    print(cos_sim(query_vec, vec))\n",
    "\n",
    "print(\"\\nEuclidean distance:\")\n",
    "print(l2(query_vec, query_vec))\n",
    "for vec in doc_vecs:\n",
    "    print(l2(query_vec, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c868fa09-e858-43e7-a9c2-b727109977b1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>思考：</b>如果我有1000万个文档要去做相似度的计算，该怎么办？\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f32378",
   "metadata": {},
   "source": [
    "### 4.3、向量数据库\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf41f4d",
   "metadata": {},
   "source": [
    "一个个的比较效率实在过低，我们需要专业的数据库来帮助我们完成这种对于向量的计算的操作。\n",
    "\n",
    "**向量数据库**，是专门为向量检索设计的中间件。这里我们使用一种开源的向量数据库 chromadb，文档地址 https://docs.trychroma.com/getting-started#1.-install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b562366",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb==0.3.29 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (0.3.29)\n",
      "Requirement already satisfied: pandas>=1.3 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb==0.3.29) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.28 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb==0.3.29) (2.32.3)\n",
      "Requirement already satisfied: pydantic<2.0,>=1.9 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb==0.3.29) (1.10.16)\n",
      "Requirement already satisfied: hnswlib>=0.7 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb==0.3.29) (0.8.0)\n",
      "Requirement already satisfied: clickhouse-connect>=0.5.7 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb==0.3.29) (0.7.12)\n",
      "Requirement already satisfied: duckdb>=0.7.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb==0.3.29) (1.0.0)\n",
      "Requirement already satisfied: fastapi==0.85.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb==0.3.29) (0.85.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb==0.3.29) (1.24.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb==0.3.29) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb==0.3.29) (4.12.2)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb==0.3.29) (3.5.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb==0.3.29) (1.16.3)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb==0.3.29) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb==0.3.29) (4.66.4)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb==0.3.29) (7.7.0)\n",
      "Requirement already satisfied: graphlib-backport>=1.0.3 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from chromadb==0.3.29) (1.1.0)\n",
      "Requirement already satisfied: starlette==0.20.4 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from fastapi==0.85.1->chromadb==0.3.29) (0.20.4)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from starlette==0.20.4->fastapi==0.85.1->chromadb==0.3.29) (4.4.0)\n",
      "Requirement already satisfied: certifi in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (2024.6.2)\n",
      "Requirement already satisfied: urllib3>=1.26 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (1.26.18)\n",
      "Requirement already satisfied: pytz in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (2024.1)\n",
      "Requirement already satisfied: zstandard in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (0.22.0)\n",
      "Requirement already satisfied: lz4 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (4.3.3)\n",
      "Requirement already satisfied: coloredlogs in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (24.3.25)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (24.1)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (4.25.3)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (1.12.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from pandas>=1.3->chromadb==0.3.29) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from pandas>=1.3->chromadb==0.3.29) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from posthog>=2.4.0->chromadb==0.3.29) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from posthog>=2.4.0->chromadb==0.3.29) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from posthog>=2.4.0->chromadb==0.3.29) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from requests>=2.28->chromadb==0.3.29) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from requests>=2.28->chromadb==0.3.29) (3.7)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from tokenizers>=0.13.2->chromadb==0.3.29) (0.23.4)\n",
      "Requirement already satisfied: click>=7.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.3.29) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.3.29) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (1.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (6.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (0.22.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (12.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.3.29) (3.15.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.3.29) (2024.6.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.3.29) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.3.29) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi==0.85.1->chromadb==0.3.29) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /root/miniconda3/envs/aigclass3.8/lib/python3.8/site-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi==0.85.1->chromadb==0.3.29) (1.2.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install chromadb==0.3.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae3681ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了演示方便，我们只取两页（第一章）\n",
    "paragraphs = extract_text_from_pdf(\n",
    "    \"llama2.pdf\", \n",
    "    page_numbers=[2, 3], \n",
    "    min_line_length=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe3071c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "\n",
    "class MyVectorDBConnector:\n",
    "    def __init__(self, collection_name, embedding_fn):\n",
    "        chroma_client = chromadb.Client(Settings(allow_reset=True))\n",
    "\n",
    "        # 为了演示，实际不需要每次 reset()\n",
    "        chroma_client.reset()\n",
    "\n",
    "        # 创建一个 collection\n",
    "        self.collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "        self.embedding_fn = embedding_fn\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "        '''向 collection 中添加文档与向量'''\n",
    "        self.collection.add(\n",
    "            embeddings=self.embedding_fn(documents),  # 每个文档的向量\n",
    "            documents=documents,  # 文档的原文\n",
    "            ids=[f\"id{i}\" for i in range(len(documents))]  # 每个文档的 id\n",
    "        )\n",
    "\n",
    "    def search(self, query, top_n):\n",
    "        '''检索向量数据库'''\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=self.embedding_fn([query]),\n",
    "            n_results=top_n\n",
    "        )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0984edbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个向量数据库对象\n",
    "vector_db = MyVectorDBConnector(\"demo\", get_embeddings)\n",
    "# 向向量数据库中添加文档\n",
    "vector_db.add_documents(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d521942",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Llama 2有多少参数\"\n",
    "results = vector_db.search(user_query, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb60bb48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.§\n",
      "\n",
      " In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for para in results['documents'][0]:\n",
    "    print(para+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6a64af",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>澄清几个关键概念：</b><ul>\n",
    "    <li>向量数据库的意义是快速的检索；</li>\n",
    "    <li>向量数据库本身不生成向量，向量是由 Embedding 模型产生的；</li>\n",
    "    <li>向量数据库与传统的关系型数据库是互补的，不是替代关系，在实际应用中根据实际需求经常同时使用。</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1447f7d9",
   "metadata": {},
   "source": [
    "### 4.3.1、向量数据库服务\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72302d6e",
   "metadata": {},
   "source": [
    "Server 端\n",
    "\n",
    "```sh\n",
    "chroma run --path /db_path\n",
    "```\n",
    "\n",
    "Client 端\n",
    "\n",
    "```python\n",
    "import chromadb\n",
    "chroma_client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdf86fd",
   "metadata": {},
   "source": [
    "### 4.3.2、主流向量数据库功能对比\n",
    "\n",
    "<img src=\"vectordb.png\" style=\"margin-left: 0px\" width=600px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc0e5ee",
   "metadata": {},
   "source": [
    "- FAISS: Meta 开源的向量检索引擎 https://github.com/facebookresearch/faiss\n",
    "- Pinecone: 商用向量数据库，只有云服务 https://www.pinecone.io/\n",
    "- Milvus: 开源向量数据库，同时有云服务 https://milvus.io/\n",
    "- Weaviate: 开源向量数据库，同时有云服务 https://weaviate.io/\n",
    "- Qdrant: 开源向量数据库，同时有云服务 https://qdrant.tech/\n",
    "- PGVector: Postgres 的开源向量检索引擎 https://github.com/pgvector/pgvector\n",
    "- RediSearch: Redis 的开源向量检索引擎 https://github.com/RediSearch/RediSearch\n",
    "- ElasticSearch 也支持向量检索 https://www.elastic.co/enterprise-search/vector-search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c049d7",
   "metadata": {},
   "source": [
    "### 4.4、基于向量检索的 RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b7028d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG_Bot:\n",
    "    def __init__(self, vector_db, llm_api, n_results=2):\n",
    "        self.vector_db = vector_db\n",
    "        self.llm_api = llm_api\n",
    "        self.n_results = n_results\n",
    "\n",
    "    def chat(self, user_query):\n",
    "        # 1. 检索\n",
    "        search_results = self.vector_db.search(user_query, self.n_results)\n",
    "\n",
    "        # 2. 构建 Prompt\n",
    "        prompt = build_prompt(\n",
    "            prompt_template, info=search_results['documents'][0], query=user_query)\n",
    "\n",
    "        # 3. 调用 LLM\n",
    "        response = self.llm_api(prompt)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b17078a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已知信息中提到了\"Llama 2-Chat\"，这是一个专为对话使用情况优化的Llama 2的精调版本。因此，是的，Llama 2有对话版。\n"
     ]
    }
   ],
   "source": [
    "# 创建一个RAG机器人\n",
    "bot = RAG_Bot(\n",
    "    vector_db,\n",
    "    llm_api=get_completion\n",
    ")\n",
    "\n",
    "user_query = \"llama 2有对话版吗？\"\n",
    "\n",
    "response = bot.chat(user_query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a3cf0a",
   "metadata": {},
   "source": [
    "### 4.5、如果想要换个国产模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1771afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# 通过鉴权接口获取 access token\n",
    "def get_access_token():\n",
    "    \"\"\"\n",
    "    使用 AK，SK 生成鉴权签名（Access Token）\n",
    "    :return: access_token，或是None(如果错误)\n",
    "    \"\"\"\n",
    "    url = \"https://aip.baidubce.com/oauth/2.0/token\"\n",
    "    params = {\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"client_id\": os.getenv('ERNIE_CLIENT_ID'),\n",
    "        \"client_secret\": os.getenv('ERNIE_CLIENT_SECRET')\n",
    "    }\n",
    "\n",
    "    return str(requests.post(url, params=params).json().get(\"access_token\"))\n",
    "\n",
    "# 调用文心千帆 调用 BGE Embedding 接口\n",
    "def get_embeddings_bge(prompts):\n",
    "    url = \"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/embeddings/bge_large_en?access_token=\" + get_access_token()\n",
    "    payload = json.dumps({\n",
    "        \"input\": prompts\n",
    "    })\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    response = requests.request(\n",
    "        \"POST\", url, headers=headers, data=payload).json()\n",
    "    data = response[\"data\"]\n",
    "    return [x[\"embedding\"] for x in data]\n",
    "\n",
    "\n",
    "# 调用文心4.0对话接口\n",
    "def get_completion_ernie(prompt):\n",
    "\n",
    "    url = \"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/completions_pro?access_token=\" + get_access_token()\n",
    "    payload = json.dumps({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    response = requests.request(\n",
    "        \"POST\", url, headers=headers, data=payload).json()\n",
    "\n",
    "    return response[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e014ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个向量数据库对象\n",
    "new_vector_db = MyVectorDBConnector(\n",
    "    \"demo_ernie\",\n",
    "    embedding_fn=get_embeddings_bge\n",
    ")\n",
    "# 向向量数据库中添加文档\n",
    "new_vector_db.add_documents(paragraphs)\n",
    "\n",
    "# 创建一个RAG机器人\n",
    "new_bot = RAG_Bot(\n",
    "    new_vector_db,\n",
    "    llm_api=get_completion_ernie\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e393a6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"how many parameters does llama 2 have?\"\n",
    "\n",
    "response = new_bot.chat(user_query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a990f018",
   "metadata": {},
   "source": [
    "### 4.6、OpenAI 新发布的两个 Embedding 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15605413",
   "metadata": {},
   "source": [
    "2024年1月25日，OpenAI 新发布了两个 Embedding 模型\n",
    "\n",
    "- text-embedding-3-large\n",
    "- text-embedding-3-small\n",
    "\n",
    "其最大特点是，支持自定义的缩短向量维度，从而在几乎不影响最终效果的情况下降低向量检索与相似度计算的复杂度。\n",
    "\n",
    "通俗的说：**越大越准、越小越快。** 官方公布的评测结果:\n",
    "\n",
    "<img src=\"mteb.png\" style=\"margin-left: 0px\" width=600px>\n",
    "\n",
    "注：[MTEB](https://huggingface.co/blog/mteb) 是一个大规模多任务的 Embedding 模型公开评测集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "97da8b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim: 128\n",
      "Cosine distance:\n",
      "0.2865773001182426\n",
      "0.41830986590456204\n",
      "0.21462566338499087\n",
      "0.15146227929798226\n",
      "0.17059296471763005\n",
      "\n",
      "Euclidean distance:\n",
      "1.1945063968652392\n",
      "1.0786010898034128\n",
      "1.2532951931301017\n",
      "1.3027185279508469\n",
      "1.287949624282259\n"
     ]
    }
   ],
   "source": [
    "model = \"text-embedding-3-large\"\n",
    "dimensions = 128\n",
    "\n",
    "query = \"国际争端\"\n",
    "\n",
    "# 且能支持跨语言\n",
    "# query = \"global conflicts\"\n",
    "\n",
    "documents = [\n",
    "    \"联合国就苏丹达尔富尔地区大规模暴力事件发出警告\",\n",
    "    \"土耳其、芬兰、瑞典与北约代表将继续就瑞典“入约”问题进行谈判\",\n",
    "    \"日本岐阜市陆上自卫队射击场内发生枪击事件 3人受伤\",\n",
    "    \"国家游泳中心（水立方）：恢复游泳、嬉水乐园等水上项目运营\",\n",
    "    \"我国首次在空间站开展舱外辐射生物学暴露实验\",\n",
    "]\n",
    "\n",
    "query_vec = get_embeddings([query],model=model,dimensions=dimensions)[0]\n",
    "doc_vecs = get_embeddings(documents,model=model,dimensions=dimensions)\n",
    "\n",
    "print(\"Dim: {}\".format(len(query_vec)))\n",
    "\n",
    "print(\"Cosine distance:\")\n",
    "for vec in doc_vecs:\n",
    "    print(cos_sim(query_vec, vec))\n",
    "\n",
    "print(\"\\nEuclidean distance:\")\n",
    "for vec in doc_vecs:\n",
    "    print(l2(query_vec, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90485a68",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>扩展阅读：这种可变长度的 Embedding 技术背后的原理叫做 <a href=\"https://arxiv.org/abs/2205.13147\">Matryoshka Representation Learning</a> </b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d193f77-9d40-40c3-9018-f7d78d3a8f39",
   "metadata": {},
   "source": [
    "## 总结一下 RAG\n",
    "\n",
    "\n",
    "![](./rag-flow.jpg)\n",
    "\n",
    "\n",
    "如图所示，RAG主要由两个部分构成：\n",
    "- **建立索引**：首先要清洗和提取原始数据，将 PDF、Docx等不同格式的文件解析为纯文本数据；然后将文本数据分割成更小的片段（chunk）；最后将这些片段经过嵌入模型转换成向量数据（此过程叫做embedding），并将原始语料块和嵌入向量以键值对形式存储到向量数据库中，以便进行后续快速且频繁的搜索。这就是建立索引的过程。\n",
    "    - 文档加载，并按一定条件**切割**成片段\n",
    "    - 将切割后的文档转化为 embedding\n",
    "    - 把 embedding 存储到 embeddingStore 中\n",
    "- **检索生成**：系统会获取到用户输入，随后计算出用户的问题与向量数据库中的文档块之间的相似度，选择相似度最高的K个文档块（K值可以自己设置）作为回答当前问题的知识。知识与问题会合并到提示词模板中提交给大模型，大模型给出回复。这就是检索生成的过程。\n",
    "    - 把用户 Query 转化为 embedding，queryEmbedding\n",
    "    - 检索最相似的几个文档，找到最相似的 K 个\n",
    "    - 取回 K 个最相似的文档文本\n",
    "    - 把文本发送给大模型“包装”，生成最终返回给用户的文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29728ed4-30ca-485f-9615-ba6d1e41b407",
   "metadata": {},
   "source": [
    "## 作业：\n",
    "- 1. 独立完成 RAG 流程的构建代码\n",
    "- 2. 输入你自己的文档，让RAG来回答，看效果如何？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
